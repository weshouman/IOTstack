{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IOTStack Wiki Welcome to the IOTstack Wiki: Use the list of contents at the left of this page to explore this Wiki. If you are viewing this on a device that does not show the list by default, click the \"\u2261\" icon. If you are looking for information on a specific container, click on the \"Containers\" folder at the bottom of the list. If you are just getting started with IOTstack, see Getting Started .","title":"IOTStack Wiki"},{"location":"#iotstack-wiki","text":"Welcome to the IOTstack Wiki: Use the list of contents at the left of this page to explore this Wiki. If you are viewing this on a device that does not show the list by default, click the \"\u2261\" icon. If you are looking for information on a specific container, click on the \"Containers\" folder at the bottom of the list. If you are just getting started with IOTstack, see Getting Started .","title":"IOTStack Wiki"},{"location":"Accessing-your-Device-from-the-internet/","text":"Accessing your device from the internet The challenge most of us face with remotely accessing your home network is that you don't have a static IP. From time to time the IP that your ISP assigns to you changes and it's difficult to keep up. Fortunately, there is a solution, a DynamicDNS. The section below shows you how to set up an easy to remember address that follows your public IP no matter when it changes. Secondly, how do you get into your home network? Your router has a firewall that is designed to keep the rest of the internet out of your network to protect you. Here we install a VPN and configure the firewall to only allow very secure VPN traffic in. DuckDNS If you want to have a dynamic DNS point to your Public IP I added a helper script. Register with duckdns.org and create a subdomain name. Then edit the nano ~/IOTstack/duck/duck.sh file and add your DOMAINS=\"YOUR_DOMAINS\" DUCKDNS_TOKEN=\"YOUR_DUCKDNS_TOKEN\" first test the script to make sure it works sudo ~/IOTstack/duck/duck.sh then cat /var/log/duck.log . If you get KO then something has gone wrong and you should check out your settings in the script. If you get an OK then you can do the next step. Create a cron job by running the following command crontab -e You will be asked to use an editor option 1 for nano should be fine paste the following in the editor */5 * * * * ~/IOTstack/duck/duck.sh then ctrl+s and ctrl+x to save Your Public IP should be updated every five minutes PiVPN pimylifeup.com has an excellent tutorial on how to install PiVPN In point 17 and 18 they mention using noip for their dynamic DNS. Here you can use the DuckDNS address if you created one. Don't forget you need to open the port 1194 on your firewall. Most people won't be able to VPN from inside their network so download OpenVPN client for your mobile phone and try to connect over mobile data. ( More info. ) Once you activate your VPN (from your phone/laptop/work computer) you will effectively be on your home network and you can access your devices as if you were on the wifi at home. I personally use the VPN any time I'm on public wifi, all your traffic is secure. Zerotier https://www.zerotier.com/ Zerotier is an alternative to PiVPN that doesn't require port forwarding on your router. It does however require registering for their free tier service here . Kevin Zhang has written a how to guide here . Just note that the install link is outdated and should be: curl -s 'https://raw.githubusercontent.com/zerotier/ZeroTierOne/master/doc/contact%40zerotier.com.gpg' | gpg --import && \\ if z=$(curl -s 'https://install.zerotier.com/' | gpg); then echo \"$z\" | sudo bash; fi","title":"Accessing your device from the internet"},{"location":"Accessing-your-Device-from-the-internet/#accessing-your-device-from-the-internet","text":"The challenge most of us face with remotely accessing your home network is that you don't have a static IP. From time to time the IP that your ISP assigns to you changes and it's difficult to keep up. Fortunately, there is a solution, a DynamicDNS. The section below shows you how to set up an easy to remember address that follows your public IP no matter when it changes. Secondly, how do you get into your home network? Your router has a firewall that is designed to keep the rest of the internet out of your network to protect you. Here we install a VPN and configure the firewall to only allow very secure VPN traffic in.","title":"Accessing your device from the internet"},{"location":"Accessing-your-Device-from-the-internet/#duckdns","text":"If you want to have a dynamic DNS point to your Public IP I added a helper script. Register with duckdns.org and create a subdomain name. Then edit the nano ~/IOTstack/duck/duck.sh file and add your DOMAINS=\"YOUR_DOMAINS\" DUCKDNS_TOKEN=\"YOUR_DUCKDNS_TOKEN\" first test the script to make sure it works sudo ~/IOTstack/duck/duck.sh then cat /var/log/duck.log . If you get KO then something has gone wrong and you should check out your settings in the script. If you get an OK then you can do the next step. Create a cron job by running the following command crontab -e You will be asked to use an editor option 1 for nano should be fine paste the following in the editor */5 * * * * ~/IOTstack/duck/duck.sh then ctrl+s and ctrl+x to save Your Public IP should be updated every five minutes","title":"DuckDNS"},{"location":"Accessing-your-Device-from-the-internet/#pivpn","text":"pimylifeup.com has an excellent tutorial on how to install PiVPN In point 17 and 18 they mention using noip for their dynamic DNS. Here you can use the DuckDNS address if you created one. Don't forget you need to open the port 1194 on your firewall. Most people won't be able to VPN from inside their network so download OpenVPN client for your mobile phone and try to connect over mobile data. ( More info. ) Once you activate your VPN (from your phone/laptop/work computer) you will effectively be on your home network and you can access your devices as if you were on the wifi at home. I personally use the VPN any time I'm on public wifi, all your traffic is secure.","title":"PiVPN"},{"location":"Accessing-your-Device-from-the-internet/#zerotier","text":"https://www.zerotier.com/ Zerotier is an alternative to PiVPN that doesn't require port forwarding on your router. It does however require registering for their free tier service here . Kevin Zhang has written a how to guide here . Just note that the install link is outdated and should be: curl -s 'https://raw.githubusercontent.com/zerotier/ZeroTierOne/master/doc/contact%40zerotier.com.gpg' | gpg --import && \\ if z=$(curl -s 'https://install.zerotier.com/' | gpg); then echo \"$z\" | sudo bash; fi","title":"Zerotier"},{"location":"Backup-and-Restore/","text":"Backing up and restoring IOTstack This page explains how to use the backup and restore functionality of IOTstack. Backup The backup command can be executed from IOTstack's menu, or from a cronjob. Running backup To ensure that all your data is saved correctly, the stack should be brought down. This is mainly due to databases potentially being in a state that could cause data loss. There are 2 ways to run backups: * From the menu: Backup and Restore > Run backup * Running the following command: bash ./scripts/backup.sh The command that's run from the command line can also be executed from a cronjob: 0 2 * * * cd /home/pi/IOTstack && /bin/bash ./scripts/backup.sh The current directory of bash must be in IOTstack's directory, to ensure that it can find the relative paths of the files it's meant to back up. In the example above, it's assume that it's inside the pi user's home directory. Arguments ./scripts/backup.sh {TYPE=3} {USER=$(whoami)} Types: 1 = Backup with Date A tarball file will be created that contains the date and time the backup was started, in the filename. 2 = Rolling Date A tarball file will be created that contains the day of the week (0-6) the backup was started, in the filename. If a tarball already exists with the same name, it will be overwritten. 3 = Both User: This parameter only becomes active if run as root. This script will default to the current logged in user If this parameter is not supplied when run as root, the script will ask for the username as input Backups: * You can find the backups in the ./backups/ folder. With rolling being in ./backups/rolling/ and date backups in ./backups/backup/ * Log files can also be found in the ./backups/logs/ directory. Examples: ./scripts/backup.sh ./scripts/backup.sh 3 Either of these will run both backups. ./scripts/backup.sh 2 This will only produce a backup in the rollowing folder. It will be called 'backup_XX.tar.gz' where XX is the current day of the week (as an int) sudo bash ./scripts/backup.sh 2 pi This will only produce a backup in the rollowing folder and change all the permissions to the 'pi' user. Restore There are 2 ways to run a restore: * From the menu: Backup and Restore > Restore from backup * Running the following command: bash ./scripts/restore.sh Important : The restore script assumes that the IOTstack directory is fresh, as if it was just cloned. If it is not fresh, errors may occur, or your data may not correctly be restored even if no errors are apparent. Note : It is suggested that you test that your backups can be restored after initially setting up, and anytime you add or remove a service. Major updates to services can also break backups. Arguments ./scripts/restore.sh {FILENAME=backup.tar.gz} {noask} The restore script takes 2 arguments: * Filename: The name of the backup file. The file must be present in the ./backups/ directory, or a subfolder in it. That means it should be moved from ./backups/backup to ./backups/ , or that you need to specify the backup portion of the directory (see examples) * NoAsk: If a second parameter is present, is acts as setting the no ask flag to true. Pre and post script hooks The script checks if there are any pre and post back up hooks to execute commands. Both of these files will be included in the backup, and have also been added to the .gitignore file, so that they will not be touched when IOTstack updates. Prebackup script hook The prebackup hook script is executed before any compression happens and before anything is written to the temporary backup manifest file ( ./.tmp/backup-list_{{NAME}}.txt ). It can be used to prepare any services (such as databases that IOTstack isn't aware of) for backing up. To use it, simple create a ./pre_backup.sh file in IOTstack's main directory. It will be executed next time a backup runs. Postbackup script hook The postbackup hook script is executed after the tarball file has been written to disk, and before the final backup log information is written to disk. To use it, simple create a ./post_backup.sh file in IOTstack's main directory. It will be executed after the next time a backup runs. Post restore script hook The post restore hook script is executed after all files have been extracted and written to disk. It can be used to apply permissions that your custom services may require. To use it, simple create a ./post_restore.sh file in IOTstack's main directory. It will be executed after a restore happens. Third party integration This section explains how to backup your files with 3rd party software. Dropbox Coming soon. Google Drive Coming soon. rsync Coming soon. Duplicati Coming soon. SFTP Coming soon.","title":"Backing up and restoring IOTstack"},{"location":"Backup-and-Restore/#backing-up-and-restoring-iotstack","text":"This page explains how to use the backup and restore functionality of IOTstack.","title":"Backing up and restoring IOTstack"},{"location":"Backup-and-Restore/#backup","text":"The backup command can be executed from IOTstack's menu, or from a cronjob.","title":"Backup"},{"location":"Backup-and-Restore/#running-backup","text":"To ensure that all your data is saved correctly, the stack should be brought down. This is mainly due to databases potentially being in a state that could cause data loss. There are 2 ways to run backups: * From the menu: Backup and Restore > Run backup * Running the following command: bash ./scripts/backup.sh The command that's run from the command line can also be executed from a cronjob: 0 2 * * * cd /home/pi/IOTstack && /bin/bash ./scripts/backup.sh The current directory of bash must be in IOTstack's directory, to ensure that it can find the relative paths of the files it's meant to back up. In the example above, it's assume that it's inside the pi user's home directory.","title":"Running backup"},{"location":"Backup-and-Restore/#arguments","text":"./scripts/backup.sh {TYPE=3} {USER=$(whoami)} Types: 1 = Backup with Date A tarball file will be created that contains the date and time the backup was started, in the filename. 2 = Rolling Date A tarball file will be created that contains the day of the week (0-6) the backup was started, in the filename. If a tarball already exists with the same name, it will be overwritten. 3 = Both User: This parameter only becomes active if run as root. This script will default to the current logged in user If this parameter is not supplied when run as root, the script will ask for the username as input Backups: * You can find the backups in the ./backups/ folder. With rolling being in ./backups/rolling/ and date backups in ./backups/backup/ * Log files can also be found in the ./backups/logs/ directory.","title":"Arguments"},{"location":"Backup-and-Restore/#examples","text":"./scripts/backup.sh ./scripts/backup.sh 3 Either of these will run both backups. ./scripts/backup.sh 2 This will only produce a backup in the rollowing folder. It will be called 'backup_XX.tar.gz' where XX is the current day of the week (as an int) sudo bash ./scripts/backup.sh 2 pi This will only produce a backup in the rollowing folder and change all the permissions to the 'pi' user.","title":"Examples:"},{"location":"Backup-and-Restore/#restore","text":"There are 2 ways to run a restore: * From the menu: Backup and Restore > Restore from backup * Running the following command: bash ./scripts/restore.sh Important : The restore script assumes that the IOTstack directory is fresh, as if it was just cloned. If it is not fresh, errors may occur, or your data may not correctly be restored even if no errors are apparent. Note : It is suggested that you test that your backups can be restored after initially setting up, and anytime you add or remove a service. Major updates to services can also break backups.","title":"Restore"},{"location":"Backup-and-Restore/#arguments_1","text":"./scripts/restore.sh {FILENAME=backup.tar.gz} {noask} The restore script takes 2 arguments: * Filename: The name of the backup file. The file must be present in the ./backups/ directory, or a subfolder in it. That means it should be moved from ./backups/backup to ./backups/ , or that you need to specify the backup portion of the directory (see examples) * NoAsk: If a second parameter is present, is acts as setting the no ask flag to true.","title":"Arguments"},{"location":"Backup-and-Restore/#pre-and-post-script-hooks","text":"The script checks if there are any pre and post back up hooks to execute commands. Both of these files will be included in the backup, and have also been added to the .gitignore file, so that they will not be touched when IOTstack updates.","title":"Pre and post script hooks"},{"location":"Backup-and-Restore/#prebackup-script-hook","text":"The prebackup hook script is executed before any compression happens and before anything is written to the temporary backup manifest file ( ./.tmp/backup-list_{{NAME}}.txt ). It can be used to prepare any services (such as databases that IOTstack isn't aware of) for backing up. To use it, simple create a ./pre_backup.sh file in IOTstack's main directory. It will be executed next time a backup runs.","title":"Prebackup script hook"},{"location":"Backup-and-Restore/#postbackup-script-hook","text":"The postbackup hook script is executed after the tarball file has been written to disk, and before the final backup log information is written to disk. To use it, simple create a ./post_backup.sh file in IOTstack's main directory. It will be executed after the next time a backup runs.","title":"Postbackup script hook"},{"location":"Backup-and-Restore/#post-restore-script-hook","text":"The post restore hook script is executed after all files have been extracted and written to disk. It can be used to apply permissions that your custom services may require. To use it, simple create a ./post_restore.sh file in IOTstack's main directory. It will be executed after a restore happens.","title":"Post restore script hook"},{"location":"Backup-and-Restore/#third-party-integration","text":"This section explains how to backup your files with 3rd party software.","title":"Third party integration"},{"location":"Backup-and-Restore/#dropbox","text":"Coming soon.","title":"Dropbox"},{"location":"Backup-and-Restore/#google-drive","text":"Coming soon.","title":"Google Drive"},{"location":"Backup-and-Restore/#rsync","text":"Coming soon.","title":"rsync"},{"location":"Backup-and-Restore/#duplicati","text":"Coming soon.","title":"Duplicati"},{"location":"Backup-and-Restore/#sftp","text":"Coming soon.","title":"SFTP"},{"location":"BuildStack-RandomPassword/","text":"Build Stack Random Services Password This page explains how to have a service generate a random password during build time. This will require that your service have a working options menu. Keep in mind that updating strings in a service's yaml config isn't limited to passwords. A word of caution Many services often set a password on their initial spin up and store it internally. That means if if the password is changed by the menu afterwards, it may not be reflected in the service. By default the password specified in the documentation should be used, unless the user specifically selected to use a randomly generated one. In the future, the feature to specify a password manually may be added in, much like how ports can be customised. A basic example Inside the service's service.yml file, a special string can be added in for the build script to find and replace. Commonly the string is %randomPassword% , but technically any string can be used. The same string can be used multiple times for the same password to be used multiple times, and/or multiple difference strings can be used for multiple passwords. mariadb: image: linuxserver/mariadb container_name: mariadb environment: - MYSQL_ROOT_PASSWORD=%randomAdminPassword% - MYSQL_DATABASE=default - MYSQL_USER=mariadbuser - MYSQL_PASSWORD=%randomPassword% These strings will be updated during the Prebuild Hook stage when building. The code to make this happen is shown below. Code commonly used to update passwords This code can basically be copy-pasted into your service's build.py file. You are welcome to expand upon it if required. It will probably be refactored into a utils function in the future to adear to DRY (Don't Repeat Yourself) practices. def preBuild(): # Multi-service load. Most services only include a single service. The exception being NextCloud where the database information needs to match between NextCloud and MariaDB (as defined in NextCloud's 'service.yml' file, not IOTstack's MariaDB). with open((r'%s/' % serviceTemplate) + servicesFileName) as objServiceFile: serviceYamlTemplate = yaml.load(objServiceFile) oldBuildCache = {} try: with open(r'%s' % buildCache) as objBuildCache: # Load previous build, if it exists oldBuildCache = yaml.load(objBuildCache) except: pass buildCacheServices = {} if \"services\" in oldBuildCache: # If a previous build does exist, load it so that we can reuse the password from it if required. buildCacheServices = oldBuildCache[\"services\"] if not os.path.exists(serviceService): # Create the service directory for the service os.makedirs(serviceService, exist_ok=True) # Check if buildSettings file exists (from previous build), or create one if it doesn't (in the else block). if os.path.exists(buildSettings): # Password randomisation with open(r'%s' % buildSettings) as objBuildSettingsFile: piHoleYamlBuildOptions = yaml.load(objBuildSettingsFile) if ( piHoleYamlBuildOptions[\"databasePasswordOption\"] == \"Randomise database password for this build\" or piHoleYamlBuildOptions[\"databasePasswordOption\"] == \"Randomise database password every build\" or deconzYamlBuildOptions[\"databasePasswordOption\"] == \"Use default password for this build\" ): if deconzYamlBuildOptions[\"databasePasswordOption\"] == \"Use default password for this build\": newAdminPassword = \"######\" # Update to what's specified in your documentation newPassword = \"######\" # Update to what's specified in your documentation else: # Generate our passwords newAdminPassword = generateRandomString() newPassword = generateRandomString() # Here we loop through each service included in the current service's `service.yml` file and update the password strings. for (index, serviceName) in enumerate(serviceYamlTemplate): dockerComposeServicesYaml[serviceName] = serviceYamlTemplate[serviceName] if \"environment\" in serviceYamlTemplate[serviceName]: for (envIndex, envName) in enumerate(serviceYamlTemplate[serviceName][\"environment\"]): envName = envName.replace(\"%randomPassword%\", newPassword) envName = envName.replace(\"%randomAdminPassword%\", newAdminPassword) dockerComposeServicesYaml[serviceName][\"environment\"][envIndex] = envName # If the user had selected to only update the password once, ensure the build options file is updated. if (piHoleYamlBuildOptions[\"databasePasswordOption\"] == \"Randomise database password for this build\"): piHoleYamlBuildOptions[\"databasePasswordOption\"] = \"Do nothing\" with open(buildSettings, 'w') as outputFile: yaml.dump(piHoleYamlBuildOptions, outputFile) else: # Do nothing - don't change password for (index, serviceName) in enumerate(buildCacheServices): if serviceName in buildCacheServices: # Load service from cache if exists (to maintain password) dockerComposeServicesYaml[serviceName] = buildCacheServices[serviceName] else: dockerComposeServicesYaml[serviceName] = serviceYamlTemplate[serviceName] # Build options file didn't exist, so create one, and also use default password (default action). else: print(\"PiHole Warning: Build settings file not found, using default password\") time.sleep(1) newAdminPassword = \"######\" # Update to what's specified in your documentation newPassword = \"######\" # Update to what's specified in your documentation for (index, serviceName) in enumerate(serviceYamlTemplate): dockerComposeServicesYaml[serviceName] = serviceYamlTemplate[serviceName] if \"environment\" in serviceYamlTemplate[serviceName]: for (envIndex, envName) in enumerate(serviceYamlTemplate[serviceName][\"environment\"]): envName = envName.replace(\"%randomPassword%\", newPassword) envName = envName.replace(\"%randomAdminPassword%\", newAdminPassword) dockerComposeServicesYaml[serviceName][\"environment\"][envIndex] = envName piHoleYamlBuildOptions = { \"version\": \"1\", \"application\": \"IOTstack\", \"service\": \"PiHole\", \"comment\": \"PiHole Build Options\" } piHoleYamlBuildOptions[\"databasePasswordOption\"] = \"Do nothing\" with open(buildSettings, 'w') as outputFile: yaml.dump(piHoleYamlBuildOptions, outputFile) return True Code for your service's menu While not needed, since the default action is to create a random password, it is a good idea to allow the user to choose what to do. This can be achieved by giving them access to a password menu. This code can be placed in your service's build.py file, that will show a new menu option, allowing users to select it and be taken to a password settings screen. Remember that you need to have an already working menu, and to place this code into it. import signal ... def setPasswordOptions(): global needsRender global hasRebuiltAddons passwordOptionsMenuFilePath = \"./.templates/{currentService}/passwords.py\".format(currentService=currentServiceName) with open(passwordOptionsMenuFilePath, \"rb\") as pythonDynamicImportFile: code = compile(pythonDynamicImportFile.read(), passwordOptionsMenuFilePath, \"exec\") execGlobals = { \"currentServiceName\": currentServiceName, \"renderMode\": renderMode } execLocals = {} screenActive = False exec(code, execGlobals, execLocals) signal.signal(signal.SIGWINCH, onResize) screenActive = True needsRender = 1 ... def createMenu(): global yourServicesBuildOptions global serviceService yourServicesBuildOptions = [] yourServicesBuildOptions.append([ \"Your Service Password Options\", setPasswordOptions ]) yourServicesBuildOptions.append([\"Go back\", goBack]) Password settings screen The code for the Password settings is lengthy, but it's pasted here for convienence #!/usr/bin/env python3 import signal def main(): from blessed import Terminal from deps.chars import specialChars, commonTopBorder, commonBottomBorder, commonEmptyLine from deps.consts import servicesDirectory, templatesDirectory, buildSettingsFileName import time import subprocess import ruamel.yamls import os global signal global currentServiceName global menuSelectionInProgress global mainMenuList global currentMenuItemIndex global renderMode global paginationSize global paginationStartIndex global hideHelpText yaml = ruamel.yaml.YAML() yaml.preserve_quotes = True try: # If not already set, then set it. hideHelpText = hideHelpText except: hideHelpText = False term = Terminal() hotzoneLocation = [((term.height // 16) + 6), 0] paginationToggle = [10, term.height - 25] paginationStartIndex = 0 paginationSize = paginationToggle[0] serviceService = servicesDirectory + currentServiceName serviceTemplate = templatesDirectory + currentServiceName buildSettings = serviceService + buildSettingsFileName def goBack(): global menuSelectionInProgress global needsRender menuSelectionInProgress = False needsRender = 1 return True mainMenuList = [] hotzoneLocation = [((term.height // 16) + 6), 0] menuSelectionInProgress = True currentMenuItemIndex = 0 menuNavigateDirection = 0 # Render Modes: # 0 = No render needed # 1 = Full render # 2 = Hotzone only needsRender = 1 def onResize(sig, action): global mainMenuList global currentMenuItemIndex mainRender(1, mainMenuList, currentMenuItemIndex) def generateLineText(text, textLength=None, paddingBefore=0, lineLength=64): result = \"\" for i in range(paddingBefore): result += \" \" textPrintableCharactersLength = textLength if (textPrintableCharactersLength) == None: textPrintableCharactersLength = len(text) result += text remainingSpace = lineLength - textPrintableCharactersLength for i in range(remainingSpace): result += \" \" return result def renderHotZone(term, renderType, menu, selection, hotzoneLocation, paddingBefore = 4): global paginationSize selectedTextLength = len(\"-> \") print(term.move(hotzoneLocation[0], hotzoneLocation[1])) if paginationStartIndex >= 1: print(term.center(\"{b} {uaf} {uaf}{uaf}{uaf} {ual} {b}\".format( b=specialChars[renderMode][\"borderVertical\"], uaf=specialChars[renderMode][\"upArrowFull\"], ual=specialChars[renderMode][\"upArrowLine\"] ))) else: print(term.center(commonEmptyLine(renderMode))) for (index, menuItem) in enumerate(menu): # Menu loop if index >= paginationStartIndex and index < paginationStartIndex + paginationSize: lineText = generateLineText(menuItem[0], paddingBefore=paddingBefore) # Menu highlight logic if index == selection: formattedLineText = '-> {t.blue_on_green}{title}{t.normal} <-'.format(t=term, title=menuItem[0]) paddedLineText = generateLineText(formattedLineText, textLength=len(menuItem[0]) + selectedTextLength, paddingBefore=paddingBefore - selectedTextLength) toPrint = paddedLineText else: toPrint = '{title}{t.normal}'.format(t=term, title=lineText) # ##### # Menu check render logic if menuItem[1][\"checked\"]: toPrint = \" (X) \" + toPrint else: toPrint = \" ( ) \" + toPrint toPrint = \"{bv} {toPrint} {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"], toPrint=toPrint) # Generate border toPrint = term.center(toPrint) # Center Text (All lines should have the same amount of printable characters) # ##### print(toPrint) if paginationStartIndex + paginationSize < len(menu): print(term.center(\"{b} {daf} {daf}{daf}{daf} {dal} {b}\".format( b=specialChars[renderMode][\"borderVertical\"], daf=specialChars[renderMode][\"downArrowFull\"], dal=specialChars[renderMode][\"downArrowLine\"] ))) else: print(term.center(commonEmptyLine(renderMode))) print(term.center(commonEmptyLine(renderMode))) print(term.center(commonEmptyLine(renderMode))) def mainRender(needsRender, menu, selection): global paginationStartIndex global paginationSize term = Terminal() if selection >= paginationStartIndex + paginationSize: paginationStartIndex = selection - (paginationSize - 1) + 1 needsRender = 1 if selection <= paginationStartIndex - 1: paginationStartIndex = selection needsRender = 1 if needsRender == 1: print(term.clear()) print(term.move_y(term.height // 16)) print(term.black_on_cornsilk4(term.center('IOTstack YourServices Password Options'))) print(\"\") print(term.center(commonTopBorder(renderMode))) print(term.center(commonEmptyLine(renderMode))) print(term.center(\"{bv} Select Password Option {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(commonEmptyLine(renderMode))) if needsRender >= 1: renderHotZone(term, needsRender, menu, selection, hotzoneLocation) if needsRender == 1: print(term.center(commonEmptyLine(renderMode))) if not hideHelpText: if term.height < 32: print(term.center(commonEmptyLine(renderMode))) print(term.center(\"{bv} Not enough vertical room to render controls help text {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(commonEmptyLine(renderMode))) else: print(term.center(commonEmptyLine(renderMode))) print(term.center(\"{bv} Controls: {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Space] to select option {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Up] and [Down] to move selection cursor {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [H] Show/hide this text {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Enter] to build and save option {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Escape] to cancel changes {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(commonEmptyLine(renderMode))) print(term.center(commonEmptyLine(renderMode))) print(term.center(commonBottomBorder(renderMode))) def runSelection(selection): import types if len(mainMenuList[selection]) > 1 and isinstance(mainMenuList[selection][1], types.FunctionType): mainMenuList[selection][1]() else: print(term.green_reverse('IOTstack Error: No function assigned to menu item: \"{}\"'.format(mainMenuList[selection][0]))) def isMenuItemSelectable(menu, index): if len(menu) > index: if len(menu[index]) > 1: if \"skip\" in menu[index][1] and menu[index][1][\"skip\"] == True: return False return True def loadOptionsMenu(): global mainMenuList mainMenuList.append([\"Use default password for this build\", { \"checked\": True }]) mainMenuList.append([\"Randomise database password for this build\", { \"checked\": False }]) mainMenuList.append([\"Randomise database password every build\", { \"checked\": False }]) mainMenuList.append([\"Do nothing\", { \"checked\": False }]) def checkMenuItem(selection): global mainMenuList for (index, menuItem) in enumerate(mainMenuList): mainMenuList[index][1][\"checked\"] = False mainMenuList[selection][1][\"checked\"] = True def saveOptions(): try: if not os.path.exists(serviceService): os.makedirs(serviceService, exist_ok=True) if os.path.exists(buildSettings): with open(r'%s' % buildSettings) as objBuildSettingsFile: yourServicesYamlBuildOptions = yaml.load(objBuildSettingsFile) else: yourServices = { \"version\": \"1\", \"application\": \"IOTstack\", \"service\": \"Your Service\", \"comment\": \"Your Service Build Options\" } yourServices[\"databasePasswordOption\"] = \"\" for (index, menuOption) in enumerate(mainMenuList): if menuOption[1][\"checked\"]: yourServices[\"databasePasswordOption\"] = menuOption[0] break with open(buildSettings, 'w') as outputFile: yaml.dump(yourServices, outputFile) except Exception as err: print(\"Error saving Your Services Password options\", currentServiceName) print(err) return False global hasRebuiltHardwareSelection hasRebuiltHardwareSelection = True return True def loadOptions(): try: if not os.path.exists(serviceService): os.makedirs(serviceService, exist_ok=True) if os.path.exists(buildSettings): with open(r'%s' % buildSettings) as objBuildSettingsFile: yourServicesYamlBuildOptions = yaml.load(objBuildSettingsFile) for (index, menuOption) in enumerate(mainMenuList): if menuOption[0] == yourServicesYamlBuildOptions[\"databasePasswordOption\"]: checkMenuItem(index) break except Exception as err: print(\"Error loading Your Services Password options\", currentServiceName) print(err) return False return True if __name__ == 'builtins': global signal term = Terminal() signal.signal(signal.SIGWINCH, onResize) loadOptionsMenu() loadOptions() with term.fullscreen(): menuNavigateDirection = 0 mainRender(needsRender, mainMenuList, currentMenuItemIndex) menuSelectionInProgress = True with term.cbreak(): while menuSelectionInProgress: menuNavigateDirection = 0 if not needsRender == 0: # Only rerender when changed to prevent flickering mainRender(needsRender, mainMenuList, currentMenuItemIndex) needsRender = 0 key = term.inkey(esc_delay=0.05) if key.is_sequence: if key.name == 'KEY_TAB': if paginationSize == paginationToggle[0]: paginationSize = paginationToggle[1] else: paginationSize = paginationToggle[0] mainRender(1, mainMenuList, currentMenuItemIndex) if key.name == 'KEY_DOWN': menuNavigateDirection += 1 if key.name == 'KEY_UP': menuNavigateDirection -= 1 if key.name == 'KEY_ENTER': if saveOptions(): return True else: print(\"Something went wrong. Try saving the list again.\") if key.name == 'KEY_ESCAPE': menuSelectionInProgress = False return True elif key: if key == ' ': # Space pressed checkMenuItem(currentMenuItemIndex) # Update checked list needsRender = 2 elif key == 'h': # H pressed if hideHelpText: hideHelpText = False else: hideHelpText = True mainRender(1, mainMenuList, currentMenuItemIndex) if menuNavigateDirection != 0: # If a direction was pressed, find next selectable item currentMenuItemIndex += menuNavigateDirection currentMenuItemIndex = currentMenuItemIndex % len(mainMenuList) needsRender = 2 while not isMenuItemSelectable(mainMenuList, currentMenuItemIndex): currentMenuItemIndex += menuNavigateDirection currentMenuItemIndex = currentMenuItemIndex % len(mainMenuList) return True return True originalSignalHandler = signal.getsignal(signal.SIGINT) main() signal.signal(signal.SIGWINCH, originalSignalHandler)","title":"Build Stack Random Services Password"},{"location":"BuildStack-RandomPassword/#build-stack-random-services-password","text":"This page explains how to have a service generate a random password during build time. This will require that your service have a working options menu. Keep in mind that updating strings in a service's yaml config isn't limited to passwords.","title":"Build Stack Random Services Password"},{"location":"BuildStack-RandomPassword/#a-word-of-caution","text":"Many services often set a password on their initial spin up and store it internally. That means if if the password is changed by the menu afterwards, it may not be reflected in the service. By default the password specified in the documentation should be used, unless the user specifically selected to use a randomly generated one. In the future, the feature to specify a password manually may be added in, much like how ports can be customised.","title":"A word of caution"},{"location":"BuildStack-RandomPassword/#a-basic-example","text":"Inside the service's service.yml file, a special string can be added in for the build script to find and replace. Commonly the string is %randomPassword% , but technically any string can be used. The same string can be used multiple times for the same password to be used multiple times, and/or multiple difference strings can be used for multiple passwords. mariadb: image: linuxserver/mariadb container_name: mariadb environment: - MYSQL_ROOT_PASSWORD=%randomAdminPassword% - MYSQL_DATABASE=default - MYSQL_USER=mariadbuser - MYSQL_PASSWORD=%randomPassword% These strings will be updated during the Prebuild Hook stage when building. The code to make this happen is shown below.","title":"A basic example"},{"location":"BuildStack-RandomPassword/#code-commonly-used-to-update-passwords","text":"This code can basically be copy-pasted into your service's build.py file. You are welcome to expand upon it if required. It will probably be refactored into a utils function in the future to adear to DRY (Don't Repeat Yourself) practices. def preBuild(): # Multi-service load. Most services only include a single service. The exception being NextCloud where the database information needs to match between NextCloud and MariaDB (as defined in NextCloud's 'service.yml' file, not IOTstack's MariaDB). with open((r'%s/' % serviceTemplate) + servicesFileName) as objServiceFile: serviceYamlTemplate = yaml.load(objServiceFile) oldBuildCache = {} try: with open(r'%s' % buildCache) as objBuildCache: # Load previous build, if it exists oldBuildCache = yaml.load(objBuildCache) except: pass buildCacheServices = {} if \"services\" in oldBuildCache: # If a previous build does exist, load it so that we can reuse the password from it if required. buildCacheServices = oldBuildCache[\"services\"] if not os.path.exists(serviceService): # Create the service directory for the service os.makedirs(serviceService, exist_ok=True) # Check if buildSettings file exists (from previous build), or create one if it doesn't (in the else block). if os.path.exists(buildSettings): # Password randomisation with open(r'%s' % buildSettings) as objBuildSettingsFile: piHoleYamlBuildOptions = yaml.load(objBuildSettingsFile) if ( piHoleYamlBuildOptions[\"databasePasswordOption\"] == \"Randomise database password for this build\" or piHoleYamlBuildOptions[\"databasePasswordOption\"] == \"Randomise database password every build\" or deconzYamlBuildOptions[\"databasePasswordOption\"] == \"Use default password for this build\" ): if deconzYamlBuildOptions[\"databasePasswordOption\"] == \"Use default password for this build\": newAdminPassword = \"######\" # Update to what's specified in your documentation newPassword = \"######\" # Update to what's specified in your documentation else: # Generate our passwords newAdminPassword = generateRandomString() newPassword = generateRandomString() # Here we loop through each service included in the current service's `service.yml` file and update the password strings. for (index, serviceName) in enumerate(serviceYamlTemplate): dockerComposeServicesYaml[serviceName] = serviceYamlTemplate[serviceName] if \"environment\" in serviceYamlTemplate[serviceName]: for (envIndex, envName) in enumerate(serviceYamlTemplate[serviceName][\"environment\"]): envName = envName.replace(\"%randomPassword%\", newPassword) envName = envName.replace(\"%randomAdminPassword%\", newAdminPassword) dockerComposeServicesYaml[serviceName][\"environment\"][envIndex] = envName # If the user had selected to only update the password once, ensure the build options file is updated. if (piHoleYamlBuildOptions[\"databasePasswordOption\"] == \"Randomise database password for this build\"): piHoleYamlBuildOptions[\"databasePasswordOption\"] = \"Do nothing\" with open(buildSettings, 'w') as outputFile: yaml.dump(piHoleYamlBuildOptions, outputFile) else: # Do nothing - don't change password for (index, serviceName) in enumerate(buildCacheServices): if serviceName in buildCacheServices: # Load service from cache if exists (to maintain password) dockerComposeServicesYaml[serviceName] = buildCacheServices[serviceName] else: dockerComposeServicesYaml[serviceName] = serviceYamlTemplate[serviceName] # Build options file didn't exist, so create one, and also use default password (default action). else: print(\"PiHole Warning: Build settings file not found, using default password\") time.sleep(1) newAdminPassword = \"######\" # Update to what's specified in your documentation newPassword = \"######\" # Update to what's specified in your documentation for (index, serviceName) in enumerate(serviceYamlTemplate): dockerComposeServicesYaml[serviceName] = serviceYamlTemplate[serviceName] if \"environment\" in serviceYamlTemplate[serviceName]: for (envIndex, envName) in enumerate(serviceYamlTemplate[serviceName][\"environment\"]): envName = envName.replace(\"%randomPassword%\", newPassword) envName = envName.replace(\"%randomAdminPassword%\", newAdminPassword) dockerComposeServicesYaml[serviceName][\"environment\"][envIndex] = envName piHoleYamlBuildOptions = { \"version\": \"1\", \"application\": \"IOTstack\", \"service\": \"PiHole\", \"comment\": \"PiHole Build Options\" } piHoleYamlBuildOptions[\"databasePasswordOption\"] = \"Do nothing\" with open(buildSettings, 'w') as outputFile: yaml.dump(piHoleYamlBuildOptions, outputFile) return True","title":"Code commonly used to update passwords"},{"location":"BuildStack-RandomPassword/#code-for-your-services-menu","text":"While not needed, since the default action is to create a random password, it is a good idea to allow the user to choose what to do. This can be achieved by giving them access to a password menu. This code can be placed in your service's build.py file, that will show a new menu option, allowing users to select it and be taken to a password settings screen. Remember that you need to have an already working menu, and to place this code into it. import signal ... def setPasswordOptions(): global needsRender global hasRebuiltAddons passwordOptionsMenuFilePath = \"./.templates/{currentService}/passwords.py\".format(currentService=currentServiceName) with open(passwordOptionsMenuFilePath, \"rb\") as pythonDynamicImportFile: code = compile(pythonDynamicImportFile.read(), passwordOptionsMenuFilePath, \"exec\") execGlobals = { \"currentServiceName\": currentServiceName, \"renderMode\": renderMode } execLocals = {} screenActive = False exec(code, execGlobals, execLocals) signal.signal(signal.SIGWINCH, onResize) screenActive = True needsRender = 1 ... def createMenu(): global yourServicesBuildOptions global serviceService yourServicesBuildOptions = [] yourServicesBuildOptions.append([ \"Your Service Password Options\", setPasswordOptions ]) yourServicesBuildOptions.append([\"Go back\", goBack])","title":"Code for your service's menu"},{"location":"BuildStack-RandomPassword/#password-settings-screen","text":"The code for the Password settings is lengthy, but it's pasted here for convienence #!/usr/bin/env python3 import signal def main(): from blessed import Terminal from deps.chars import specialChars, commonTopBorder, commonBottomBorder, commonEmptyLine from deps.consts import servicesDirectory, templatesDirectory, buildSettingsFileName import time import subprocess import ruamel.yamls import os global signal global currentServiceName global menuSelectionInProgress global mainMenuList global currentMenuItemIndex global renderMode global paginationSize global paginationStartIndex global hideHelpText yaml = ruamel.yaml.YAML() yaml.preserve_quotes = True try: # If not already set, then set it. hideHelpText = hideHelpText except: hideHelpText = False term = Terminal() hotzoneLocation = [((term.height // 16) + 6), 0] paginationToggle = [10, term.height - 25] paginationStartIndex = 0 paginationSize = paginationToggle[0] serviceService = servicesDirectory + currentServiceName serviceTemplate = templatesDirectory + currentServiceName buildSettings = serviceService + buildSettingsFileName def goBack(): global menuSelectionInProgress global needsRender menuSelectionInProgress = False needsRender = 1 return True mainMenuList = [] hotzoneLocation = [((term.height // 16) + 6), 0] menuSelectionInProgress = True currentMenuItemIndex = 0 menuNavigateDirection = 0 # Render Modes: # 0 = No render needed # 1 = Full render # 2 = Hotzone only needsRender = 1 def onResize(sig, action): global mainMenuList global currentMenuItemIndex mainRender(1, mainMenuList, currentMenuItemIndex) def generateLineText(text, textLength=None, paddingBefore=0, lineLength=64): result = \"\" for i in range(paddingBefore): result += \" \" textPrintableCharactersLength = textLength if (textPrintableCharactersLength) == None: textPrintableCharactersLength = len(text) result += text remainingSpace = lineLength - textPrintableCharactersLength for i in range(remainingSpace): result += \" \" return result def renderHotZone(term, renderType, menu, selection, hotzoneLocation, paddingBefore = 4): global paginationSize selectedTextLength = len(\"-> \") print(term.move(hotzoneLocation[0], hotzoneLocation[1])) if paginationStartIndex >= 1: print(term.center(\"{b} {uaf} {uaf}{uaf}{uaf} {ual} {b}\".format( b=specialChars[renderMode][\"borderVertical\"], uaf=specialChars[renderMode][\"upArrowFull\"], ual=specialChars[renderMode][\"upArrowLine\"] ))) else: print(term.center(commonEmptyLine(renderMode))) for (index, menuItem) in enumerate(menu): # Menu loop if index >= paginationStartIndex and index < paginationStartIndex + paginationSize: lineText = generateLineText(menuItem[0], paddingBefore=paddingBefore) # Menu highlight logic if index == selection: formattedLineText = '-> {t.blue_on_green}{title}{t.normal} <-'.format(t=term, title=menuItem[0]) paddedLineText = generateLineText(formattedLineText, textLength=len(menuItem[0]) + selectedTextLength, paddingBefore=paddingBefore - selectedTextLength) toPrint = paddedLineText else: toPrint = '{title}{t.normal}'.format(t=term, title=lineText) # ##### # Menu check render logic if menuItem[1][\"checked\"]: toPrint = \" (X) \" + toPrint else: toPrint = \" ( ) \" + toPrint toPrint = \"{bv} {toPrint} {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"], toPrint=toPrint) # Generate border toPrint = term.center(toPrint) # Center Text (All lines should have the same amount of printable characters) # ##### print(toPrint) if paginationStartIndex + paginationSize < len(menu): print(term.center(\"{b} {daf} {daf}{daf}{daf} {dal} {b}\".format( b=specialChars[renderMode][\"borderVertical\"], daf=specialChars[renderMode][\"downArrowFull\"], dal=specialChars[renderMode][\"downArrowLine\"] ))) else: print(term.center(commonEmptyLine(renderMode))) print(term.center(commonEmptyLine(renderMode))) print(term.center(commonEmptyLine(renderMode))) def mainRender(needsRender, menu, selection): global paginationStartIndex global paginationSize term = Terminal() if selection >= paginationStartIndex + paginationSize: paginationStartIndex = selection - (paginationSize - 1) + 1 needsRender = 1 if selection <= paginationStartIndex - 1: paginationStartIndex = selection needsRender = 1 if needsRender == 1: print(term.clear()) print(term.move_y(term.height // 16)) print(term.black_on_cornsilk4(term.center('IOTstack YourServices Password Options'))) print(\"\") print(term.center(commonTopBorder(renderMode))) print(term.center(commonEmptyLine(renderMode))) print(term.center(\"{bv} Select Password Option {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(commonEmptyLine(renderMode))) if needsRender >= 1: renderHotZone(term, needsRender, menu, selection, hotzoneLocation) if needsRender == 1: print(term.center(commonEmptyLine(renderMode))) if not hideHelpText: if term.height < 32: print(term.center(commonEmptyLine(renderMode))) print(term.center(\"{bv} Not enough vertical room to render controls help text {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(commonEmptyLine(renderMode))) else: print(term.center(commonEmptyLine(renderMode))) print(term.center(\"{bv} Controls: {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Space] to select option {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Up] and [Down] to move selection cursor {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [H] Show/hide this text {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Enter] to build and save option {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(\"{bv} [Escape] to cancel changes {bv}\".format(bv=specialChars[renderMode][\"borderVertical\"]))) print(term.center(commonEmptyLine(renderMode))) print(term.center(commonEmptyLine(renderMode))) print(term.center(commonBottomBorder(renderMode))) def runSelection(selection): import types if len(mainMenuList[selection]) > 1 and isinstance(mainMenuList[selection][1], types.FunctionType): mainMenuList[selection][1]() else: print(term.green_reverse('IOTstack Error: No function assigned to menu item: \"{}\"'.format(mainMenuList[selection][0]))) def isMenuItemSelectable(menu, index): if len(menu) > index: if len(menu[index]) > 1: if \"skip\" in menu[index][1] and menu[index][1][\"skip\"] == True: return False return True def loadOptionsMenu(): global mainMenuList mainMenuList.append([\"Use default password for this build\", { \"checked\": True }]) mainMenuList.append([\"Randomise database password for this build\", { \"checked\": False }]) mainMenuList.append([\"Randomise database password every build\", { \"checked\": False }]) mainMenuList.append([\"Do nothing\", { \"checked\": False }]) def checkMenuItem(selection): global mainMenuList for (index, menuItem) in enumerate(mainMenuList): mainMenuList[index][1][\"checked\"] = False mainMenuList[selection][1][\"checked\"] = True def saveOptions(): try: if not os.path.exists(serviceService): os.makedirs(serviceService, exist_ok=True) if os.path.exists(buildSettings): with open(r'%s' % buildSettings) as objBuildSettingsFile: yourServicesYamlBuildOptions = yaml.load(objBuildSettingsFile) else: yourServices = { \"version\": \"1\", \"application\": \"IOTstack\", \"service\": \"Your Service\", \"comment\": \"Your Service Build Options\" } yourServices[\"databasePasswordOption\"] = \"\" for (index, menuOption) in enumerate(mainMenuList): if menuOption[1][\"checked\"]: yourServices[\"databasePasswordOption\"] = menuOption[0] break with open(buildSettings, 'w') as outputFile: yaml.dump(yourServices, outputFile) except Exception as err: print(\"Error saving Your Services Password options\", currentServiceName) print(err) return False global hasRebuiltHardwareSelection hasRebuiltHardwareSelection = True return True def loadOptions(): try: if not os.path.exists(serviceService): os.makedirs(serviceService, exist_ok=True) if os.path.exists(buildSettings): with open(r'%s' % buildSettings) as objBuildSettingsFile: yourServicesYamlBuildOptions = yaml.load(objBuildSettingsFile) for (index, menuOption) in enumerate(mainMenuList): if menuOption[0] == yourServicesYamlBuildOptions[\"databasePasswordOption\"]: checkMenuItem(index) break except Exception as err: print(\"Error loading Your Services Password options\", currentServiceName) print(err) return False return True if __name__ == 'builtins': global signal term = Terminal() signal.signal(signal.SIGWINCH, onResize) loadOptionsMenu() loadOptions() with term.fullscreen(): menuNavigateDirection = 0 mainRender(needsRender, mainMenuList, currentMenuItemIndex) menuSelectionInProgress = True with term.cbreak(): while menuSelectionInProgress: menuNavigateDirection = 0 if not needsRender == 0: # Only rerender when changed to prevent flickering mainRender(needsRender, mainMenuList, currentMenuItemIndex) needsRender = 0 key = term.inkey(esc_delay=0.05) if key.is_sequence: if key.name == 'KEY_TAB': if paginationSize == paginationToggle[0]: paginationSize = paginationToggle[1] else: paginationSize = paginationToggle[0] mainRender(1, mainMenuList, currentMenuItemIndex) if key.name == 'KEY_DOWN': menuNavigateDirection += 1 if key.name == 'KEY_UP': menuNavigateDirection -= 1 if key.name == 'KEY_ENTER': if saveOptions(): return True else: print(\"Something went wrong. Try saving the list again.\") if key.name == 'KEY_ESCAPE': menuSelectionInProgress = False return True elif key: if key == ' ': # Space pressed checkMenuItem(currentMenuItemIndex) # Update checked list needsRender = 2 elif key == 'h': # H pressed if hideHelpText: hideHelpText = False else: hideHelpText = True mainRender(1, mainMenuList, currentMenuItemIndex) if menuNavigateDirection != 0: # If a direction was pressed, find next selectable item currentMenuItemIndex += menuNavigateDirection currentMenuItemIndex = currentMenuItemIndex % len(mainMenuList) needsRender = 2 while not isMenuItemSelectable(mainMenuList, currentMenuItemIndex): currentMenuItemIndex += menuNavigateDirection currentMenuItemIndex = currentMenuItemIndex % len(mainMenuList) return True return True originalSignalHandler = signal.getsignal(signal.SIGINT) main() signal.signal(signal.SIGWINCH, originalSignalHandler)","title":"Password settings screen"},{"location":"BuildStack-Services/","text":"Build Stack Services system This page explains how the build stack system works for developers. How to define a new service A service only requires 2 files: * service.yml - Contains data for docker-compose * build.py - Contains logic that the menu system uses. A basic service Inside the service.yml is where the service data for docker-compose is housed, for example: adminer: container_name: adminer image: adminer restart: unless-stopped ports: - \"9080:8080\" It is important that the service name match the directory that it's in - that means that the adminer service must be placed into a folder called adminer inside the ./.templates directory. Basic build code for service At the very least, the build.py requires the following code: #!/usr/bin/env python3 issues = {} # Returned issues dict buildHooks = {} # Options, and others hooks haltOnErrors = True # Main wrapper function. Required to make local vars work correctly def main(): global currentServiceName # Name of the current service # This lets the menu know whether to put \" >> Options \" or not # This function is REQUIRED. def checkForOptionsHook(): try: buildHooks[\"options\"] = callable(runOptionsMenu) except: buildHooks[\"options\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPreBuildHook(): try: buildHooks[\"preBuildHook\"] = callable(preBuild) except: buildHooks[\"preBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPostBuildHook(): try: buildHooks[\"postBuildHook\"] = callable(postBuild) except: buildHooks[\"postBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForRunChecksHook(): try: buildHooks[\"runChecksHook\"] = callable(runChecks) except: buildHooks[\"runChecksHook\"] = False return buildHooks return buildHooks # Entrypoint for execution if haltOnErrors: eval(toRun)() else: try: eval(toRun)() except: pass # This check isn't required, but placed here for debugging purposes global currentServiceName # Name of the current service if currentServiceName == 'adminer': # Make sure you update this. main() else: print(\"Error. '{}' Tried to run 'adminer' config\".format(currentServiceName)) This code doesn't have any port conflicting checking or menu code in it, and just allows the service to be built as is. The best way to learn on extending the functionality of the service's build script is to look at the other services' build scripts. You can also check out the advanced sections on adding menus and checking for issues for services though for a deeper explanation of specific situations. Basic code for a service that uses bash If Python isn't your thing, here's a code blob you can copy and paste. Just be sure to update the lines where the comments start with --- #!/usr/bin/env python3 issues = {} # Returned issues dict buildHooks = {} # Options, and others hooks haltOnErrors = True # Main wrapper function. Required to make local vars work correctly def main(): import subprocess global dockerComposeServicesYaml # The loaded memory YAML of all checked services global toRun # Switch for which function to run when executed global buildHooks # Where to place the options menu result global currentServiceName # Name of the current service global issues # Returned issues dict global haltOnErrors # Turn on to allow erroring from deps.consts import servicesDirectory, templatesDirectory, volumesDirectory, servicesFileName # runtime vars serviceVolume = volumesDirectory + currentServiceName # Unused in example serviceService = servicesDirectory + currentServiceName # Unused in example serviceTemplate = templatesDirectory + currentServiceName # This lets the menu know whether to put \" >> Options \" or not # This function is REQUIRED. def checkForOptionsHook(): try: buildHooks[\"options\"] = callable(runOptionsMenu) except: buildHooks[\"options\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPreBuildHook(): try: buildHooks[\"preBuildHook\"] = callable(preBuild) except: buildHooks[\"preBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPostBuildHook(): try: buildHooks[\"postBuildHook\"] = callable(postBuild) except: buildHooks[\"postBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForRunChecksHook(): try: buildHooks[\"runChecksHook\"] = callable(runChecks) except: buildHooks[\"runChecksHook\"] = False return buildHooks return buildHooks # This service will not check anything unless this is set # This function is optional, and will run each time the menu is rendered def runChecks(): checkForIssues() return [] # This function is optional, and will run after the docker-compose.yml file is written to disk. def postBuild(): return True # This function is optional, and will run just before the build docker-compose.yml code. def preBuild(): execComm = \"bash {currentServiceTemplate}/build.sh\".format(currentServiceTemplate=serviceTemplate) # --- You may want to change this print(\"[Wireguard]: \", execComm) # --- Ensure to update the service name with yours subprocess.call(execComm, shell=True) # This is where the magic happens return True # ##################################### # Supporting functions below # ##################################### def checkForIssues(): return True if haltOnErrors: eval(toRun)() else: try: eval(toRun)() except: pass # This check isn't required, but placed here for debugging purposes global currentServiceName # Name of the current service if currentServiceName == 'wireguard': # --- Ensure to update the service name with yours main() else: print(\"Error. '{}' Tried to run 'wireguard' config\".format(currentServiceName)) # --- Ensure to update the service name with yours","title":"Build Stack Services system"},{"location":"BuildStack-Services/#build-stack-services-system","text":"This page explains how the build stack system works for developers.","title":"Build Stack Services system"},{"location":"BuildStack-Services/#how-to-define-a-new-service","text":"A service only requires 2 files: * service.yml - Contains data for docker-compose * build.py - Contains logic that the menu system uses.","title":"How to define a new service"},{"location":"BuildStack-Services/#a-basic-service","text":"Inside the service.yml is where the service data for docker-compose is housed, for example: adminer: container_name: adminer image: adminer restart: unless-stopped ports: - \"9080:8080\" It is important that the service name match the directory that it's in - that means that the adminer service must be placed into a folder called adminer inside the ./.templates directory.","title":"A basic service"},{"location":"BuildStack-Services/#basic-build-code-for-service","text":"At the very least, the build.py requires the following code: #!/usr/bin/env python3 issues = {} # Returned issues dict buildHooks = {} # Options, and others hooks haltOnErrors = True # Main wrapper function. Required to make local vars work correctly def main(): global currentServiceName # Name of the current service # This lets the menu know whether to put \" >> Options \" or not # This function is REQUIRED. def checkForOptionsHook(): try: buildHooks[\"options\"] = callable(runOptionsMenu) except: buildHooks[\"options\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPreBuildHook(): try: buildHooks[\"preBuildHook\"] = callable(preBuild) except: buildHooks[\"preBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPostBuildHook(): try: buildHooks[\"postBuildHook\"] = callable(postBuild) except: buildHooks[\"postBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForRunChecksHook(): try: buildHooks[\"runChecksHook\"] = callable(runChecks) except: buildHooks[\"runChecksHook\"] = False return buildHooks return buildHooks # Entrypoint for execution if haltOnErrors: eval(toRun)() else: try: eval(toRun)() except: pass # This check isn't required, but placed here for debugging purposes global currentServiceName # Name of the current service if currentServiceName == 'adminer': # Make sure you update this. main() else: print(\"Error. '{}' Tried to run 'adminer' config\".format(currentServiceName)) This code doesn't have any port conflicting checking or menu code in it, and just allows the service to be built as is. The best way to learn on extending the functionality of the service's build script is to look at the other services' build scripts. You can also check out the advanced sections on adding menus and checking for issues for services though for a deeper explanation of specific situations.","title":"Basic build code for service"},{"location":"BuildStack-Services/#basic-code-for-a-service-that-uses-bash","text":"If Python isn't your thing, here's a code blob you can copy and paste. Just be sure to update the lines where the comments start with --- #!/usr/bin/env python3 issues = {} # Returned issues dict buildHooks = {} # Options, and others hooks haltOnErrors = True # Main wrapper function. Required to make local vars work correctly def main(): import subprocess global dockerComposeServicesYaml # The loaded memory YAML of all checked services global toRun # Switch for which function to run when executed global buildHooks # Where to place the options menu result global currentServiceName # Name of the current service global issues # Returned issues dict global haltOnErrors # Turn on to allow erroring from deps.consts import servicesDirectory, templatesDirectory, volumesDirectory, servicesFileName # runtime vars serviceVolume = volumesDirectory + currentServiceName # Unused in example serviceService = servicesDirectory + currentServiceName # Unused in example serviceTemplate = templatesDirectory + currentServiceName # This lets the menu know whether to put \" >> Options \" or not # This function is REQUIRED. def checkForOptionsHook(): try: buildHooks[\"options\"] = callable(runOptionsMenu) except: buildHooks[\"options\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPreBuildHook(): try: buildHooks[\"preBuildHook\"] = callable(preBuild) except: buildHooks[\"preBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForPostBuildHook(): try: buildHooks[\"postBuildHook\"] = callable(postBuild) except: buildHooks[\"postBuildHook\"] = False return buildHooks return buildHooks # This function is REQUIRED. def checkForRunChecksHook(): try: buildHooks[\"runChecksHook\"] = callable(runChecks) except: buildHooks[\"runChecksHook\"] = False return buildHooks return buildHooks # This service will not check anything unless this is set # This function is optional, and will run each time the menu is rendered def runChecks(): checkForIssues() return [] # This function is optional, and will run after the docker-compose.yml file is written to disk. def postBuild(): return True # This function is optional, and will run just before the build docker-compose.yml code. def preBuild(): execComm = \"bash {currentServiceTemplate}/build.sh\".format(currentServiceTemplate=serviceTemplate) # --- You may want to change this print(\"[Wireguard]: \", execComm) # --- Ensure to update the service name with yours subprocess.call(execComm, shell=True) # This is where the magic happens return True # ##################################### # Supporting functions below # ##################################### def checkForIssues(): return True if haltOnErrors: eval(toRun)() else: try: eval(toRun)() except: pass # This check isn't required, but placed here for debugging purposes global currentServiceName # Name of the current service if currentServiceName == 'wireguard': # --- Ensure to update the service name with yours main() else: print(\"Error. '{}' Tried to run 'wireguard' config\".format(currentServiceName)) # --- Ensure to update the service name with yours","title":"Basic code for a service that uses bash"},{"location":"Contributing-Services/","text":"Contributing a service to IOTstack On this page you can find information on how to contribute a service to IOTstack. We are generally very accepting of new services where they are useful. Keep in mind that if it is not IOTstack, selfhosted, or automation related we may not approve the PR. Services will grow over time, we may split up the buildstack menu into subsections or create filters to make organising all the services we provide easier to find. Checks service.yml file is correct build.py file is correct Service allows for changing external WUI port from Build Stack's options menu if service uses a HTTP/S port Use a default password, or allow the user to generate a random password for the service for initial installation. If the service asks to setup an account this can be ignored. Ensure Default Configs is updated with WUI port and username/password. Must detect port confilicts with other services on BuildStack Menu. Pre and Post hooks work with no errors. Does not require user to edit config files in order to get the service running. Ensure that your service can be backed up and restored without errors or data loss. Any configs that are required before getting the service running should be configured in the service's options menu (and a BuildStack menu Issue should be displayed if not). Fork the repo and push the changes to your fork. Create a cross repo PR for the mods to review. We may request additional changes from you. Follow up If your new service is approved and merged then congratulations! Please watch the Issues page on github over the next few days and weeks to see if any users have questions or issues with your new service. Links: * Default configs * Password configuration for Services * Build Stack Menu System * Coding a new service * IOTstack issues","title":"Contributing a service to IOTstack"},{"location":"Contributing-Services/#contributing-a-service-to-iotstack","text":"On this page you can find information on how to contribute a service to IOTstack. We are generally very accepting of new services where they are useful. Keep in mind that if it is not IOTstack, selfhosted, or automation related we may not approve the PR. Services will grow over time, we may split up the buildstack menu into subsections or create filters to make organising all the services we provide easier to find.","title":"Contributing a service to IOTstack"},{"location":"Contributing-Services/#checks","text":"service.yml file is correct build.py file is correct Service allows for changing external WUI port from Build Stack's options menu if service uses a HTTP/S port Use a default password, or allow the user to generate a random password for the service for initial installation. If the service asks to setup an account this can be ignored. Ensure Default Configs is updated with WUI port and username/password. Must detect port confilicts with other services on BuildStack Menu. Pre and Post hooks work with no errors. Does not require user to edit config files in order to get the service running. Ensure that your service can be backed up and restored without errors or data loss. Any configs that are required before getting the service running should be configured in the service's options menu (and a BuildStack menu Issue should be displayed if not). Fork the repo and push the changes to your fork. Create a cross repo PR for the mods to review. We may request additional changes from you.","title":"Checks"},{"location":"Contributing-Services/#follow-up","text":"If your new service is approved and merged then congratulations! Please watch the Issues page on github over the next few days and weeks to see if any users have questions or issues with your new service. Links: * Default configs * Password configuration for Services * Build Stack Menu System * Coding a new service * IOTstack issues","title":"Follow up"},{"location":"Custom/","text":"Custom services and overriding default settings for IOTstack You can specify modifcations to the docker-compose.yml file, including your own networks and custom containers/services. Create a file called compose-override.yml in the main directory, and place your modifications into it. These changes will be merged into the docker-compose.yml file next time you run the build script. The compose-override.yml file has been added to the .gitignore file, so it shouldn't be touched when upgrading IOTstack. It has been added to the backup script, and so will be included when you back up and restore IOTstack. Always test your backups though! New versions of IOTstack may break previous builds. How it works After the build process has been completed, a temporary docker compose file is created in the tmp directory. The script then checks if compose-override.yml exists: If it exists, then continue to step 3 If it does not exist, copy the temporary docker compose file to the main directory and rename it to docker-compose.yml . Using the yaml_merge.py script, merge both the compose-override.yml and the temporary docker compose file together; Using the temporary file as the default values and interating through each level of the yaml structure, check to see if the compose-override.yml has a value set. Output the final file to the main directory, calling it docker-compose.yml . A word of caution If you specify an override for a service, and then rebuild the docker-compose.yml file, but deselect the service from the list, then the YAML merging will still produce that override. For example, lets say NodeRed was selected to have have the following override specified in compose-override.yml : services: nodered: restart: always When rebuilding the menu, ensure to have NodeRed service always included because if it's no longer included, the only values showing in the final docker-compose.yml file for NodeRed will be the restart key and its value. Docker Compose will error with the following message: Service nodered has neither an image nor a build context specified. At least one must be provided. When attempting to bring the services up with docker-compose up -d . Either remove the override for NodeRed in compose-override.yml and rebuild the stack, or ensure that NodeRed is built with the stack to fix this. Examples Overriding default settings Lets assume you put the following into the compose-override.yml file: services: mosquitto: ports: - 1996:1996 - 9001:9001 Normally the mosquitto service would be built like this inside the docker-compose.yml file: version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1883:1883 - 9001:9001 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Take special note of the ports list. If you run the build script with the compose-override.yml file in place, and open up the final docker-compose.yml file, you will notice that the port list have been replaced with the ones you specified in the compose-override.yml file. version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1996:1996 - 9001:9001 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Do note that it will replace the entire list, if you were to specify services: mosquitto: ports: - 1996:1996 Then the final output will be: version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1996:1996 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Using env files instead of docker-compose variables If you need or prefer to use *.env files for docker-compose environment variables in a separate file instead of using overrides, you can do so like this: services: grafana: env_file: - ./services/grafana/grafana.env environment: This will remove the default environment variables set in the template, and tell docker-compose to use the variables specified in your file. It is not mandatory that the .env file be placed in the service's service directory, but is strongly suggested. Keep in mind the PostBuild Script functionality to automatically copy your .env files into their directories on successful build if you need to. Adding custom services Custom services can be added in a similar way to overriding default settings for standard services. Lets add a Minecraft and rcon server to IOTstack. Firstly, put the following into compose-override.yml : services: mosquitto: ports: - 1996:1996 - 9001:9001 minecraft: image: itzg/minecraft-server ports: - \"25565:25565\" volumes: - \"./volumes/minecraft:/data\" environment: EULA: \"TRUE\" TYPE: \"PAPER\" ENABLE_RCON: \"true\" RCON_PASSWORD: \"PASSWORD\" RCON_PORT: 28016 VERSION: \"1.15.2\" REPLACE_ENV_VARIABLES: \"TRUE\" ENV_VARIABLE_PREFIX: \"CFG_\" CFG_DB_HOST: \"http://localhost:3306\" CFG_DB_NAME: \"IOTstack Minecraft\" CFG_DB_PASSWORD_FILE: \"/run/secrets/db_password\" restart: unless-stopped rcon: image: itzg/rcon ports: - \"4326:4326\" - \"4327:4327\" volumes: - \"./volumes/rcon_data:/opt/rcon-web-admin/db\" secrets: db_password: file: ./db_password Then create the service directory that the new instance will use to store persistant data: mkdir -p ./volumes/minecraft and mkdir -p ./volumes/rcon_data Obviously you will need to give correct folder names depending on the volumes you specify for your custom services. If your new service doesn't require persistant storage, then you can skip this step. Then simply run the ./menu.sh command, and rebuild the stack with what ever services you had before. Using the Mosquitto example above, the final docker-compose.yml file will look like: version: '3.6' services: mosquitto: ports: - 1996:1996 - 9001:9001 container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: '1883' volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl minecraft: image: itzg/minecraft-server ports: - 25565:25565 volumes: - ./volumes/minecraft:/data environment: EULA: 'TRUE' TYPE: PAPER ENABLE_RCON: 'true' RCON_PASSWORD: PASSWORD RCON_PORT: 28016 VERSION: 1.15.2 REPLACE_ENV_VARIABLES: 'TRUE' ENV_VARIABLE_PREFIX: CFG_ CFG_DB_HOST: http://localhost:3306 CFG_DB_NAME: IOTstack Minecraft CFG_DB_PASSWORD_FILE: /run/secrets/db_password restart: unless-stopped rcon: image: itzg/rcon ports: - 4326:4326 - 4327:4327 volumes: - ./volumes/rcon_data:/opt/rcon-web-admin/db secrets: db_password: file: ./db_password Do note that the order of the YAML keys is not guaranteed.","title":"Custom services and overriding default settings for IOTstack"},{"location":"Custom/#custom-services-and-overriding-default-settings-for-iotstack","text":"You can specify modifcations to the docker-compose.yml file, including your own networks and custom containers/services. Create a file called compose-override.yml in the main directory, and place your modifications into it. These changes will be merged into the docker-compose.yml file next time you run the build script. The compose-override.yml file has been added to the .gitignore file, so it shouldn't be touched when upgrading IOTstack. It has been added to the backup script, and so will be included when you back up and restore IOTstack. Always test your backups though! New versions of IOTstack may break previous builds.","title":"Custom services and overriding default settings for IOTstack"},{"location":"Custom/#how-it-works","text":"After the build process has been completed, a temporary docker compose file is created in the tmp directory. The script then checks if compose-override.yml exists: If it exists, then continue to step 3 If it does not exist, copy the temporary docker compose file to the main directory and rename it to docker-compose.yml . Using the yaml_merge.py script, merge both the compose-override.yml and the temporary docker compose file together; Using the temporary file as the default values and interating through each level of the yaml structure, check to see if the compose-override.yml has a value set. Output the final file to the main directory, calling it docker-compose.yml .","title":"How it works"},{"location":"Custom/#a-word-of-caution","text":"If you specify an override for a service, and then rebuild the docker-compose.yml file, but deselect the service from the list, then the YAML merging will still produce that override. For example, lets say NodeRed was selected to have have the following override specified in compose-override.yml : services: nodered: restart: always When rebuilding the menu, ensure to have NodeRed service always included because if it's no longer included, the only values showing in the final docker-compose.yml file for NodeRed will be the restart key and its value. Docker Compose will error with the following message: Service nodered has neither an image nor a build context specified. At least one must be provided. When attempting to bring the services up with docker-compose up -d . Either remove the override for NodeRed in compose-override.yml and rebuild the stack, or ensure that NodeRed is built with the stack to fix this.","title":"A word of caution"},{"location":"Custom/#examples","text":"","title":"Examples"},{"location":"Custom/#overriding-default-settings","text":"Lets assume you put the following into the compose-override.yml file: services: mosquitto: ports: - 1996:1996 - 9001:9001 Normally the mosquitto service would be built like this inside the docker-compose.yml file: version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1883:1883 - 9001:9001 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Take special note of the ports list. If you run the build script with the compose-override.yml file in place, and open up the final docker-compose.yml file, you will notice that the port list have been replaced with the ones you specified in the compose-override.yml file. version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1996:1996 - 9001:9001 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Do note that it will replace the entire list, if you were to specify services: mosquitto: ports: - 1996:1996 Then the final output will be: version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1996:1996 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl","title":"Overriding default settings"},{"location":"Custom/#using-env-files-instead-of-docker-compose-variables","text":"If you need or prefer to use *.env files for docker-compose environment variables in a separate file instead of using overrides, you can do so like this: services: grafana: env_file: - ./services/grafana/grafana.env environment: This will remove the default environment variables set in the template, and tell docker-compose to use the variables specified in your file. It is not mandatory that the .env file be placed in the service's service directory, but is strongly suggested. Keep in mind the PostBuild Script functionality to automatically copy your .env files into their directories on successful build if you need to.","title":"Using env files instead of docker-compose variables"},{"location":"Custom/#adding-custom-services","text":"Custom services can be added in a similar way to overriding default settings for standard services. Lets add a Minecraft and rcon server to IOTstack. Firstly, put the following into compose-override.yml : services: mosquitto: ports: - 1996:1996 - 9001:9001 minecraft: image: itzg/minecraft-server ports: - \"25565:25565\" volumes: - \"./volumes/minecraft:/data\" environment: EULA: \"TRUE\" TYPE: \"PAPER\" ENABLE_RCON: \"true\" RCON_PASSWORD: \"PASSWORD\" RCON_PORT: 28016 VERSION: \"1.15.2\" REPLACE_ENV_VARIABLES: \"TRUE\" ENV_VARIABLE_PREFIX: \"CFG_\" CFG_DB_HOST: \"http://localhost:3306\" CFG_DB_NAME: \"IOTstack Minecraft\" CFG_DB_PASSWORD_FILE: \"/run/secrets/db_password\" restart: unless-stopped rcon: image: itzg/rcon ports: - \"4326:4326\" - \"4327:4327\" volumes: - \"./volumes/rcon_data:/opt/rcon-web-admin/db\" secrets: db_password: file: ./db_password Then create the service directory that the new instance will use to store persistant data: mkdir -p ./volumes/minecraft and mkdir -p ./volumes/rcon_data Obviously you will need to give correct folder names depending on the volumes you specify for your custom services. If your new service doesn't require persistant storage, then you can skip this step. Then simply run the ./menu.sh command, and rebuild the stack with what ever services you had before. Using the Mosquitto example above, the final docker-compose.yml file will look like: version: '3.6' services: mosquitto: ports: - 1996:1996 - 9001:9001 container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: '1883' volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl minecraft: image: itzg/minecraft-server ports: - 25565:25565 volumes: - ./volumes/minecraft:/data environment: EULA: 'TRUE' TYPE: PAPER ENABLE_RCON: 'true' RCON_PASSWORD: PASSWORD RCON_PORT: 28016 VERSION: 1.15.2 REPLACE_ENV_VARIABLES: 'TRUE' ENV_VARIABLE_PREFIX: CFG_ CFG_DB_HOST: http://localhost:3306 CFG_DB_NAME: IOTstack Minecraft CFG_DB_PASSWORD_FILE: /run/secrets/db_password restart: unless-stopped rcon: image: itzg/rcon ports: - 4326:4326 - 4327:4327 volumes: - ./volumes/rcon_data:/opt/rcon-web-admin/db secrets: db_password: file: ./db_password Do note that the order of the YAML keys is not guaranteed.","title":"Adding custom services"},{"location":"Default-Configs/","text":"Build Stack Default Passwords for Services Here you can find a list of the default configurations for IOTstack for quick referece. A word of caution While it is convienent to leave passwords and ports set to their factory value, for security reasons we strongly encourage you to use a randomly generated password for services that require passwords, and/or setup a reverse nginx proxy to require authentication before proxying to services. Only allowing connections originating from LAN or VPN is another way to help secure your services. Security requires a multi-pronged approach. Do note that the ports listed are not all of the ports containers use. They are mearly the WUI ports. List of defaults Service Name Default Username Default Password Default External HTTP/S WUI Port Multiple Passwords adminer none none 9080 No blynk_server none none 8180 No deconz none IOtSt4ckDec0nZ 8090 No diyhue none none 8070 No dozzle none none 8080 No espruinohub none none none No gitea none none 7920 No grafana none none 3000 No home_assistant none none 8123 No homebridge none none 4040 No influxdb none none none Yes mariadb mariadbuser IOtSt4ckmariaDbPw none Yes mosquitto none none none No motioneye none none 8765 No nextcloud none none 9321 No nodered nodered nodered 1880 No openhab none none 4050 No pihole none IOtSt4ckP1Hol3 8089 No plex none none none No portainer none none 9002 No portainer-ce none none 9001 No postgres postuser IOtSt4ckpostgresDbPw none Yes python none none none No rtl_433 none none none No tasmoadmin none none 8088 No telegraf none none none No timescaledb timescaleuser IOtSt4ckTim3Scale none No transmission none none 9091 No webthingsio_gateway none none 4060 No zigbee2mqtt none none none No zigbee2mqtt_assistant none none none No","title":"Build Stack Default Passwords for Services"},{"location":"Default-Configs/#build-stack-default-passwords-for-services","text":"Here you can find a list of the default configurations for IOTstack for quick referece.","title":"Build Stack Default Passwords for Services"},{"location":"Default-Configs/#a-word-of-caution","text":"While it is convienent to leave passwords and ports set to their factory value, for security reasons we strongly encourage you to use a randomly generated password for services that require passwords, and/or setup a reverse nginx proxy to require authentication before proxying to services. Only allowing connections originating from LAN or VPN is another way to help secure your services. Security requires a multi-pronged approach. Do note that the ports listed are not all of the ports containers use. They are mearly the WUI ports.","title":"A word of caution"},{"location":"Default-Configs/#list-of-defaults","text":"Service Name Default Username Default Password Default External HTTP/S WUI Port Multiple Passwords adminer none none 9080 No blynk_server none none 8180 No deconz none IOtSt4ckDec0nZ 8090 No diyhue none none 8070 No dozzle none none 8080 No espruinohub none none none No gitea none none 7920 No grafana none none 3000 No home_assistant none none 8123 No homebridge none none 4040 No influxdb none none none Yes mariadb mariadbuser IOtSt4ckmariaDbPw none Yes mosquitto none none none No motioneye none none 8765 No nextcloud none none 9321 No nodered nodered nodered 1880 No openhab none none 4050 No pihole none IOtSt4ckP1Hol3 8089 No plex none none none No portainer none none 9002 No portainer-ce none none 9001 No postgres postuser IOtSt4ckpostgresDbPw none Yes python none none none No rtl_433 none none none No tasmoadmin none none 8088 No telegraf none none none No timescaledb timescaleuser IOtSt4ckTim3Scale none No transmission none none 9091 No webthingsio_gateway none none 4060 No zigbee2mqtt none none none No zigbee2mqtt_assistant none none none No","title":"List of defaults"},{"location":"Docker-commands/","text":"Docker commands Aliases I've added bash aliases for stopping and starting the stack. They can be installed in the docker commands menu. These commands no longer need to be executed from the IOTstack directory and can be executed in any directory alias iotstack_up=\"docker-compose -f ~/IOTstack/docker-compose.yml up -d\" alias iotstack_down=\"docker-compose -f ~/IOTstack/docker-compose.yml down\" alias iotstack_start=\"docker-compose -f ~/IOTstack/docker-compose.yml start\" alias iotstack_stop=\"docker-compose -f ~/IOTstack/docker-compose.yml stop\" alias iotstack_update=\"docker-compose -f ~/IOTstack/docker-compose.yml pull\" alias iotstack_build=\"docker-compose -f ~/IOTstack/docker-compose.yml build\" You can now type iotstack_up , they even accept additional parameters iotstack_stop portainer","title":"Docker commands"},{"location":"Docker-commands/#docker-commands","text":"","title":"Docker commands"},{"location":"Docker-commands/#aliases","text":"I've added bash aliases for stopping and starting the stack. They can be installed in the docker commands menu. These commands no longer need to be executed from the IOTstack directory and can be executed in any directory alias iotstack_up=\"docker-compose -f ~/IOTstack/docker-compose.yml up -d\" alias iotstack_down=\"docker-compose -f ~/IOTstack/docker-compose.yml down\" alias iotstack_start=\"docker-compose -f ~/IOTstack/docker-compose.yml start\" alias iotstack_stop=\"docker-compose -f ~/IOTstack/docker-compose.yml stop\" alias iotstack_update=\"docker-compose -f ~/IOTstack/docker-compose.yml pull\" alias iotstack_build=\"docker-compose -f ~/IOTstack/docker-compose.yml build\" You can now type iotstack_up , they even accept additional parameters iotstack_stop portainer","title":"Aliases"},{"location":"Getting-Started/","text":"Getting Started introduction to IOTstack - videos Andreas Spiess Video #295: Raspberry Pi Server based on Docker, with VPN, Dropbox backup, Influx, Grafana, etc: IOTstack Andreas Spiess Video #352: Raspberry Pi4 Home Automation Server (incl. Docker, OpenHAB, HASSIO, NextCloud) assumptions IOTstack makes the following assumptions: Your hardware is a Raspberry Pi (typically a 3B+ or 4B) It has a reasonably-recent version of Raspberry Pi OS (aka \"Raspbian\" installed) which has been kept up-to-date with: $ sudo apt update $ sudo apt upgrade -y You are logged-in as the user \"pi\" User \"pi\" has the user ID 1000 The home directory for user \"pi\" is /home/pi/ IOTstack is installed at /home/pi/IOTstack (with that exact spelling). The first five assumptions are Raspberry Pi defaults on a clean installation. The sixth is what you get if you follow these instructions faithfully. Please don't read these assumptions as saying that IOTstack will not run on other hardware, other operating systems, or as a different user. It is just that IOTstack gets most of its testing under these conditions. The further you get from these implicit assumptions, the more your mileage may vary. new installation automatic (recommended) Install curl : $ sudo apt install -y curl Run the following command: $ curl -fsSL https://raw.githubusercontent.com/SensorsIot/IOTstack/master/install.sh | bash Run the menu and choose your containers: $ cd ~/IOTstack $ ./menu.sh Bring up your stack: $ cd ~/IOTstack $ docker-compose up -d manual Install git : $ sudo apt install -y git Clone IOTstack: If you want \"new menu\": $ git clone https://github.com/SensorsIot/IOTstack.git ~/IOTstack If you prefer \"old menu\": $ git clone -b old-menu https://github.com/SensorsIot/IOTstack.git ~/IOTstack Run the menu and choose your containers: $ cd ~/IOTstack $ ./menu.sh Note: If you are running \"old menu\" for the first time, you will be guided to \"Install Docker\". That will end in a reboot, after which you should re-enter the menu and choose your containers. Bring up your stack: $ cd ~/IOTstack $ docker-compose up -d scripted If you prefer to automate your installations using scripts, see: Installing Docker for IOTstack . migrating from the old repo (gcgarner)? If you are still running on gcgarner/IOTstack and need to migrate to SensorsIot/IOTstack, see: Migrating IOTstack from gcgarner to SensorsIot . recommended system patch Run the following commands: $ sudo bash -c '[ $(egrep -c \"^allowinterfaces eth0,wlan0\" /etc/dhcpcd.conf) -eq 0 ] && echo \"allowinterfaces eth0,wlan0\" >> /etc/dhcpcd.conf' $ sudo reboot See Issue 219 and Issue 253 for more information. a word about the sudo command Many first-time users of IOTstack get into difficulty by misusing the sudo command. The problem is best understood by example. In the following, you would expect ~ (tilde) to expand to /home/pi . It does: $ echo ~/IOTstack /home/pi/IOTstack The command below sends the same echo command to bash for execution. This is what happens when you type the name of a shell script. You get a new instance of bash to run the script: $ bash -c 'echo ~/IOTstack' /home/pi/IOTstack Same answer. Again, this is what you expect. But now try it with sudo on the front: $ sudo bash -c 'echo ~/IOTstack' /root/IOTstack Different answer. It is different because sudo means \"become root, and then run the command\". The process of becoming root changes the home directory, and that changes the definition of ~ . Any script designed for working with IOTstack assumes ~ (or the equivalent $HOME variable) expands to /home/pi . That assumption is invalidated if the script is run by sudo . Of necessity, any script designed for working with IOTstack will have to invoke sudo inside the script when it is required . You do not need to second-guess the script's designer. Please try to minimise your use of sudo when you are working with IOTstack. Here are some rules of thumb: Is what you are about to run a script? If yes, check whether the script already contains sudo commands. Using menu.sh as the example: $ grep -c 'sudo' ~/IOTstack/menu.sh 28 There are numerous uses of sudo within menu.sh . That means the designer thought about when sudo was needed. Did the command you just executed work without sudo ? Note the emphasis on the past tense. If yes, then your work is done. If no, and the error suggests elevated privileges are necessary, then re-execute the last command like this: $ sudo !! It takes time, patience and practice to learn when sudo is actually needed. Over-using sudo out of habit, or because you were following a bad example you found on the web, is a very good way to find that you have created so many problems for yourself that will need to reinstall your IOTstack. Please err on the side of caution! the IOTstack menu The menu is used to install Docker and then build the docker-compose.yml file which is necessary for starting the stack. The menu is only an aid. It is a good idea to learn the docker and docker-compose commands if you plan on using Docker in the long run. menu item: Install Docker (old menu only) Please do not try to install docker and docker-compose via sudo apt install . There's more to it than that. Docker needs to be installed by menu.sh . The menu will prompt you to install docker if it detects that docker is not already installed. You can manually install it from within the Native Installs menu: $ cd ~/IOTstack $ ./menu.sh Select \"Native Installs\" Select \"Install Docker and Docker-Compose\" Follow the prompts. The process finishes by asking you to reboot. Do that! Note: New menu (master branch) automates this step. menu item: Build Stack docker-compose uses a docker-compose.yml file to configure all your services. The docker-compose.yml file is created by the menu: $ cd ~/IOTstack $ ./menu.sh Select \"Build Stack\" Follow the on-screen prompts and select the containers you need. The best advice we can give is \"start small\". Limit yourself to the core containers you actually need (eg Mosquitto, Node-RED, InfluxDB, Grafana, Portainer). You can always add more containers later. Some users have gone overboard with their initial selections and have run into what seem to be Raspberry Pi OS limitations. Key point: If you are running \"new menu\" (master branch) and you select Node-RED, you must press the right-arrow and choose at least one add-on node. If you skip this step, Node-RED will not build properly. Old menu forces you to choose add-on nodes for Node-RED. The process finishes by asking you to bring up the stack: $ cd ~/IOTstack $ docker-compose up -d The first time you run up the stack docker will download all the images from DockerHub. How long this takes will depend on how many containers you selected and the speed of your internet connection. Some containers also need to be built locally. Node-RED is an example. Depending on the Node-RED nodes you select, building the image can also take a very long time. This is especially true if you select the SQLite node. Be patient (and ignore the huge number of warnings). menu item: Docker commands The commands in this menu execute shell scripts in the root of the project. other menu items The old and new menus differ in the options they offer. You should come back and explore them once your stack is built and running. switching menus current menu (master branch) $ cd ~/IOTstack/ $ git pull $ git checkout master $ ./menu.sh old menu (old-menu branch) $ cd ~/IOTstack/ $ git pull $ git checkout old-menu $ ./menu.sh experimental branch Switch to the experimental branch to try the latest and greatest features. $ cd ~/IOTstack/ $ git pull $ git checkout experimental $ ./menu.sh Notes: Please make sure you have a good backup before you start. The experimental branch may be broken, or may break your setup. Please report any issues. Remember: you can switch git branches as much as you like without breaking anything. simply launching the menu (any version) won't change anything providing you exit before letting the menu complete. running the menu to completion will change your docker-compose.yml and supporting structures in ~/IOTstack/services . running docker-compose up -d will change your running containers. The way back is to take down your stack, restore a backup, and bring up your stack again. useful commands: docker \\& docker-compose Handy rules: docker commands can be executed from anywhere, but docker-compose commands need to be executed from within ~/IOTstack starting your IOTstack To start the stack: $ cd ~/IOTstack $ docker-compose up -d Once the stack has been brought up, it will stay up until you take it down. This includes shutdowns and reboots of your Raspberry Pi. If you do not want the stack to start automatically after a reboot, you need to stop the stack before you issue the reboot command. logging journald errors If you get docker logging error like: Cannot create container for service [service name here]: unknown log opt 'max-file' for journald log driver Run the command: sudo nano /etc/docker/daemon.json change: \"log-driver\": \"journald\", to: \"log-driver\": \"json-file\", Logging limits were added to prevent Docker using up lots of RAM if log2ram is enabled, or SD cards being filled with log data and degraded from unnecessary IO. See Docker Logging configurations You can also turn logging off or set it to use another option for any service by using the IOTstack docker-compose-override.yml file mentioned at IOTstack/Custom . starting an individual container To start the stack: $ cd ~/IOTstack $ docker-compose up -d \u00abcontainer\u00bb stopping your IOTstack Stopping aka \"downing\" the stack stops and deletes all containers, and removes the internal network: $ cd ~/IOTstack $ docker-compose down To stop the stack without removing containers, run: $ cd ~/IOTstack $ docker-compose stop stopping an individual container stop can also be used to stop individual containers, like this: $ cd ~/IOTstack $ docker-compose stop \u00abcontainer\u00bb This puts the container in a kind of suspended animation. You can resume the container with $ cd ~/IOTstack $ docker-compose start \u00abcontainer\u00bb There is no equivalent of down for a single container. It needs two commands: $ cd ~/IOTstack $ docker-compose stop \u00abcontainer\u00bb $ docker-compose rm -f \u00abcontainer\u00bb To reactivate a container which has been stopped and removed: $ cd ~/IOTstack $ docker-compose up -d \u00abcontainer\u00bb checking container status You can check the status of containers with: $ docker ps or $ cd ~/IOTstack $ docker-compose ps viewing container logs You can inspect the logs of most containers like this: $ docker logs \u00abcontainer\u00bb for example: $ docker logs nodered You can also follow a container's log as new entries are added by using the -f flag: $ docker logs -f nodered Terminate with a Control+C. Note that restarting a container will also terminate a followed log. restarting a container You can restart a container in several ways: $ cd ~/IOTstack $ docker-compose restart \u00abcontainer\u00bb This kind of restart is the least-powerful form of restart. A good way to think of it is \"the container is only restarted, it is not rebuilt\". If you change a docker-compose.yml setting for a container and/or an environment variable file referenced by docker-compose.yml then a restart is usually not enough to bring the change into effect. You need to make docker-compose notice the change: $ cd ~/IOTstack $ docker-compose up -d \u00abcontainer\u00bb This type of \"restart\" rebuilds the container. Alternatively, to force a container to rebuild (without changing either docker-compose.yml or an environment variable file): $ cd ~/IOTstack $ docker-compose stop \u00abcontainer\u00bb $ docker-compose rm -f \u00abcontainer\u00bb $ docker-compose up -d \u00abcontainer\u00bb See also updating images built from Dockerfiles if you need to force docker-compose to notice a change to a Dockerfile. persistent data Docker allows a container's designer to map folders inside a container to a folder on your disk (SD, SSD, HD). This is done with the \"volumes\" key in docker-compose.yml . Consider the following snippet for Node-RED: volumes: - ./volumes/nodered/data:/data You read this as two paths, separated by a colon. The: external path is ./volumes/nodered/data internal path is /data In this context, the leading \".\" means \"the folder containing docker-compose.yml \", so the external path is actually: ~/IOTstack/volumes/nodered/data If a process running inside the container modifies any file or folder within: /data the change is mirrored outside the container at the same relative path within: ~/IOTstack/volumes/nodered/data The same is true in reverse. Any change made to any file or folder outside the container within: ~/IOTstack/volumes/nodered/data is mirrored at the same relative path inside the container at: /data deleting persistent data If you need a \"clean slate\" for a container, you can delete its volumes. Using InfluxDB as an example: $ cd ~/IOTstack $ docker-compose stop influxdb $ sudo rm -rf ./volumes/influxdb $ docker-compose up -d influxdb When docker-compose tries to bring up InfluxDB, it will notice this volume mapping in docker-compose.yml : volumes: - ./volumes/influxdb/data:/var/lib/influxdb and check to see whether ./volumes/influxdb/data is present. Finding it not there, it does the equivalent of: $ sudo mkdir -p ./volumes/influxdb/data When InfluxDB starts, it sees that the folder on right-hand-side of the volumes mapping ( /var/lib/influxdb ) is empty and initialises new databases. This is how most containers behave. But there are exceptions. A good example of an exception is Mosquitto which does not re-initialise correctly so you should avoid removing its persistent store. stack maintenance update Raspberry Pi OS You should keep your Raspberry Pi up-to-date. Despite the word \"container\" suggesting that containers are fully self-contained, they sometimes depend on operating system components (\"WireGuard\" is an example). $ sudo apt update $ sudo apt upgrade -y git pull Although the menu will generally do this for you, it does not hurt to keep your local copy of the IOTstack repository in sync with the master version on GitHub. $ cd ~/IOTstack $ git pull container image updates There are two kinds of images used in IOTstack: Those not built using Dockerfiles (the majority) Those built using Dockerfiles (special cases) A Dockerfile is a set of instructions designed to customise an image before it is instantiated to become a running container. The easiest way to work out which type of image you are looking at is to inspect the container's service definition in your docker-compose.yml file. If the service definition contains the: image: keyword then the image is not built using a Dockerfile. build: keyword then the image is built using a Dockerfile. updating images not built from Dockerfiles If new versions of this type of image become available on DockerHub, your local IOTstack copies can be updated by a pull command: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune The pull downloads any new images. It does this without disrupting the running stack. The up -d notices any newly-downloaded images, builds new containers, and swaps old-for-new. There is barely any downtime for affected containers. updating images built from Dockerfiles Containers built using Dockerfiles have a two-step process: A base image is downloaded from from DockerHub; and then The Dockerfile \"runs\" to build a local image. Node-RED is a good example of a container built from a Dockerfile. The Dockerfile defines some (or possibly all) of your add-on nodes, such as those needed for InfluxDB or Tasmota. There are two separate update situations that you need to consider: If your Dockerfile changes; or If a newer base image appears on DockerHub Node-RED also provides a good example of why your Dockerfile might change: if you decide to add or remove add-on nodes. Note: You can also add nodes to Node-RED using Manage Palette. when Dockerfile changes ( local image only) When your Dockerfile changes, you need to rebuild like this: $ cd ~/IOTstack $ docker-compose up --build -d \u00abcontainer\u00bb $ docker system prune This only rebuilds the local image and, even then, only if docker-compose senses a material change to the Dockerfile. If you are trying to force the inclusion of a later version of an add-on node, you need to treat it like a DockerHub update . Key point: The base image is not affected by this type of update. Note: You can also use this type of build if you get an error after modifying Node-RED's environment: $ cd ~/IOTstack $ docker-compose up --build -d nodered when DockerHub updates ( base and local images) When a newer version of the base image appears on DockerHub, you need to rebuild like this: $ cd ~/IOTstack $ docker-compose build --no-cache --pull \u00abcontainer\u00bb $ docker-compose up -d \u00abcontainer\u00bb $ docker system prune This causes DockerHub to be checked for the later version of the base image, downloading it as needed. Then, the Dockerfile is run to produce a new local image. The Dockerfile run happens even if a new base image was not downloaded in the previous step. deleting unused images As your system evolves and new images come down from DockerHub, you may find that more disk space is being occupied than you expected. Try running: $ docker system prune This recovers anything no longer in use. If you add a container via menu.sh and later remove it (either manually or via menu.sh ), the associated images(s) will probably persist. You can check which images are installed via: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE influxdb latest 1361b14bf545 5 days ago 264MB grafana/grafana latest b9dfd6bb8484 13 days ago 149MB iotstack_nodered latest 21d5a6b7b57b 2 weeks ago 540MB portainer/portainer-ce latest 5526251cc61f 5 weeks ago 163MB eclipse-mosquitto latest 4af162db6b4c 6 weeks ago 8.65MB nodered/node-red latest fa3bc6f20464 2 months ago 376MB portainer/portainer latest dbf28ba50432 2 months ago 62.5MB Both \"Portainer CE\" and \"Portainer\" are in that list. Assuming \"Portainer\" is no longer in use, it can be removed by using either its repository name or its Image ID. In other words, the following two commands are synonyms: $ docker rmi portainer/portainer $ docker rmi dbf28ba50432 In general, you can use the repository name to remove an image but the Image ID is sometimes needed. The most common situation where you are likely to need the Image ID is after an image has been updated on DockerHub and pulled down to your Raspberry Pi. You will find two containers with the same name. One will be tagged \"latest\" (the running version) while the other will be tagged \" \" (the prior version). You use the Image ID to resolve the ambiguity. the nuclear option - use with caution You can use Git to delete all files and folders to return your folder to the freshly cloned state. Warning: YOU WILL LOSE ALL YOUR DATA . $ cd ~/IOTstack $ sudo git clean -d -x -f This is probably the only time you should ever need to use sudo in conjunction with git for IOTstack. This is not recoverable!","title":"Getting Started"},{"location":"Getting-Started/#getting-started","text":"","title":"Getting Started"},{"location":"Getting-Started/#introduction-to-iotstack-videos","text":"Andreas Spiess Video #295: Raspberry Pi Server based on Docker, with VPN, Dropbox backup, Influx, Grafana, etc: IOTstack Andreas Spiess Video #352: Raspberry Pi4 Home Automation Server (incl. Docker, OpenHAB, HASSIO, NextCloud)","title":" introduction to IOTstack - videos "},{"location":"Getting-Started/#assumptions","text":"IOTstack makes the following assumptions: Your hardware is a Raspberry Pi (typically a 3B+ or 4B) It has a reasonably-recent version of Raspberry Pi OS (aka \"Raspbian\" installed) which has been kept up-to-date with: $ sudo apt update $ sudo apt upgrade -y You are logged-in as the user \"pi\" User \"pi\" has the user ID 1000 The home directory for user \"pi\" is /home/pi/ IOTstack is installed at /home/pi/IOTstack (with that exact spelling). The first five assumptions are Raspberry Pi defaults on a clean installation. The sixth is what you get if you follow these instructions faithfully. Please don't read these assumptions as saying that IOTstack will not run on other hardware, other operating systems, or as a different user. It is just that IOTstack gets most of its testing under these conditions. The further you get from these implicit assumptions, the more your mileage may vary.","title":" assumptions "},{"location":"Getting-Started/#new-installation","text":"","title":" new installation "},{"location":"Getting-Started/#automatic-recommended","text":"Install curl : $ sudo apt install -y curl Run the following command: $ curl -fsSL https://raw.githubusercontent.com/SensorsIot/IOTstack/master/install.sh | bash Run the menu and choose your containers: $ cd ~/IOTstack $ ./menu.sh Bring up your stack: $ cd ~/IOTstack $ docker-compose up -d","title":" automatic (recommended) "},{"location":"Getting-Started/#manual","text":"Install git : $ sudo apt install -y git Clone IOTstack: If you want \"new menu\": $ git clone https://github.com/SensorsIot/IOTstack.git ~/IOTstack If you prefer \"old menu\": $ git clone -b old-menu https://github.com/SensorsIot/IOTstack.git ~/IOTstack Run the menu and choose your containers: $ cd ~/IOTstack $ ./menu.sh Note: If you are running \"old menu\" for the first time, you will be guided to \"Install Docker\". That will end in a reboot, after which you should re-enter the menu and choose your containers. Bring up your stack: $ cd ~/IOTstack $ docker-compose up -d","title":" manual "},{"location":"Getting-Started/#scripted","text":"If you prefer to automate your installations using scripts, see: Installing Docker for IOTstack .","title":" scripted "},{"location":"Getting-Started/#migrating-from-the-old-repo-gcgarner","text":"If you are still running on gcgarner/IOTstack and need to migrate to SensorsIot/IOTstack, see: Migrating IOTstack from gcgarner to SensorsIot .","title":" migrating from the old repo (gcgarner)? "},{"location":"Getting-Started/#recommended-system-patch","text":"Run the following commands: $ sudo bash -c '[ $(egrep -c \"^allowinterfaces eth0,wlan0\" /etc/dhcpcd.conf) -eq 0 ] && echo \"allowinterfaces eth0,wlan0\" >> /etc/dhcpcd.conf' $ sudo reboot See Issue 219 and Issue 253 for more information.","title":" recommended system patch "},{"location":"Getting-Started/#a-word-about-the-sudo-command","text":"Many first-time users of IOTstack get into difficulty by misusing the sudo command. The problem is best understood by example. In the following, you would expect ~ (tilde) to expand to /home/pi . It does: $ echo ~/IOTstack /home/pi/IOTstack The command below sends the same echo command to bash for execution. This is what happens when you type the name of a shell script. You get a new instance of bash to run the script: $ bash -c 'echo ~/IOTstack' /home/pi/IOTstack Same answer. Again, this is what you expect. But now try it with sudo on the front: $ sudo bash -c 'echo ~/IOTstack' /root/IOTstack Different answer. It is different because sudo means \"become root, and then run the command\". The process of becoming root changes the home directory, and that changes the definition of ~ . Any script designed for working with IOTstack assumes ~ (or the equivalent $HOME variable) expands to /home/pi . That assumption is invalidated if the script is run by sudo . Of necessity, any script designed for working with IOTstack will have to invoke sudo inside the script when it is required . You do not need to second-guess the script's designer. Please try to minimise your use of sudo when you are working with IOTstack. Here are some rules of thumb: Is what you are about to run a script? If yes, check whether the script already contains sudo commands. Using menu.sh as the example: $ grep -c 'sudo' ~/IOTstack/menu.sh 28 There are numerous uses of sudo within menu.sh . That means the designer thought about when sudo was needed. Did the command you just executed work without sudo ? Note the emphasis on the past tense. If yes, then your work is done. If no, and the error suggests elevated privileges are necessary, then re-execute the last command like this: $ sudo !! It takes time, patience and practice to learn when sudo is actually needed. Over-using sudo out of habit, or because you were following a bad example you found on the web, is a very good way to find that you have created so many problems for yourself that will need to reinstall your IOTstack. Please err on the side of caution!","title":" a word about the sudo command "},{"location":"Getting-Started/#the-iotstack-menu","text":"The menu is used to install Docker and then build the docker-compose.yml file which is necessary for starting the stack. The menu is only an aid. It is a good idea to learn the docker and docker-compose commands if you plan on using Docker in the long run.","title":" the IOTstack menu "},{"location":"Getting-Started/#menu-item-install-docker-old-menu-only","text":"Please do not try to install docker and docker-compose via sudo apt install . There's more to it than that. Docker needs to be installed by menu.sh . The menu will prompt you to install docker if it detects that docker is not already installed. You can manually install it from within the Native Installs menu: $ cd ~/IOTstack $ ./menu.sh Select \"Native Installs\" Select \"Install Docker and Docker-Compose\" Follow the prompts. The process finishes by asking you to reboot. Do that! Note: New menu (master branch) automates this step.","title":" menu item: Install Docker  (old menu only)"},{"location":"Getting-Started/#menu-item-build-stack","text":"docker-compose uses a docker-compose.yml file to configure all your services. The docker-compose.yml file is created by the menu: $ cd ~/IOTstack $ ./menu.sh Select \"Build Stack\" Follow the on-screen prompts and select the containers you need. The best advice we can give is \"start small\". Limit yourself to the core containers you actually need (eg Mosquitto, Node-RED, InfluxDB, Grafana, Portainer). You can always add more containers later. Some users have gone overboard with their initial selections and have run into what seem to be Raspberry Pi OS limitations. Key point: If you are running \"new menu\" (master branch) and you select Node-RED, you must press the right-arrow and choose at least one add-on node. If you skip this step, Node-RED will not build properly. Old menu forces you to choose add-on nodes for Node-RED. The process finishes by asking you to bring up the stack: $ cd ~/IOTstack $ docker-compose up -d The first time you run up the stack docker will download all the images from DockerHub. How long this takes will depend on how many containers you selected and the speed of your internet connection. Some containers also need to be built locally. Node-RED is an example. Depending on the Node-RED nodes you select, building the image can also take a very long time. This is especially true if you select the SQLite node. Be patient (and ignore the huge number of warnings).","title":" menu item: Build Stack "},{"location":"Getting-Started/#menu-item-docker-commands","text":"The commands in this menu execute shell scripts in the root of the project.","title":" menu item: Docker commands "},{"location":"Getting-Started/#other-menu-items","text":"The old and new menus differ in the options they offer. You should come back and explore them once your stack is built and running.","title":" other menu items "},{"location":"Getting-Started/#switching-menus","text":"","title":" switching menus "},{"location":"Getting-Started/#current-menu-master-branch","text":"$ cd ~/IOTstack/ $ git pull $ git checkout master $ ./menu.sh","title":" current menu (master branch) "},{"location":"Getting-Started/#old-menu-old-menu-branch","text":"$ cd ~/IOTstack/ $ git pull $ git checkout old-menu $ ./menu.sh","title":" old menu (old-menu branch)"},{"location":"Getting-Started/#experimental-branch","text":"Switch to the experimental branch to try the latest and greatest features. $ cd ~/IOTstack/ $ git pull $ git checkout experimental $ ./menu.sh Notes: Please make sure you have a good backup before you start. The experimental branch may be broken, or may break your setup. Please report any issues. Remember: you can switch git branches as much as you like without breaking anything. simply launching the menu (any version) won't change anything providing you exit before letting the menu complete. running the menu to completion will change your docker-compose.yml and supporting structures in ~/IOTstack/services . running docker-compose up -d will change your running containers. The way back is to take down your stack, restore a backup, and bring up your stack again.","title":" experimental branch "},{"location":"Getting-Started/#useful-commands-docker-docker-compose","text":"Handy rules: docker commands can be executed from anywhere, but docker-compose commands need to be executed from within ~/IOTstack","title":" useful commands: docker \\&amp; docker-compose "},{"location":"Getting-Started/#starting-your-iotstack","text":"To start the stack: $ cd ~/IOTstack $ docker-compose up -d Once the stack has been brought up, it will stay up until you take it down. This includes shutdowns and reboots of your Raspberry Pi. If you do not want the stack to start automatically after a reboot, you need to stop the stack before you issue the reboot command.","title":" starting your IOTstack "},{"location":"Getting-Started/#logging-journald-errors","text":"If you get docker logging error like: Cannot create container for service [service name here]: unknown log opt 'max-file' for journald log driver Run the command: sudo nano /etc/docker/daemon.json change: \"log-driver\": \"journald\", to: \"log-driver\": \"json-file\", Logging limits were added to prevent Docker using up lots of RAM if log2ram is enabled, or SD cards being filled with log data and degraded from unnecessary IO. See Docker Logging configurations You can also turn logging off or set it to use another option for any service by using the IOTstack docker-compose-override.yml file mentioned at IOTstack/Custom .","title":" logging journald errors "},{"location":"Getting-Started/#starting-an-individual-container","text":"To start the stack: $ cd ~/IOTstack $ docker-compose up -d \u00abcontainer\u00bb","title":" starting an individual container "},{"location":"Getting-Started/#stopping-your-iotstack","text":"Stopping aka \"downing\" the stack stops and deletes all containers, and removes the internal network: $ cd ~/IOTstack $ docker-compose down To stop the stack without removing containers, run: $ cd ~/IOTstack $ docker-compose stop","title":" stopping your IOTstack "},{"location":"Getting-Started/#stopping-an-individual-container","text":"stop can also be used to stop individual containers, like this: $ cd ~/IOTstack $ docker-compose stop \u00abcontainer\u00bb This puts the container in a kind of suspended animation. You can resume the container with $ cd ~/IOTstack $ docker-compose start \u00abcontainer\u00bb There is no equivalent of down for a single container. It needs two commands: $ cd ~/IOTstack $ docker-compose stop \u00abcontainer\u00bb $ docker-compose rm -f \u00abcontainer\u00bb To reactivate a container which has been stopped and removed: $ cd ~/IOTstack $ docker-compose up -d \u00abcontainer\u00bb","title":" stopping an individual container "},{"location":"Getting-Started/#checking-container-status","text":"You can check the status of containers with: $ docker ps or $ cd ~/IOTstack $ docker-compose ps","title":" checking container status "},{"location":"Getting-Started/#viewing-container-logs","text":"You can inspect the logs of most containers like this: $ docker logs \u00abcontainer\u00bb for example: $ docker logs nodered You can also follow a container's log as new entries are added by using the -f flag: $ docker logs -f nodered Terminate with a Control+C. Note that restarting a container will also terminate a followed log.","title":" viewing container logs "},{"location":"Getting-Started/#restarting-a-container","text":"You can restart a container in several ways: $ cd ~/IOTstack $ docker-compose restart \u00abcontainer\u00bb This kind of restart is the least-powerful form of restart. A good way to think of it is \"the container is only restarted, it is not rebuilt\". If you change a docker-compose.yml setting for a container and/or an environment variable file referenced by docker-compose.yml then a restart is usually not enough to bring the change into effect. You need to make docker-compose notice the change: $ cd ~/IOTstack $ docker-compose up -d \u00abcontainer\u00bb This type of \"restart\" rebuilds the container. Alternatively, to force a container to rebuild (without changing either docker-compose.yml or an environment variable file): $ cd ~/IOTstack $ docker-compose stop \u00abcontainer\u00bb $ docker-compose rm -f \u00abcontainer\u00bb $ docker-compose up -d \u00abcontainer\u00bb See also updating images built from Dockerfiles if you need to force docker-compose to notice a change to a Dockerfile.","title":" restarting a container "},{"location":"Getting-Started/#persistent-data","text":"Docker allows a container's designer to map folders inside a container to a folder on your disk (SD, SSD, HD). This is done with the \"volumes\" key in docker-compose.yml . Consider the following snippet for Node-RED: volumes: - ./volumes/nodered/data:/data You read this as two paths, separated by a colon. The: external path is ./volumes/nodered/data internal path is /data In this context, the leading \".\" means \"the folder containing docker-compose.yml \", so the external path is actually: ~/IOTstack/volumes/nodered/data If a process running inside the container modifies any file or folder within: /data the change is mirrored outside the container at the same relative path within: ~/IOTstack/volumes/nodered/data The same is true in reverse. Any change made to any file or folder outside the container within: ~/IOTstack/volumes/nodered/data is mirrored at the same relative path inside the container at: /data","title":" persistent data "},{"location":"Getting-Started/#deleting-persistent-data","text":"If you need a \"clean slate\" for a container, you can delete its volumes. Using InfluxDB as an example: $ cd ~/IOTstack $ docker-compose stop influxdb $ sudo rm -rf ./volumes/influxdb $ docker-compose up -d influxdb When docker-compose tries to bring up InfluxDB, it will notice this volume mapping in docker-compose.yml : volumes: - ./volumes/influxdb/data:/var/lib/influxdb and check to see whether ./volumes/influxdb/data is present. Finding it not there, it does the equivalent of: $ sudo mkdir -p ./volumes/influxdb/data When InfluxDB starts, it sees that the folder on right-hand-side of the volumes mapping ( /var/lib/influxdb ) is empty and initialises new databases. This is how most containers behave. But there are exceptions. A good example of an exception is Mosquitto which does not re-initialise correctly so you should avoid removing its persistent store.","title":" deleting persistent data "},{"location":"Getting-Started/#stack-maintenance","text":"","title":" stack maintenance "},{"location":"Getting-Started/#update-raspberry-pi-os","text":"You should keep your Raspberry Pi up-to-date. Despite the word \"container\" suggesting that containers are fully self-contained, they sometimes depend on operating system components (\"WireGuard\" is an example). $ sudo apt update $ sudo apt upgrade -y","title":" update Raspberry Pi OS "},{"location":"Getting-Started/#git-pull","text":"Although the menu will generally do this for you, it does not hurt to keep your local copy of the IOTstack repository in sync with the master version on GitHub. $ cd ~/IOTstack $ git pull","title":" git pull "},{"location":"Getting-Started/#container-image-updates","text":"There are two kinds of images used in IOTstack: Those not built using Dockerfiles (the majority) Those built using Dockerfiles (special cases) A Dockerfile is a set of instructions designed to customise an image before it is instantiated to become a running container. The easiest way to work out which type of image you are looking at is to inspect the container's service definition in your docker-compose.yml file. If the service definition contains the: image: keyword then the image is not built using a Dockerfile. build: keyword then the image is built using a Dockerfile.","title":" container image updates "},{"location":"Getting-Started/#updating-images-not-built-from-dockerfiles","text":"If new versions of this type of image become available on DockerHub, your local IOTstack copies can be updated by a pull command: $ cd ~/IOTstack $ docker-compose pull $ docker-compose up -d $ docker system prune The pull downloads any new images. It does this without disrupting the running stack. The up -d notices any newly-downloaded images, builds new containers, and swaps old-for-new. There is barely any downtime for affected containers.","title":" updating images not built from Dockerfiles "},{"location":"Getting-Started/#updating-images-built-from-dockerfiles","text":"Containers built using Dockerfiles have a two-step process: A base image is downloaded from from DockerHub; and then The Dockerfile \"runs\" to build a local image. Node-RED is a good example of a container built from a Dockerfile. The Dockerfile defines some (or possibly all) of your add-on nodes, such as those needed for InfluxDB or Tasmota. There are two separate update situations that you need to consider: If your Dockerfile changes; or If a newer base image appears on DockerHub Node-RED also provides a good example of why your Dockerfile might change: if you decide to add or remove add-on nodes. Note: You can also add nodes to Node-RED using Manage Palette.","title":" updating images built from Dockerfiles "},{"location":"Getting-Started/#when-dockerfile-changes-local-image-only","text":"When your Dockerfile changes, you need to rebuild like this: $ cd ~/IOTstack $ docker-compose up --build -d \u00abcontainer\u00bb $ docker system prune This only rebuilds the local image and, even then, only if docker-compose senses a material change to the Dockerfile. If you are trying to force the inclusion of a later version of an add-on node, you need to treat it like a DockerHub update . Key point: The base image is not affected by this type of update. Note: You can also use this type of build if you get an error after modifying Node-RED's environment: $ cd ~/IOTstack $ docker-compose up --build -d nodered","title":" when Dockerfile changes (local image only) "},{"location":"Getting-Started/#when-dockerhub-updates-base-and-local-images","text":"When a newer version of the base image appears on DockerHub, you need to rebuild like this: $ cd ~/IOTstack $ docker-compose build --no-cache --pull \u00abcontainer\u00bb $ docker-compose up -d \u00abcontainer\u00bb $ docker system prune This causes DockerHub to be checked for the later version of the base image, downloading it as needed. Then, the Dockerfile is run to produce a new local image. The Dockerfile run happens even if a new base image was not downloaded in the previous step.","title":" when DockerHub updates (base and local images) "},{"location":"Getting-Started/#deleting-unused-images","text":"As your system evolves and new images come down from DockerHub, you may find that more disk space is being occupied than you expected. Try running: $ docker system prune This recovers anything no longer in use. If you add a container via menu.sh and later remove it (either manually or via menu.sh ), the associated images(s) will probably persist. You can check which images are installed via: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE influxdb latest 1361b14bf545 5 days ago 264MB grafana/grafana latest b9dfd6bb8484 13 days ago 149MB iotstack_nodered latest 21d5a6b7b57b 2 weeks ago 540MB portainer/portainer-ce latest 5526251cc61f 5 weeks ago 163MB eclipse-mosquitto latest 4af162db6b4c 6 weeks ago 8.65MB nodered/node-red latest fa3bc6f20464 2 months ago 376MB portainer/portainer latest dbf28ba50432 2 months ago 62.5MB Both \"Portainer CE\" and \"Portainer\" are in that list. Assuming \"Portainer\" is no longer in use, it can be removed by using either its repository name or its Image ID. In other words, the following two commands are synonyms: $ docker rmi portainer/portainer $ docker rmi dbf28ba50432 In general, you can use the repository name to remove an image but the Image ID is sometimes needed. The most common situation where you are likely to need the Image ID is after an image has been updated on DockerHub and pulled down to your Raspberry Pi. You will find two containers with the same name. One will be tagged \"latest\" (the running version) while the other will be tagged \" \" (the prior version). You use the Image ID to resolve the ambiguity.","title":" deleting unused images "},{"location":"Getting-Started/#the-nuclear-option-use-with-caution","text":"You can use Git to delete all files and folders to return your folder to the freshly cloned state. Warning: YOU WILL LOSE ALL YOUR DATA . $ cd ~/IOTstack $ sudo git clean -d -x -f This is probably the only time you should ever need to use sudo in conjunction with git for IOTstack. This is not recoverable!","title":" the nuclear option - use with caution "},{"location":"Home/","text":"Wiki The README is moving to the Wiki, It's easier to add content and example to the Wiki vs the README.md Getting Started Updating the project How the script works Understanding Containers Docker Commands Docker Networks Containers Portainer Portainer-ce Portainer Agent Node-RED Grafana Mosquitto PostgreSQL Adminer openHAB Home Assistant Pi-Hole zigbee2MQTT Plex TasmoAdmin RTL_433 EspruinoHub (testing) Next-Cloud MariaDB MotionEye Blynk Server diyHue Python Heimdall DashMachine Homer Custom containers Native installs RTL_433 RPIEasy Backups Docker backups Recovery (coming soon) Remote Access VPN and Dynamic DNS x2go Miscellaneous log2ram Dropbox-Uploader","title":"Wiki"},{"location":"Home/#wiki","text":"The README is moving to the Wiki, It's easier to add content and example to the Wiki vs the README.md Getting Started Updating the project How the script works Understanding Containers","title":"Wiki"},{"location":"Home/#docker","text":"Commands Docker Networks","title":"Docker"},{"location":"Home/#containers","text":"Portainer Portainer-ce Portainer Agent Node-RED Grafana Mosquitto PostgreSQL Adminer openHAB Home Assistant Pi-Hole zigbee2MQTT Plex TasmoAdmin RTL_433 EspruinoHub (testing) Next-Cloud MariaDB MotionEye Blynk Server diyHue Python Heimdall DashMachine Homer Custom containers","title":"Containers"},{"location":"Home/#native-installs","text":"RTL_433 RPIEasy","title":"Native installs"},{"location":"Home/#backups","text":"Docker backups Recovery (coming soon)","title":"Backups"},{"location":"Home/#remote-access","text":"VPN and Dynamic DNS x2go","title":"Remote Access"},{"location":"Home/#miscellaneous","text":"log2ram Dropbox-Uploader","title":"Miscellaneous"},{"location":"How-the-script-works/","text":"How the script works The build script creates the ./services directory and populates it from the template file in .templates . The script then appends the text withing each service.yml file to the docker-compose.yml . When the stack is rebuild the menu doesn not overwrite the service folder if it already exists. Make sure to sync any alterations you have made to the docker-compose.yml file with the respective service.yml so that on your next build your changes pull through. The .gitignore file is setup such that if you do a git pull origin master it does not overwrite the files you have already created. Because the build script does not overwrite your service directory any changes in the .templates directory will have no affect on the services you have already made. You will need to move your service folder out to get the latest version of the template.","title":"How the script works"},{"location":"How-the-script-works/#how-the-script-works","text":"The build script creates the ./services directory and populates it from the template file in .templates . The script then appends the text withing each service.yml file to the docker-compose.yml . When the stack is rebuild the menu doesn not overwrite the service folder if it already exists. Make sure to sync any alterations you have made to the docker-compose.yml file with the respective service.yml so that on your next build your changes pull through. The .gitignore file is setup such that if you do a git pull origin master it does not overwrite the files you have already created. Because the build script does not overwrite your service directory any changes in the .templates directory will have no affect on the services you have already made. You will need to move your service folder out to get the latest version of the template.","title":"How the script works"},{"location":"Menu-System/","text":"Menu system This page explains how the menu system works for developers. Background Originally this script was written in bash. After a while it became obvious that bash wasn't well suited to dealing with all the different types of configuration files, and logic that goes with configuring everything. IOTstack needs to be accessible to all levels of programmers and tinkerers, not just ones experienced with Linux and bash. For this reason, it was rewritten in Python since the language syntax is easier to understand, and is more commonly used for scripting and programming than bash. Bash is still used in IOTstack where it makes sense to use it, but the menu system itself uses Python. The code it self while not being the most well structured or efficient, was intentionally made that way so that beginners and experienced programmers could contribute to the project. We are always open to improvements if you have suggestions. Menu Structure Each screen of the menu is its own Python script. You can find most of these in the ./scripts directory. When you select an item from the menu, and it changes screens, it actually dynamically loads and executes that Python script. It passes data as required by placing it into the global variable space so that both the child and the parent script can access it. Injecting and getting globals in a child script with open(childPythonScriptPath, \"rb\") as pythonDynamicImportFile: code = compile(pythonDynamicImportFile.read(), childPythonScriptPath, \"exec\") execGlobals = { \"globalKeyName\": \"globalKeyValue\" } execLocals = {} print(globalKeyName) # Will print out 'globalKeyValue' exec(code, execGlobals, execLocals) print(globalKeyName) # Will print out 'newValue' Reading and writing global variables in a child script def someFunction: global globalKeyName print(globalKeyName) # Will print out 'globalKeyValue' globalKeyName = \"newValue\" Each menu is its own python executable. The entry point is down the bottom of the file wrapped in a main() function to prevent variable scope creep. The code at the bottom of the main() function: if __name__ == 'builtins': Is actually where the execution path runs, all the code above it is just declared so that it can be called without ordering or scope issues. Optimisations It was obvious early on that the menu system would be slow on lower end devices, such as the Raspberry Pi, especially if it were rending a 4k terminal screen from a desktop via SSH. To mitigate this issue, not all of the screen is redrawn when there is a change. A \"Hotzone\" as it's called in the code, is usually rerendered when there's a change (such as pressing up or down to change an item selection, but not when scrolling). Full screen redraws are expensive and are only used when required, for example, when scrolling the pagination, selecting or deselecting a service, expanding or collapsing the menu and so on. Environments and encoding At the very beginning of the main menu screen ( ./scripts/main_menu.py ) the function checkRenderOptions() is run to determine what characters can be displayed on the screen. It will try various character sets, and eventually default to ASCII if none of the fancier stuff can be rendered. This setting is passed into of the sub menus through the submenu's global variables so that they don't have to recheck when they load. Sub-Menus From the main screen, you will see several sections leading to various submenus. Most of these menus work in the same way as the main menu. The only exception to this rule is the Build Stack menu, which is probably the most complex part of IOTstack. Build Stack Menu Path: ./scripts/buildstack_menu.py Loading Upon loading, the Build Stack menu will get a list of folders inside the ./templates directory and check for a build.py file inside each of them. This can be seen in the generateTemplateList() function, which is executed before the first rendering happens. The menu will then check if the file ./services/docker-compose.save.yml exists. This file is used to save the configuration of the last build. This happens in the loadCurrentConfigs() function. It is important that the service name in the compose file matches the folder name, any service that doesn't will either cause an error, or won't be loaded into the menu. If a previous build did exist the menu will then run the prepareMenuState() function that basically checks which items should be ticked, and check for any issues with the ticked items by running checkForIssues() . Selection and deselection When an item is selected, 3 things happen: 1. Update the UI variable ( menu ) with function checkMenuItem(selectionIndex) to let the user know the current state. 2. Update the array holding every checked item setCheckedMenuItems() . It uses the UI variable ( menu ) to know which items are set. 3. Check for any issues with the new list of selected items by running checkForIssues() . Check for options (submenus of services) During a full render sequence (this is not a hotzone render), the build stack menu checks to see if each of the services has an options menu. It does this by executing the build.py script of each of the services and passing in checkForOptionsHook into the toRun global variable property to see if the script has a runOptionsMenu function. If the service's function result is true, without error, then the options text will appear up for that menu item. Check for issues When a service is selected or deselected on the menu, the checkForIssues() function is run. This function iterates through each of the selected menu items' folders executing the build.py script and passing in checkForRunChecksHook into the toRun global variable property to see if the script has a runChecks function. The runChecks function is different depending on the service, since each service has its own requirements. Generally though, the runChecks function should check for conflicting port conflicts again any of the other services that are enabled. The menu will still allow you to build the stack, even if issues are present, assumine there's no errors raised during the build process. Prebuild hook Pressing enter on the Build Stack menu kicks off the build process. The Build Stack menu will execute the runPrebuildHook() function. This function iterates through each of the selected menu items' folders executing the build.py script and passing in checkForPreBuildHook into the toRun global variable property to see if the script has a preBuild function. The preBuild function is different depending on the service, since each service has its own requirements. Some services may not even use the prebuild hook. The prebuild is very useful for setting up the services' configuration however. For example, it can be used to autogenerate a password for a paticular service, or copy and modify a configuration file from the ./.templates directory into the ./services or ./volumes directory. Postbuild hook The Build Stack menu will execute the runPostBuildHook() function in the final step of the build process, after the docker-compose.yml file has been written to disk. This function iterates through each of the selected menu items' folders executing the build.py script and passing in checkForPostBuildHook into the toRun global variable property to see if the script has a postBuild function. The postBuild function is different depending on the service, since each service has its own requirements. Most services won't require this function, but it can be useful for cleaning up temporary files and so on. The build process The selected services' yaml configuration is already loaded into memory before the build stack process is started. Run prebuildHooks. Create a new in memory docker-compose.yml structure. Merge the ./.templates/env.yml file into docker-compose.yml memory. If it exists merge the ./compose-override.yml file into memory Write the docker-compose in memory yaml structure to disk. Run postbuildHooks. Run postbuild.sh if it exists, with the list of services built.","title":"Menu system"},{"location":"Menu-System/#menu-system","text":"This page explains how the menu system works for developers.","title":"Menu system"},{"location":"Menu-System/#background","text":"Originally this script was written in bash. After a while it became obvious that bash wasn't well suited to dealing with all the different types of configuration files, and logic that goes with configuring everything. IOTstack needs to be accessible to all levels of programmers and tinkerers, not just ones experienced with Linux and bash. For this reason, it was rewritten in Python since the language syntax is easier to understand, and is more commonly used for scripting and programming than bash. Bash is still used in IOTstack where it makes sense to use it, but the menu system itself uses Python. The code it self while not being the most well structured or efficient, was intentionally made that way so that beginners and experienced programmers could contribute to the project. We are always open to improvements if you have suggestions.","title":"Background"},{"location":"Menu-System/#menu-structure","text":"Each screen of the menu is its own Python script. You can find most of these in the ./scripts directory. When you select an item from the menu, and it changes screens, it actually dynamically loads and executes that Python script. It passes data as required by placing it into the global variable space so that both the child and the parent script can access it.","title":"Menu Structure"},{"location":"Menu-System/#injecting-and-getting-globals-in-a-child-script","text":"with open(childPythonScriptPath, \"rb\") as pythonDynamicImportFile: code = compile(pythonDynamicImportFile.read(), childPythonScriptPath, \"exec\") execGlobals = { \"globalKeyName\": \"globalKeyValue\" } execLocals = {} print(globalKeyName) # Will print out 'globalKeyValue' exec(code, execGlobals, execLocals) print(globalKeyName) # Will print out 'newValue'","title":"Injecting and getting globals in a child script"},{"location":"Menu-System/#reading-and-writing-global-variables-in-a-child-script","text":"def someFunction: global globalKeyName print(globalKeyName) # Will print out 'globalKeyValue' globalKeyName = \"newValue\" Each menu is its own python executable. The entry point is down the bottom of the file wrapped in a main() function to prevent variable scope creep. The code at the bottom of the main() function: if __name__ == 'builtins': Is actually where the execution path runs, all the code above it is just declared so that it can be called without ordering or scope issues.","title":"Reading and writing global variables in a child script"},{"location":"Menu-System/#optimisations","text":"It was obvious early on that the menu system would be slow on lower end devices, such as the Raspberry Pi, especially if it were rending a 4k terminal screen from a desktop via SSH. To mitigate this issue, not all of the screen is redrawn when there is a change. A \"Hotzone\" as it's called in the code, is usually rerendered when there's a change (such as pressing up or down to change an item selection, but not when scrolling). Full screen redraws are expensive and are only used when required, for example, when scrolling the pagination, selecting or deselecting a service, expanding or collapsing the menu and so on.","title":"Optimisations"},{"location":"Menu-System/#environments-and-encoding","text":"At the very beginning of the main menu screen ( ./scripts/main_menu.py ) the function checkRenderOptions() is run to determine what characters can be displayed on the screen. It will try various character sets, and eventually default to ASCII if none of the fancier stuff can be rendered. This setting is passed into of the sub menus through the submenu's global variables so that they don't have to recheck when they load.","title":"Environments and encoding"},{"location":"Menu-System/#sub-menus","text":"From the main screen, you will see several sections leading to various submenus. Most of these menus work in the same way as the main menu. The only exception to this rule is the Build Stack menu, which is probably the most complex part of IOTstack.","title":"Sub-Menus"},{"location":"Menu-System/#build-stack-menu","text":"Path: ./scripts/buildstack_menu.py","title":"Build Stack Menu"},{"location":"Menu-System/#loading","text":"Upon loading, the Build Stack menu will get a list of folders inside the ./templates directory and check for a build.py file inside each of them. This can be seen in the generateTemplateList() function, which is executed before the first rendering happens. The menu will then check if the file ./services/docker-compose.save.yml exists. This file is used to save the configuration of the last build. This happens in the loadCurrentConfigs() function. It is important that the service name in the compose file matches the folder name, any service that doesn't will either cause an error, or won't be loaded into the menu. If a previous build did exist the menu will then run the prepareMenuState() function that basically checks which items should be ticked, and check for any issues with the ticked items by running checkForIssues() .","title":"Loading"},{"location":"Menu-System/#selection-and-deselection","text":"When an item is selected, 3 things happen: 1. Update the UI variable ( menu ) with function checkMenuItem(selectionIndex) to let the user know the current state. 2. Update the array holding every checked item setCheckedMenuItems() . It uses the UI variable ( menu ) to know which items are set. 3. Check for any issues with the new list of selected items by running checkForIssues() .","title":"Selection and deselection"},{"location":"Menu-System/#check-for-options-submenus-of-services","text":"During a full render sequence (this is not a hotzone render), the build stack menu checks to see if each of the services has an options menu. It does this by executing the build.py script of each of the services and passing in checkForOptionsHook into the toRun global variable property to see if the script has a runOptionsMenu function. If the service's function result is true, without error, then the options text will appear up for that menu item.","title":"Check for options (submenus of services)"},{"location":"Menu-System/#check-for-issues","text":"When a service is selected or deselected on the menu, the checkForIssues() function is run. This function iterates through each of the selected menu items' folders executing the build.py script and passing in checkForRunChecksHook into the toRun global variable property to see if the script has a runChecks function. The runChecks function is different depending on the service, since each service has its own requirements. Generally though, the runChecks function should check for conflicting port conflicts again any of the other services that are enabled. The menu will still allow you to build the stack, even if issues are present, assumine there's no errors raised during the build process.","title":"Check for issues"},{"location":"Menu-System/#prebuild-hook","text":"Pressing enter on the Build Stack menu kicks off the build process. The Build Stack menu will execute the runPrebuildHook() function. This function iterates through each of the selected menu items' folders executing the build.py script and passing in checkForPreBuildHook into the toRun global variable property to see if the script has a preBuild function. The preBuild function is different depending on the service, since each service has its own requirements. Some services may not even use the prebuild hook. The prebuild is very useful for setting up the services' configuration however. For example, it can be used to autogenerate a password for a paticular service, or copy and modify a configuration file from the ./.templates directory into the ./services or ./volumes directory.","title":"Prebuild hook"},{"location":"Menu-System/#postbuild-hook","text":"The Build Stack menu will execute the runPostBuildHook() function in the final step of the build process, after the docker-compose.yml file has been written to disk. This function iterates through each of the selected menu items' folders executing the build.py script and passing in checkForPostBuildHook into the toRun global variable property to see if the script has a postBuild function. The postBuild function is different depending on the service, since each service has its own requirements. Most services won't require this function, but it can be useful for cleaning up temporary files and so on.","title":"Postbuild hook"},{"location":"Menu-System/#the-build-process","text":"The selected services' yaml configuration is already loaded into memory before the build stack process is started. Run prebuildHooks. Create a new in memory docker-compose.yml structure. Merge the ./.templates/env.yml file into docker-compose.yml memory. If it exists merge the ./compose-override.yml file into memory Write the docker-compose in memory yaml structure to disk. Run postbuildHooks. Run postbuild.sh if it exists, with the list of services built.","title":"The build process"},{"location":"Misc/","text":"Miscellaneous log2ram https://github.com/azlux/log2ram One of the drawbacks of an sd card is that it has a limited lifespan. One way to reduce the load on the sd card is to move your log files to RAM. log2ram is a convenient tool to simply set this up. It can be installed from the miscellaneous menu. Dropbox-Uploader This a great utility to easily upload data from your PI to the cloud. https://magpi.raspberrypi.org/articles/dropbox-raspberry-pi The MagPi has an excellent explanation of the process of setting up the Dropbox API. Dropbox-Uploader is used in the backup script.","title":"Miscellaneous"},{"location":"Misc/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"Misc/#log2ram","text":"https://github.com/azlux/log2ram One of the drawbacks of an sd card is that it has a limited lifespan. One way to reduce the load on the sd card is to move your log files to RAM. log2ram is a convenient tool to simply set this up. It can be installed from the miscellaneous menu.","title":"log2ram"},{"location":"Misc/#dropbox-uploader","text":"This a great utility to easily upload data from your PI to the cloud. https://magpi.raspberrypi.org/articles/dropbox-raspberry-pi The MagPi has an excellent explanation of the process of setting up the Dropbox API. Dropbox-Uploader is used in the backup script.","title":"Dropbox-Uploader"},{"location":"Native-RTL_433/","text":"Native RTL_433 RTL_433 can be installed from the \"Native install sections\" This video demonstrates how to use RTL_433","title":"Native RTL_433"},{"location":"Native-RTL_433/#native-rtl_433","text":"RTL_433 can be installed from the \"Native install sections\" This video demonstrates how to use RTL_433","title":"Native RTL_433"},{"location":"Networking/","text":"Networking The docker-compose instruction creates an internal network for the containers to communicate in, the ports get exposed to the PI's IP address when you want to connect from outside. It also creates a \"DNS\" the name being the container name. So it is important to note that when one container talks to another they talk by name. All the containers names are lowercase like nodered, influxdb... An easy way to find out your IP is by typing ip address in the terminal and look next to eth0 or wlan0 for your IP. It is highly recommended that you set a static IP for your PI or at least reserve an IP on your router so that you know it Check the docker-compose.yml to see which ports have been used Examples You want to connect your nodered to your mqtt server. In nodered drop an mqtt node, when you need to specify the address type mosquitto You want to connect to your influxdb from grafana. You are in the Docker network and you need to use the name of the Container. The address you specify in the grafana is http://influxdb:8086 You want to connect to the web interface of grafana from your laptop. Now you are outside the container environment you type PI's IP eg 192.168.n.m:3000 Ports Many containers try to use popular ports such as 80,443,8080. For example openHAB and Adminer both want to use port 8080 for their web interface. Adminer's port has been moved 9080 to accommodate this. Please check the description of the container in the README to see if there are any changes as they may not be the same as the port you are used to. Port mapping is done in the docker-compose.yml file. Each service should have a section that reads like this: ports: - HOST_PORT:CONTAINER_PORT For adminer: ports: - 9080:8080 Port 9080 on Host Pi is mapped to port 8080 of the container. Therefore 127.0.0.1:8080 will take you to openHAB, where 127.0.0.1:9080 will take you to adminer","title":"Networking"},{"location":"Networking/#networking","text":"The docker-compose instruction creates an internal network for the containers to communicate in, the ports get exposed to the PI's IP address when you want to connect from outside. It also creates a \"DNS\" the name being the container name. So it is important to note that when one container talks to another they talk by name. All the containers names are lowercase like nodered, influxdb... An easy way to find out your IP is by typing ip address in the terminal and look next to eth0 or wlan0 for your IP. It is highly recommended that you set a static IP for your PI or at least reserve an IP on your router so that you know it Check the docker-compose.yml to see which ports have been used","title":"Networking"},{"location":"Networking/#examples","text":"You want to connect your nodered to your mqtt server. In nodered drop an mqtt node, when you need to specify the address type mosquitto You want to connect to your influxdb from grafana. You are in the Docker network and you need to use the name of the Container. The address you specify in the grafana is http://influxdb:8086 You want to connect to the web interface of grafana from your laptop. Now you are outside the container environment you type PI's IP eg 192.168.n.m:3000","title":"Examples"},{"location":"Networking/#ports","text":"Many containers try to use popular ports such as 80,443,8080. For example openHAB and Adminer both want to use port 8080 for their web interface. Adminer's port has been moved 9080 to accommodate this. Please check the description of the container in the README to see if there are any changes as they may not be the same as the port you are used to. Port mapping is done in the docker-compose.yml file. Each service should have a section that reads like this: ports: - HOST_PORT:CONTAINER_PORT For adminer: ports: - 9080:8080 Port 9080 on Host Pi is mapped to port 8080 of the container. Therefore 127.0.0.1:8080 will take you to openHAB, where 127.0.0.1:9080 will take you to adminer","title":"Ports"},{"location":"New-Menu-Release-Notes/","text":"New IOTstack Menu Background Originally this script was written in bash. After a while it became obvious that bash wasn't well suited to dealing with all the different types of configuration files, and logic that goes with configuring everything. IOTstack needs to be accessible to all levels of programmers and tinkerers, not just ones experienced with Linux and bash. For this reason, it was rewritten in Python since the language syntax is easier to understand, and is more commonly used for scripting and programming than bash. Bash is still used in IOTstack where it makes sense to use it, but the menu system itself uses Python. The code is intentionally made so that beginners and experienced programmers could contribute to the project. We are always open to improvements if you have suggestions. On-going improvements There are many features that are needing to be introduced into the new menu system. From meta tags on services for filtering, to optional nginx autoconfiguration and authentication. For this reason you may initially experience bugs (very hard to test every type of configuration!). The new menu system has been worked on and tested for 6 months and we think it's stable enough to merge into the master branch for mainstream usage. The code still needs some work to make it easier to add new services and to not require copy pasting the same code for each new service. Also to make the menu system not be needed at all (so it can be automated with bash scripts). Breaking changes There are a few changes that you need to be aware of: * Docker Environmental *.env files are no longer a thing by default. Everything needed is specified in the service.yml file, you can still optionally use them though either with Custom Overrides or with the PostBuild script. Specific config files for certain services still work as they once did. * Python 3, pip3, PyYAML and Blessed are all required to be installed. * Not backwards compatible with old menu system. You will be able to switch back to the old menu system for a period of time by changing to the old-menu branch. It will be unmaintained except for critical updates. It will eventually be removed - but not before everyone is ready to leave it. Test that your backups are working before you switch. The old-menu branch will become avaiable just before the new menu is merged into master to ensure it has the latest commits applied. Full change list Menu and everything that goes with it rewritten in Python and Blessed Easy installation script All services rewritten to be compatible with PyYAML Optional port selection for services Issue checking for services before building Options for services now in menu (no more editing service.yml files) Automatic password generation for each service Pre and post scripts for customising services Removed env files Backup and restoring more streamlined Documentation updated for all services No longer needs to be installed in the home directory ~ .","title":"New IOTstack Menu"},{"location":"New-Menu-Release-Notes/#new-iotstack-menu","text":"","title":"New IOTstack Menu"},{"location":"New-Menu-Release-Notes/#background","text":"Originally this script was written in bash. After a while it became obvious that bash wasn't well suited to dealing with all the different types of configuration files, and logic that goes with configuring everything. IOTstack needs to be accessible to all levels of programmers and tinkerers, not just ones experienced with Linux and bash. For this reason, it was rewritten in Python since the language syntax is easier to understand, and is more commonly used for scripting and programming than bash. Bash is still used in IOTstack where it makes sense to use it, but the menu system itself uses Python. The code is intentionally made so that beginners and experienced programmers could contribute to the project. We are always open to improvements if you have suggestions.","title":"Background"},{"location":"New-Menu-Release-Notes/#on-going-improvements","text":"There are many features that are needing to be introduced into the new menu system. From meta tags on services for filtering, to optional nginx autoconfiguration and authentication. For this reason you may initially experience bugs (very hard to test every type of configuration!). The new menu system has been worked on and tested for 6 months and we think it's stable enough to merge into the master branch for mainstream usage. The code still needs some work to make it easier to add new services and to not require copy pasting the same code for each new service. Also to make the menu system not be needed at all (so it can be automated with bash scripts).","title":"On-going improvements"},{"location":"New-Menu-Release-Notes/#breaking-changes","text":"There are a few changes that you need to be aware of: * Docker Environmental *.env files are no longer a thing by default. Everything needed is specified in the service.yml file, you can still optionally use them though either with Custom Overrides or with the PostBuild script. Specific config files for certain services still work as they once did. * Python 3, pip3, PyYAML and Blessed are all required to be installed. * Not backwards compatible with old menu system. You will be able to switch back to the old menu system for a period of time by changing to the old-menu branch. It will be unmaintained except for critical updates. It will eventually be removed - but not before everyone is ready to leave it. Test that your backups are working before you switch. The old-menu branch will become avaiable just before the new menu is merged into master to ensure it has the latest commits applied.","title":"Breaking changes"},{"location":"New-Menu-Release-Notes/#full-change-list","text":"Menu and everything that goes with it rewritten in Python and Blessed Easy installation script All services rewritten to be compatible with PyYAML Optional port selection for services Issue checking for services before building Options for services now in menu (no more editing service.yml files) Automatic password generation for each service Pre and post scripts for customising services Removed env files Backup and restoring more streamlined Documentation updated for all services No longer needs to be installed in the home directory ~ .","title":"Full change list"},{"location":"PostBuild-Script/","text":"Postbuild BASH Script The postbuild bash script allows for executing arbitrary execution of bash commands after the stack has been build. How to use Place a file in the main directory called postbuild.sh . When the buildstack build logic finishes, it'll execute the postbuild.sh script, passing in each service selected from the buildstack menu as a parameter. This script is run each time the buildstack logic runs. Updates The postbuild.sh file has been added to gitignore, so it won't be updated by IOTstack when IOTstack is updated. It has also been added to the backup script so that it will be backed up with your personal IOTstack backups. Example postbuild.sh script The following script will print out each of the services built, and a custom message for nodered. If it was the first time the script was executed, it'll also output \"Fresh Install\" at the end, using a .install_tainted file for knowing. #!/bin/bash for iotstackService in \"$@\" do echo \"$iotstackService\" if [ \"$iotstackService\" == \"nodered\" ]; then echo \"NodeRed Installed!\" fi done if [ ! -f .install_tainted ]; then echo \"Fresh Install!\" touch .install_tainted fi What is my purpose? The postbuild script can be used to run custom bash commands, such as moving files, or issuing commands that your services expect to be completed before running.","title":"Postbuild BASH Script"},{"location":"PostBuild-Script/#postbuild-bash-script","text":"The postbuild bash script allows for executing arbitrary execution of bash commands after the stack has been build.","title":"Postbuild BASH Script"},{"location":"PostBuild-Script/#how-to-use","text":"Place a file in the main directory called postbuild.sh . When the buildstack build logic finishes, it'll execute the postbuild.sh script, passing in each service selected from the buildstack menu as a parameter. This script is run each time the buildstack logic runs.","title":"How to use"},{"location":"PostBuild-Script/#updates","text":"The postbuild.sh file has been added to gitignore, so it won't be updated by IOTstack when IOTstack is updated. It has also been added to the backup script so that it will be backed up with your personal IOTstack backups.","title":"Updates"},{"location":"PostBuild-Script/#example-postbuildsh-script","text":"The following script will print out each of the services built, and a custom message for nodered. If it was the first time the script was executed, it'll also output \"Fresh Install\" at the end, using a .install_tainted file for knowing. #!/bin/bash for iotstackService in \"$@\" do echo \"$iotstackService\" if [ \"$iotstackService\" == \"nodered\" ]; then echo \"NodeRed Installed!\" fi done if [ ! -f .install_tainted ]; then echo \"Fresh Install!\" touch .install_tainted fi","title":"Example postbuild.sh script"},{"location":"PostBuild-Script/#what-is-my-purpose","text":"The postbuild script can be used to run custom bash commands, such as moving files, or issuing commands that your services expect to be completed before running.","title":"What is my purpose?"},{"location":"RPIEasy_native/","text":"RPIEasy RPIEasy can now be installed under the native menu The installer will install any dependencies. If ~/rpieasy exists it will update the project to its latest, if not it will clone the project Running Running RPIEasy RPIEasy can be run by sudo ~/rpieasy/RPIEasy.py To have RPIEasy start on boot in the webui under hardware look for \"RPIEasy autostart at boot\" Ports RPIEasy will select its ports from the first available one in the list (80,8080,8008). If you run Hass.io then there will be a conflict so check the next available port","title":"RPIEasy"},{"location":"RPIEasy_native/#rpieasy","text":"RPIEasy can now be installed under the native menu The installer will install any dependencies. If ~/rpieasy exists it will update the project to its latest, if not it will clone the project","title":"RPIEasy"},{"location":"RPIEasy_native/#running-running-rpieasy","text":"RPIEasy can be run by sudo ~/rpieasy/RPIEasy.py To have RPIEasy start on boot in the webui under hardware look for \"RPIEasy autostart at boot\"","title":"Running Running RPIEasy"},{"location":"RPIEasy_native/#ports","text":"RPIEasy will select its ports from the first available one in the list (80,8080,8008). If you run Hass.io then there will be a conflict so check the next available port","title":"Ports"},{"location":"Understanding-Containers/","text":"What is Docker? In simple terms, Docker is a software platform that simplifies the process of building, running, managing and distributing applications. It does this by virtualizing the operating system of the computer on which it is installed and running. The Problem Let\u2019s say you have three different Python-based applications that you plan to host on a single server (which could either be a physical or a virtual machine). Each of these applications makes use of a different version of Python, as well as the associated libraries and dependencies, differ from one application to another. Since we cannot have different versions of Python installed on the same machine, this prevents us from hosting all three applications on the same computer. The Solution Let\u2019s look at how we could solve this problem without making use of Docker. In such a scenario, we could solve this problem either by having three physical machines, or a single physical machine, which is powerful enough to host and run three virtual machines on it. Both the options would allow us to install different versions of Python on each of these machines, along with their associated dependencies. The machine on which Docker is installed and running is usually referred to as a Docker Host or Host in simple terms. So, whenever you plan to deploy an application on the host, it would create a logical entity on it to host that application. In Docker terminology, we call this logical entity a Container or Docker Container to be more precise. Whereas the kernel of the host\u2019s operating system is shared across all the containers that are running on it. This allows each container to be isolated from the other present on the same host. Thus it supports multiple containers with different application requirements and dependencies to run on the same host, as long as they have the same operating system requirements. Docker Terminology Docker Images and Docker Containers are the two essential things that you will come across daily while working with Docker. In simple terms, a Docker Image is a template that contains the application, and all the dependencies required to run that application on Docker. On the other hand, as stated earlier, a Docker Container is a logical entity. In more precise terms, it is a running instance of the Docker Image. What is Docker-Compose? Docker Compose provides a way to orchestrate multiple containers that work together. Docker compose is a simple yet powerful tool that is used to run multiple containers as a single service. For example, suppose you have an application which requires Mqtt as a communication service between IOT devices and OpenHAB instance as a Smarthome application service. In this case by docker-compose, you can create one single file (docker-compose.yml) which will create both the containers as a single service without starting each separately. It wires up the networks (literally), mounts all volumes and exposes the ports. The IOTstack with the templates and menu is a generator for that docker-compose service descriptor. How Docker Compose Works? use yaml files to configure application services (docker-compose.yaml) can start all the services with a single command ( docker-compose up ) can stop all the service with a single command ( docker-compose down ) How are the containers connected The containers are automagically connected when we run the stack with docker-compose up. The containers using same logical network (by default) where the instances can access each other with the instance logical name. Means if there is an instance called mosquitto and an openhab , when openHAB instance need to access mqtt on that case the domain name of mosquitto will be resolved as the runnuning instance of mosquitto. How the container are connected to host machine Volumes The containers are enclosed processes which state are lost with the restart of container. To be able to persist states volumes (images or directories) can be used to share data with the host. Which means if you need to persist some database, configuration or any state you have to bind volumes where the running service inside the container will write files to that binded volume. In order to understand what a Docker volume is, we first need to be clear about how the filesystem normally works in Docker. Docker images are stored as series of read-only layers. When we start a container, Docker takes the read-only image and adds a read-write layer on top. If the running container modifies an existing file, the file is copied out of the underlying read-only layer and into the top-most read-write layer where the changes are applied. The version in the read-write layer hides the underlying file, but does not destroy it -- it still exists in the underlying layer. When a Docker container is deleted, relaunching the image will start a fresh container without any of the changes made in the previously running container -- those changes are lost, thats the reason that configs, databases are not persisted, Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure of the host machine, volumes are completely managed by Docker. In IOTstack project uses the volumes directory in general to bind these container volumes. Ports When containers running a we would like to delegate some services to the outside world, for example OpenHAB web frontend have to be accessible for users. There are several ways to achive that. One is mounting the port to the most machine, this called port binding. On that case service will have a dedicated port which can be accessed, one drawback is one host port can be used one serice only. Another way is reverse proxy. The term reverse proxy (or Load Balancer in some terminology) is normally applied to a service that sits in front of one or more servers (in our case containers), accepting requests from clients for resources located on the server(s). From the client point of view, the reverse proxy appears to be the web server and so is totally transparent to the remote user. Which means several service can share same port the server will route the request by the URL (virtual domain or context path). For example, there is grafana and openHAB instances, where the opeanhab.domain.tld request will be routed to openHAB instance 8181 port while grafana.domain.tld to grafana instance 3000 port. On that case the proxy have to be mapped for host port 80 and/or 444 on host machine, the proxy server will access the containers via the docker virtual network. Source materials used: https://takacsmark.com/docker-compose-tutorial-beginners-by-example-basics/ https://www.freecodecamp.org/news/docker-simplified-96639a35ff36/ https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/ https://blog.container-solutions.com/understanding-volumes-docker","title":"What is Docker?"},{"location":"Understanding-Containers/#what-is-docker","text":"In simple terms, Docker is a software platform that simplifies the process of building, running, managing and distributing applications. It does this by virtualizing the operating system of the computer on which it is installed and running.","title":"What is Docker?"},{"location":"Understanding-Containers/#the-problem","text":"Let\u2019s say you have three different Python-based applications that you plan to host on a single server (which could either be a physical or a virtual machine). Each of these applications makes use of a different version of Python, as well as the associated libraries and dependencies, differ from one application to another. Since we cannot have different versions of Python installed on the same machine, this prevents us from hosting all three applications on the same computer.","title":"The Problem"},{"location":"Understanding-Containers/#the-solution","text":"Let\u2019s look at how we could solve this problem without making use of Docker. In such a scenario, we could solve this problem either by having three physical machines, or a single physical machine, which is powerful enough to host and run three virtual machines on it. Both the options would allow us to install different versions of Python on each of these machines, along with their associated dependencies. The machine on which Docker is installed and running is usually referred to as a Docker Host or Host in simple terms. So, whenever you plan to deploy an application on the host, it would create a logical entity on it to host that application. In Docker terminology, we call this logical entity a Container or Docker Container to be more precise. Whereas the kernel of the host\u2019s operating system is shared across all the containers that are running on it. This allows each container to be isolated from the other present on the same host. Thus it supports multiple containers with different application requirements and dependencies to run on the same host, as long as they have the same operating system requirements.","title":"The Solution"},{"location":"Understanding-Containers/#docker-terminology","text":"Docker Images and Docker Containers are the two essential things that you will come across daily while working with Docker. In simple terms, a Docker Image is a template that contains the application, and all the dependencies required to run that application on Docker. On the other hand, as stated earlier, a Docker Container is a logical entity. In more precise terms, it is a running instance of the Docker Image.","title":"Docker Terminology"},{"location":"Understanding-Containers/#what-is-docker-compose","text":"Docker Compose provides a way to orchestrate multiple containers that work together. Docker compose is a simple yet powerful tool that is used to run multiple containers as a single service. For example, suppose you have an application which requires Mqtt as a communication service between IOT devices and OpenHAB instance as a Smarthome application service. In this case by docker-compose, you can create one single file (docker-compose.yml) which will create both the containers as a single service without starting each separately. It wires up the networks (literally), mounts all volumes and exposes the ports. The IOTstack with the templates and menu is a generator for that docker-compose service descriptor.","title":"What is Docker-Compose?"},{"location":"Understanding-Containers/#how-docker-compose-works","text":"use yaml files to configure application services (docker-compose.yaml) can start all the services with a single command ( docker-compose up ) can stop all the service with a single command ( docker-compose down )","title":"How Docker Compose Works?"},{"location":"Understanding-Containers/#how-are-the-containers-connected","text":"The containers are automagically connected when we run the stack with docker-compose up. The containers using same logical network (by default) where the instances can access each other with the instance logical name. Means if there is an instance called mosquitto and an openhab , when openHAB instance need to access mqtt on that case the domain name of mosquitto will be resolved as the runnuning instance of mosquitto.","title":"How are the containers connected"},{"location":"Understanding-Containers/#how-the-container-are-connected-to-host-machine","text":"","title":"How the container are connected to host machine"},{"location":"Understanding-Containers/#volumes","text":"The containers are enclosed processes which state are lost with the restart of container. To be able to persist states volumes (images or directories) can be used to share data with the host. Which means if you need to persist some database, configuration or any state you have to bind volumes where the running service inside the container will write files to that binded volume. In order to understand what a Docker volume is, we first need to be clear about how the filesystem normally works in Docker. Docker images are stored as series of read-only layers. When we start a container, Docker takes the read-only image and adds a read-write layer on top. If the running container modifies an existing file, the file is copied out of the underlying read-only layer and into the top-most read-write layer where the changes are applied. The version in the read-write layer hides the underlying file, but does not destroy it -- it still exists in the underlying layer. When a Docker container is deleted, relaunching the image will start a fresh container without any of the changes made in the previously running container -- those changes are lost, thats the reason that configs, databases are not persisted, Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure of the host machine, volumes are completely managed by Docker. In IOTstack project uses the volumes directory in general to bind these container volumes.","title":"Volumes"},{"location":"Understanding-Containers/#ports","text":"When containers running a we would like to delegate some services to the outside world, for example OpenHAB web frontend have to be accessible for users. There are several ways to achive that. One is mounting the port to the most machine, this called port binding. On that case service will have a dedicated port which can be accessed, one drawback is one host port can be used one serice only. Another way is reverse proxy. The term reverse proxy (or Load Balancer in some terminology) is normally applied to a service that sits in front of one or more servers (in our case containers), accepting requests from clients for resources located on the server(s). From the client point of view, the reverse proxy appears to be the web server and so is totally transparent to the remote user. Which means several service can share same port the server will route the request by the URL (virtual domain or context path). For example, there is grafana and openHAB instances, where the opeanhab.domain.tld request will be routed to openHAB instance 8181 port while grafana.domain.tld to grafana instance 3000 port. On that case the proxy have to be mapped for host port 80 and/or 444 on host machine, the proxy server will access the containers via the docker virtual network. Source materials used: https://takacsmark.com/docker-compose-tutorial-beginners-by-example-basics/ https://www.freecodecamp.org/news/docker-simplified-96639a35ff36/ https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/ https://blog.container-solutions.com/understanding-volumes-docker","title":"Ports"},{"location":"Updating-the-Project/","text":"Updating the project If you ran the git checkout -- 'git ls-files -m' as suggested in the old wiki entry then please check your duck.sh because it removed your domain and token Periodically updates are made to project which include new or modified container template, changes to backups or additional features. As these are released your local copy of this project will become out of date. This section deals with how to bring your project to the latest published state. Git offers build in functionality to fetch the latest changes. git pull origin master will fetch the latest changes from GitHub without overwriting files that you have modified yourself. If you have done a local commit then your project may to handle a merge conflict. This can be verified by running git status . You can ignore if it reports duck.sh as being modified. Should you have any modified scripts or templates they can be reset to the latest version with git checkout -- scripts/ .templates/ With the new latest version of the project you can now use the menu to build your stack. If there is a particular container you would like to update its template then you can select that at the overwrite option for your container. You have the choice to not to overwrite, preserve env files or to completely overwrite any changes (passwords) After your stack had been rebuild you can run docker-compose up -d to pull in the latest changes. If you have not update your images in a while consider running the ./scripts/update.sh to get the latest version of the image from Docker hub as well","title":"Updating the project"},{"location":"Updating-the-Project/#updating-the-project","text":"If you ran the git checkout -- 'git ls-files -m' as suggested in the old wiki entry then please check your duck.sh because it removed your domain and token Periodically updates are made to project which include new or modified container template, changes to backups or additional features. As these are released your local copy of this project will become out of date. This section deals with how to bring your project to the latest published state. Git offers build in functionality to fetch the latest changes. git pull origin master will fetch the latest changes from GitHub without overwriting files that you have modified yourself. If you have done a local commit then your project may to handle a merge conflict. This can be verified by running git status . You can ignore if it reports duck.sh as being modified. Should you have any modified scripts or templates they can be reset to the latest version with git checkout -- scripts/ .templates/ With the new latest version of the project you can now use the menu to build your stack. If there is a particular container you would like to update its template then you can select that at the overwrite option for your container. You have the choice to not to overwrite, preserve env files or to completely overwrite any changes (passwords) After your stack had been rebuild you can run docker-compose up -d to pull in the latest changes. If you have not update your images in a while consider running the ./scripts/update.sh to get the latest version of the image from Docker hub as well","title":"Updating the project"},{"location":"gcgarner-migration/","text":"Migrating from gcgarner to SensorsIot These instructions explain how to migrate from gcgarner/IOTstack to SensorsIot/IOTstack . Migrating to SensorsIot/IOTstack was fairly easy when this repository was first forked from gcgarner/IOTstack. Unfortunately, what was a fairly simple switching procedure no longer works properly because conflicts have emerged. The probability of conflicts developing increases as a function of time since the fork. Conflicts were and are pretty much inevitable so a more involved procedure is needed. Migration Steps Step 1 \u2013 Check your assumptions Make sure that you are, actually , on gcgarner. Don't assume! $ git remote -v origin https://github.com/gcgarner/IOTstack.git (fetch) origin https://github.com/gcgarner/IOTstack.git (push) Do not proceed if you don't see those URLs! Step 2 \u2013 Take IOTstack down Take your stack down. This is not strictly necessary but we'll be moving the goalposts a bit so it's better to be on the safe side. $ cd ~/IOTstack $ docker-compose down Step 3 \u2013 Choose your migration method There are two basic approaches to switching from gcgarner/IOTstack to SensorsIot/IOTstack: Migration by changing upstream repository Migration by clone and merge You can think of the first as \"working with git\" while the second is \"using brute force\". The first approach will work if you haven't tried any other migration steps and/or have not made too many changes to items in your gcgarner/IOTstack that are under git control. If you are already stuck or you try the first approach and get a mess, or it all looks far too hard to sort out, then try the Migration by clone and merge approach. Migration Option 1 \u2013 change upstream repository Check for local changes Make sure you are on the master branch (you probably are so this is just a precaution), and then see if Git thinks you have made any local changes: $ cd ~/IOTstack $ git checkout master $ git status If Git reports any \"modified\" files, those will probably get in the way of a successful migration so it's a good idea to get those out of the way. For example, suppose you edited menu.sh at some point. Git would report that as: modified: menu.sh The simplest way to deal with modified files is to rename them to move them out of the way, and then restore the original: Rename your customised version by adding your initials to the end of the filename. Later, you can come back and compare your customised version with the version from GitHub and see if you want to preserve any changes. Here I'm assuming your initials are \"jqh\": $ mv menu.sh menu.sh.jqh Tell git to restore the unmodified version: $ git checkout -- menu.sh Now, repeat the Git command that complained about the file: $ git status The modified file will show up as \"untracked\" which is OK (ignore it) ``` Untracked files: (use \"git add ...\" to include in what will be committed) menu.sh.jqh ``` Synchronise with gcgarner on GitHub Make sure your local copy of gcgarner is in sync with GitHub. $ git pull Get rid of any upstream reference There may or may not be any \"upstream\" set. The most likely reason for this to happen is if you used your local copy as the basis of a Pull Request. The next command will probably return an error, which you should ignore. It's just a precaution. $ git remote remove upstream Point to SensorsIot Change your local repository to point to SensorsIot. $ git remote set-url origin https://github.com/SensorsIot/IOTstack.git Synchronise with SensorsIot on GitHub This is where things can get a bit tricky so please read these instructions carefully before you proceed. When you run the next command, it will probably give you a small fright by opening a text-editor window. Don't panic - just keep reading. Now, run this command: $ git pull -X theirs origin master The text editor window will look something like this: Merge branch 'master' of https://github.com/SensorsIot/IOTstack # Please enter a commit message to explain why this merge is necessary, # especially if it merges an updated upstream into a topic branch. # # Lines starting with '#' will be ignored, and an empty message aborts # the commit. The first line is a pre-prepared commit message, the remainder is boilerplate instructions which you can ignore. Exactly which text editor opens is a function of your EDITOR environment variable and the core.editor set in your global Git configuration. If you: remember changing EDITOR and/or core.editor then, presumably, you will know how to interact with your chosen text editor. You don't need to make any changes to this file. All you need to do is save the file and exit; don't remember changing either EDITOR or core.editor then the editor will probably be the default vi (aka vim ). You need to type \":wq\" (without the quotes) and then press return. The \":\" puts vi into command mode, the \"w\" says \"save the file\" and \"q\" means \"quit vi \". Pressing return runs the commands. Git will display a long list of stuff. It's very tempting to ignore it but it's a good idea to take a closer look, particularly for signs of error or any lines beginning with: Auto-merging At the time of writing, you can expect Git to mention these two files: Auto-merging menu.sh Auto-merging .templates/zigbee2mqtt/service.yml Those are known issues and the merge strategy -X theirs on the git pull command you have just executed deals with both, correctly, by preferring the SensorsIot version. Similar conflicts may emerge in future and those will probably be dealt with, correctly, by the same merge strategy. Nevertheless, you should still check the output very carefully for other signs of merge conflict so that you can at least be alive to the possibility that the affected files may warrant closer inspection. For example, suppose you saw: Auto-merging .templates/someRandomService/service.yml If you don't use someRandomService then you could safely ignore this on the basis that it was \"probably right\". However, if you did use that service and it started to misbehave after migration, you would know that the service.yml file was a good place to start looking for explanations. Finish with a pull At this point, only the migrated master branch is present on your local copy of the repository. The next command brings you fully in-sync with GitHub: $ git pull Migration Option 2 \u2013 clone and merge If you have been following the process correctly, your IOTstack will already be down. Rename your existing IOTstack folder Move your old IOTstack folder out of the way, like this: $ cd ~ $ mv IOTstack IOTstack.old Note: You should not need sudo for the mv command but it is OK to use it if necessary. Fetch a clean clone of SensorsIot/IOTstack $ git clone https://github.com/SensorsIot/IOTstack.git ~/IOTstack Explore the result: $ tree -aFL 1 --noreport ~/IOTstack /home/pi/IOTstack \u251c\u2500\u2500 .bash_aliases \u251c\u2500\u2500 .git/ \u251c\u2500\u2500 .github/ \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 .native/ \u251c\u2500\u2500 .templates/ \u251c\u2500\u2500 .tmp/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 docs/ \u251c\u2500\u2500 duck/ \u251c\u2500\u2500 install.sh* \u251c\u2500\u2500 menu.sh* \u251c\u2500\u2500 mkdocs.yml \u2514\u2500\u2500 scripts/ Note: If the tree command is not installed for some reason, use ls -A1F ~/IOTstack . Observe what is not there: There is no docker-compose.yml There is no backups directory There is no services directory There is no volumes directory From this, it should be self-evident that a clean checkout from GitHub is the factory for all IOTstack installations, while the contents of backups , services , volumes and docker-compose.yml represent each user's individual choices, configuration options and data. Merge old into new Execute the following commands: $ mv ~/IOTstack.old/docker-compose.yml ~/IOTstack $ mv ~/IOTstack.old/services ~/IOTstack $ sudo mv ~/IOTstack.old/volumes ~/IOTstack You should not need to use sudo for the first two commands. However, if you get a permissions conflict on either, you should proceed like this: docker-compose.yml $ sudo mv ~/IOTstack.old/docker-compose.yml ~/IOTstack $ sudo chown pi:pi ~/IOTstack/docker-compose.yml services $ sudo mv ~/IOTstack.old/services ~/IOTstack $ sudo chown -R pi:pi ~/IOTstack/services There is no need to migrate the backups directory. You are better off creating it by hand: $ mkdir ~/IOTstack/backups Step 4 \u2013 Choose your menu If you have reached this point, you have migrated to SensorsIot/IOTstack where you are on the \"master\" branch. This implies \"new menu\". The choice of menu is entirely up to you. Differences include: New menu takes a lot more screen real-estate than old menu. If you do a fair bit of work on small screens (eg iPad) you might find it hard to work with new menu. New menu creates a large number of internal Docker networks whereas old menu has one internal network to rule them all . The practical consequence is that most users see error messages for networks being defined but not used, and occasionally run into problems where two containers can't talk to each other without tinkering with the networks. Neither of those happen under old menu. See Issue 245 if you want more information on this. New menu has moved the definition of environment variables into docker-compose.yml . Old menu keeps environment variables in \"environment files\" in ~/IOTstack/services . There is no \"right\" or \"better\" about either approach. It's just something to be aware of. Under new menu, the service.yml files in ~/IOTstack/.templates have all been left-shifted by two spaces. That means you can no longer use copy and paste to test containers - you're stuck with the extra work of re-adding the spaces. Again, this doesn't matter but you do need to be aware of it. What you give up when you choose old menu is summarised in the following. If a container appears on the right hand side but not the left then it is only available in new menu. old-menu master (new menu) \u251c\u2500\u2500 adminer \u251c\u2500\u2500 adminer \u251c\u2500\u2500 blynk_server \u251c\u2500\u2500 blynk_server \u251c\u2500\u2500 dashmachine \u251c\u2500\u2500 dashmachine \u251c\u2500\u2500 deconz \u251c\u2500\u2500 deconz \u251c\u2500\u2500 diyhue \u251c\u2500\u2500 diyhue \u251c\u2500\u2500 domoticz \u251c\u2500\u2500 domoticz \u251c\u2500\u2500 dozzle \u251c\u2500\u2500 dozzle \u251c\u2500\u2500 espruinohub \u251c\u2500\u2500 espruinohub > \u251c\u2500\u2500 example_template \u251c\u2500\u2500 gitea \u251c\u2500\u2500 gitea \u251c\u2500\u2500 grafana \u251c\u2500\u2500 grafana \u251c\u2500\u2500 heimdall \u251c\u2500\u2500 heimdall > \u251c\u2500\u2500 home_assistant \u251c\u2500\u2500 homebridge \u251c\u2500\u2500 homebridge \u251c\u2500\u2500 homer \u251c\u2500\u2500 homer \u251c\u2500\u2500 influxdb \u251c\u2500\u2500 influxdb \u251c\u2500\u2500 mariadb \u251c\u2500\u2500 mariadb \u251c\u2500\u2500 mosquitto \u251c\u2500\u2500 mosquitto \u251c\u2500\u2500 motioneye \u251c\u2500\u2500 motioneye \u251c\u2500\u2500 nextcloud \u251c\u2500\u2500 nextcloud \u251c\u2500\u2500 nodered \u251c\u2500\u2500 nodered \u251c\u2500\u2500 openhab \u251c\u2500\u2500 openhab \u251c\u2500\u2500 pihole \u251c\u2500\u2500 pihole \u251c\u2500\u2500 plex \u251c\u2500\u2500 plex \u251c\u2500\u2500 portainer \u251c\u2500\u2500 portainer \u251c\u2500\u2500 portainer_agent \u251c\u2500\u2500 portainer_agent \u251c\u2500\u2500 portainer-ce \u251c\u2500\u2500 portainer-ce \u251c\u2500\u2500 postgres \u251c\u2500\u2500 postgres \u251c\u2500\u2500 prometheus \u251c\u2500\u2500 prometheus \u251c\u2500\u2500 python \u251c\u2500\u2500 python \u251c\u2500\u2500 qbittorrent \u251c\u2500\u2500 qbittorrent \u251c\u2500\u2500 rtl_433 \u251c\u2500\u2500 rtl_433 \u251c\u2500\u2500 tasmoadmin \u251c\u2500\u2500 tasmoadmin \u251c\u2500\u2500 telegraf \u251c\u2500\u2500 telegraf \u251c\u2500\u2500 timescaledb \u251c\u2500\u2500 timescaledb \u251c\u2500\u2500 transmission \u251c\u2500\u2500 transmission \u251c\u2500\u2500 webthings_gateway \u251c\u2500\u2500 webthings_gateway \u251c\u2500\u2500 wireguard \u251c\u2500\u2500 wireguard \u2514\u2500\u2500 zigbee2mqtt \u251c\u2500\u2500 zigbee2mqtt > \u2514\u2500\u2500 zigbee2mqtt_assistant You also give up the compose-override.yml functionality. On the other hand, Docker has its own docker-compose.override.yml which works with both menus. If you want to switch to the old menu: $ git checkout old-menu Any time you want to switch back to the new menu: $ git checkout master You can switch back and forth as much as you like and as often as you like. It's no harm, no foul. The branch you are on just governs what you see when you run: $ ./menu.sh Although you can freely change branches, it's probably not a good idea to try to mix-and-match your menus. Pick one menu and stick to it. Even so, nothing will change until you run your chosen menu to completion and allow it to generate a new docker-compose.yml . Step 5 \u2013 Bring up your stack Unless you have gotten ahead of yourself and have already run the menu (old or new) then nothing will have changed in the parts of your ~/IOTstack folder that define your IOTstack implementation. You can safely: $ docker-compose up -d See also There is another gist Installing Docker for IOTstack which explains how to overcome problems with outdated Docker and Docker-Compose installations. Depending on the age of your gcgarner installation, you may run into problems which will be cured by working through that gist.","title":"Migrating from gcgarner to SensorsIot"},{"location":"gcgarner-migration/#migrating-from-gcgarner-to-sensorsiot","text":"These instructions explain how to migrate from gcgarner/IOTstack to SensorsIot/IOTstack . Migrating to SensorsIot/IOTstack was fairly easy when this repository was first forked from gcgarner/IOTstack. Unfortunately, what was a fairly simple switching procedure no longer works properly because conflicts have emerged. The probability of conflicts developing increases as a function of time since the fork. Conflicts were and are pretty much inevitable so a more involved procedure is needed.","title":"Migrating from gcgarner to SensorsIot"},{"location":"gcgarner-migration/#migration-steps","text":"","title":" Migration Steps "},{"location":"gcgarner-migration/#step-1-check-your-assumptions","text":"Make sure that you are, actually , on gcgarner. Don't assume! $ git remote -v origin https://github.com/gcgarner/IOTstack.git (fetch) origin https://github.com/gcgarner/IOTstack.git (push) Do not proceed if you don't see those URLs!","title":" Step 1 \u2013 Check your assumptions "},{"location":"gcgarner-migration/#step-2-take-iotstack-down","text":"Take your stack down. This is not strictly necessary but we'll be moving the goalposts a bit so it's better to be on the safe side. $ cd ~/IOTstack $ docker-compose down","title":" Step 2 \u2013 Take IOTstack down "},{"location":"gcgarner-migration/#step-3-choose-your-migration-method","text":"There are two basic approaches to switching from gcgarner/IOTstack to SensorsIot/IOTstack: Migration by changing upstream repository Migration by clone and merge You can think of the first as \"working with git\" while the second is \"using brute force\". The first approach will work if you haven't tried any other migration steps and/or have not made too many changes to items in your gcgarner/IOTstack that are under git control. If you are already stuck or you try the first approach and get a mess, or it all looks far too hard to sort out, then try the Migration by clone and merge approach.","title":" Step 3 \u2013 Choose your migration method "},{"location":"gcgarner-migration/#migration-option-1-change-upstream-repository","text":"","title":" Migration Option 1 \u2013 change upstream repository "},{"location":"gcgarner-migration/#check-for-local-changes","text":"Make sure you are on the master branch (you probably are so this is just a precaution), and then see if Git thinks you have made any local changes: $ cd ~/IOTstack $ git checkout master $ git status If Git reports any \"modified\" files, those will probably get in the way of a successful migration so it's a good idea to get those out of the way. For example, suppose you edited menu.sh at some point. Git would report that as: modified: menu.sh The simplest way to deal with modified files is to rename them to move them out of the way, and then restore the original: Rename your customised version by adding your initials to the end of the filename. Later, you can come back and compare your customised version with the version from GitHub and see if you want to preserve any changes. Here I'm assuming your initials are \"jqh\": $ mv menu.sh menu.sh.jqh Tell git to restore the unmodified version: $ git checkout -- menu.sh Now, repeat the Git command that complained about the file: $ git status The modified file will show up as \"untracked\" which is OK (ignore it) ``` Untracked files: (use \"git add ...\" to include in what will be committed) menu.sh.jqh ```","title":" Check for local changes "},{"location":"gcgarner-migration/#synchronise-with-gcgarner-on-github","text":"Make sure your local copy of gcgarner is in sync with GitHub. $ git pull","title":" Synchronise with gcgarner on GitHub "},{"location":"gcgarner-migration/#get-rid-of-any-upstream-reference","text":"There may or may not be any \"upstream\" set. The most likely reason for this to happen is if you used your local copy as the basis of a Pull Request. The next command will probably return an error, which you should ignore. It's just a precaution. $ git remote remove upstream","title":" Get rid of any upstream reference "},{"location":"gcgarner-migration/#point-to-sensorsiot","text":"Change your local repository to point to SensorsIot. $ git remote set-url origin https://github.com/SensorsIot/IOTstack.git","title":" Point to SensorsIot "},{"location":"gcgarner-migration/#synchronise-with-sensorsiot-on-github","text":"This is where things can get a bit tricky so please read these instructions carefully before you proceed. When you run the next command, it will probably give you a small fright by opening a text-editor window. Don't panic - just keep reading. Now, run this command: $ git pull -X theirs origin master The text editor window will look something like this: Merge branch 'master' of https://github.com/SensorsIot/IOTstack # Please enter a commit message to explain why this merge is necessary, # especially if it merges an updated upstream into a topic branch. # # Lines starting with '#' will be ignored, and an empty message aborts # the commit. The first line is a pre-prepared commit message, the remainder is boilerplate instructions which you can ignore. Exactly which text editor opens is a function of your EDITOR environment variable and the core.editor set in your global Git configuration. If you: remember changing EDITOR and/or core.editor then, presumably, you will know how to interact with your chosen text editor. You don't need to make any changes to this file. All you need to do is save the file and exit; don't remember changing either EDITOR or core.editor then the editor will probably be the default vi (aka vim ). You need to type \":wq\" (without the quotes) and then press return. The \":\" puts vi into command mode, the \"w\" says \"save the file\" and \"q\" means \"quit vi \". Pressing return runs the commands. Git will display a long list of stuff. It's very tempting to ignore it but it's a good idea to take a closer look, particularly for signs of error or any lines beginning with: Auto-merging At the time of writing, you can expect Git to mention these two files: Auto-merging menu.sh Auto-merging .templates/zigbee2mqtt/service.yml Those are known issues and the merge strategy -X theirs on the git pull command you have just executed deals with both, correctly, by preferring the SensorsIot version. Similar conflicts may emerge in future and those will probably be dealt with, correctly, by the same merge strategy. Nevertheless, you should still check the output very carefully for other signs of merge conflict so that you can at least be alive to the possibility that the affected files may warrant closer inspection. For example, suppose you saw: Auto-merging .templates/someRandomService/service.yml If you don't use someRandomService then you could safely ignore this on the basis that it was \"probably right\". However, if you did use that service and it started to misbehave after migration, you would know that the service.yml file was a good place to start looking for explanations.","title":" Synchronise with SensorsIot on GitHub "},{"location":"gcgarner-migration/#finish-with-a-pull","text":"At this point, only the migrated master branch is present on your local copy of the repository. The next command brings you fully in-sync with GitHub: $ git pull","title":" Finish with a pull "},{"location":"gcgarner-migration/#migration-option-2-clone-and-merge","text":"If you have been following the process correctly, your IOTstack will already be down.","title":" Migration Option 2 \u2013 clone and merge "},{"location":"gcgarner-migration/#rename-your-existing-iotstack-folder","text":"Move your old IOTstack folder out of the way, like this: $ cd ~ $ mv IOTstack IOTstack.old Note: You should not need sudo for the mv command but it is OK to use it if necessary.","title":" Rename your existing IOTstack folder "},{"location":"gcgarner-migration/#fetch-a-clean-clone-of-sensorsiotiotstack","text":"$ git clone https://github.com/SensorsIot/IOTstack.git ~/IOTstack Explore the result: $ tree -aFL 1 --noreport ~/IOTstack /home/pi/IOTstack \u251c\u2500\u2500 .bash_aliases \u251c\u2500\u2500 .git/ \u251c\u2500\u2500 .github/ \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 .native/ \u251c\u2500\u2500 .templates/ \u251c\u2500\u2500 .tmp/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 docs/ \u251c\u2500\u2500 duck/ \u251c\u2500\u2500 install.sh* \u251c\u2500\u2500 menu.sh* \u251c\u2500\u2500 mkdocs.yml \u2514\u2500\u2500 scripts/ Note: If the tree command is not installed for some reason, use ls -A1F ~/IOTstack . Observe what is not there: There is no docker-compose.yml There is no backups directory There is no services directory There is no volumes directory From this, it should be self-evident that a clean checkout from GitHub is the factory for all IOTstack installations, while the contents of backups , services , volumes and docker-compose.yml represent each user's individual choices, configuration options and data.","title":" Fetch a clean clone of SensorsIot/IOTstack "},{"location":"gcgarner-migration/#merge-old-into-new","text":"Execute the following commands: $ mv ~/IOTstack.old/docker-compose.yml ~/IOTstack $ mv ~/IOTstack.old/services ~/IOTstack $ sudo mv ~/IOTstack.old/volumes ~/IOTstack You should not need to use sudo for the first two commands. However, if you get a permissions conflict on either, you should proceed like this: docker-compose.yml $ sudo mv ~/IOTstack.old/docker-compose.yml ~/IOTstack $ sudo chown pi:pi ~/IOTstack/docker-compose.yml services $ sudo mv ~/IOTstack.old/services ~/IOTstack $ sudo chown -R pi:pi ~/IOTstack/services There is no need to migrate the backups directory. You are better off creating it by hand: $ mkdir ~/IOTstack/backups","title":" Merge old into new "},{"location":"gcgarner-migration/#step-4-choose-your-menu","text":"If you have reached this point, you have migrated to SensorsIot/IOTstack where you are on the \"master\" branch. This implies \"new menu\". The choice of menu is entirely up to you. Differences include: New menu takes a lot more screen real-estate than old menu. If you do a fair bit of work on small screens (eg iPad) you might find it hard to work with new menu. New menu creates a large number of internal Docker networks whereas old menu has one internal network to rule them all . The practical consequence is that most users see error messages for networks being defined but not used, and occasionally run into problems where two containers can't talk to each other without tinkering with the networks. Neither of those happen under old menu. See Issue 245 if you want more information on this. New menu has moved the definition of environment variables into docker-compose.yml . Old menu keeps environment variables in \"environment files\" in ~/IOTstack/services . There is no \"right\" or \"better\" about either approach. It's just something to be aware of. Under new menu, the service.yml files in ~/IOTstack/.templates have all been left-shifted by two spaces. That means you can no longer use copy and paste to test containers - you're stuck with the extra work of re-adding the spaces. Again, this doesn't matter but you do need to be aware of it. What you give up when you choose old menu is summarised in the following. If a container appears on the right hand side but not the left then it is only available in new menu. old-menu master (new menu) \u251c\u2500\u2500 adminer \u251c\u2500\u2500 adminer \u251c\u2500\u2500 blynk_server \u251c\u2500\u2500 blynk_server \u251c\u2500\u2500 dashmachine \u251c\u2500\u2500 dashmachine \u251c\u2500\u2500 deconz \u251c\u2500\u2500 deconz \u251c\u2500\u2500 diyhue \u251c\u2500\u2500 diyhue \u251c\u2500\u2500 domoticz \u251c\u2500\u2500 domoticz \u251c\u2500\u2500 dozzle \u251c\u2500\u2500 dozzle \u251c\u2500\u2500 espruinohub \u251c\u2500\u2500 espruinohub > \u251c\u2500\u2500 example_template \u251c\u2500\u2500 gitea \u251c\u2500\u2500 gitea \u251c\u2500\u2500 grafana \u251c\u2500\u2500 grafana \u251c\u2500\u2500 heimdall \u251c\u2500\u2500 heimdall > \u251c\u2500\u2500 home_assistant \u251c\u2500\u2500 homebridge \u251c\u2500\u2500 homebridge \u251c\u2500\u2500 homer \u251c\u2500\u2500 homer \u251c\u2500\u2500 influxdb \u251c\u2500\u2500 influxdb \u251c\u2500\u2500 mariadb \u251c\u2500\u2500 mariadb \u251c\u2500\u2500 mosquitto \u251c\u2500\u2500 mosquitto \u251c\u2500\u2500 motioneye \u251c\u2500\u2500 motioneye \u251c\u2500\u2500 nextcloud \u251c\u2500\u2500 nextcloud \u251c\u2500\u2500 nodered \u251c\u2500\u2500 nodered \u251c\u2500\u2500 openhab \u251c\u2500\u2500 openhab \u251c\u2500\u2500 pihole \u251c\u2500\u2500 pihole \u251c\u2500\u2500 plex \u251c\u2500\u2500 plex \u251c\u2500\u2500 portainer \u251c\u2500\u2500 portainer \u251c\u2500\u2500 portainer_agent \u251c\u2500\u2500 portainer_agent \u251c\u2500\u2500 portainer-ce \u251c\u2500\u2500 portainer-ce \u251c\u2500\u2500 postgres \u251c\u2500\u2500 postgres \u251c\u2500\u2500 prometheus \u251c\u2500\u2500 prometheus \u251c\u2500\u2500 python \u251c\u2500\u2500 python \u251c\u2500\u2500 qbittorrent \u251c\u2500\u2500 qbittorrent \u251c\u2500\u2500 rtl_433 \u251c\u2500\u2500 rtl_433 \u251c\u2500\u2500 tasmoadmin \u251c\u2500\u2500 tasmoadmin \u251c\u2500\u2500 telegraf \u251c\u2500\u2500 telegraf \u251c\u2500\u2500 timescaledb \u251c\u2500\u2500 timescaledb \u251c\u2500\u2500 transmission \u251c\u2500\u2500 transmission \u251c\u2500\u2500 webthings_gateway \u251c\u2500\u2500 webthings_gateway \u251c\u2500\u2500 wireguard \u251c\u2500\u2500 wireguard \u2514\u2500\u2500 zigbee2mqtt \u251c\u2500\u2500 zigbee2mqtt > \u2514\u2500\u2500 zigbee2mqtt_assistant You also give up the compose-override.yml functionality. On the other hand, Docker has its own docker-compose.override.yml which works with both menus. If you want to switch to the old menu: $ git checkout old-menu Any time you want to switch back to the new menu: $ git checkout master You can switch back and forth as much as you like and as often as you like. It's no harm, no foul. The branch you are on just governs what you see when you run: $ ./menu.sh Although you can freely change branches, it's probably not a good idea to try to mix-and-match your menus. Pick one menu and stick to it. Even so, nothing will change until you run your chosen menu to completion and allow it to generate a new docker-compose.yml .","title":" Step 4 \u2013 Choose your menu "},{"location":"gcgarner-migration/#step-5-bring-up-your-stack","text":"Unless you have gotten ahead of yourself and have already run the menu (old or new) then nothing will have changed in the parts of your ~/IOTstack folder that define your IOTstack implementation. You can safely: $ docker-compose up -d","title":" Step 5 \u2013 Bring up your stack "},{"location":"gcgarner-migration/#see-also","text":"There is another gist Installing Docker for IOTstack which explains how to overcome problems with outdated Docker and Docker-Compose installations. Depending on the age of your gcgarner installation, you may run into problems which will be cured by working through that gist.","title":" See also "},{"location":"Containers/Adminer/","text":"Adminer References Docker Website About This is a nice tool for managing databases. Web interface has moved to port 9080. There was an issue where openHAB and Adminer were using the same ports. If you have an port conflict edit the docker-compose.yml and under the adminer service change the line to read: ports: - 9080:8080","title":"Adminer"},{"location":"Containers/Adminer/#adminer","text":"","title":"Adminer"},{"location":"Containers/Adminer/#references","text":"Docker Website","title":"References"},{"location":"Containers/Adminer/#about","text":"This is a nice tool for managing databases. Web interface has moved to port 9080. There was an issue where openHAB and Adminer were using the same ports. If you have an port conflict edit the docker-compose.yml and under the adminer service change the line to read: ports: - 9080:8080","title":"About"},{"location":"Containers/Blynk_server/","text":"Blynk server This is a custom implementation of Blynk Server blynk_server: build: ./services/blynk_server/. container_name: blynk_server restart: unless-stopped ports: - 8180:8080 - 8441:8441 - 9443:9443 volumes: - ./volumes/blynk_server/data:/data To connect to the admin interface navigate to <your pis IP>:9443/admin I don't know anything about this service so you will need to read though the setup on the Project Homepage When setting up the application on your mobile be sure to select custom setup here Writeup From @877dev Getting started Log into admin panel at https://youripaddress:9443/admin (Use your Pi's IP address, and ignore Chrome warning). Default credentials: user:admin@blynk.cc pass:admin Change username and password Click on Users > \"email address\" and edit email, name and password. Save changes Restarting the container using Portainer may be required to take effect. Setup gmail Optional step, useful for getting the auth token emailed to you. (To be added once confirmed working....) iOS/Android app setup Login the app as per the photos HERE Press \"New Project\" Give it a name, choose device \"Raspberry Pi 3 B\" so you have plenty of virtual pins available, and lastly select WiFi. Create project and the auth token will be emailed to you (if emails configured). You can also find the token in app under the phone app settings, or in the admin web interface by clicking Users>\"email address\" and scroll down to token. Quick usage guide for app Press on the empty page, the widgets will appear from the right. Select your widget, let's say a button. It appears on the page, press on it to configure. Give it a name and colour if you want. Press on PIN, and select virtual. Choose any pin i.e. V0 Press ok. To start the project running, press top right Play button. You will get an offline message, because no devices are connected to your project via the token. Enter node red..... Node red Install node-red-contrib-blynk-ws from pallette manager Drag a \"write event\" node into your flow, and connect to a debug node Configure the Blynk node for the first time: URL: wss://youripaddress:9443/websockets more info HERE Enter your auth token from before and save/exit. When you deploy the flow, notice the app shows connected message, as does the Blynk node. Press the button on the app, you will notice the payload is sent to the debug node. What next? Further information and advanced setup: https://github.com/blynkkk/blynk-server Check the documentation: https://docs.blynk.cc/ Visit the community forum pages: https://community.blynk.cc/ Interesting post by Peter Knight on MQTT/Node Red flows: https://community.blynk.cc/t/my-home-automation-projects-built-with-mqtt-and-node-red/29045 Some Blynk flow examples: https://github.com/877dev/Node-Red-flow-examples","title":"Blynk server"},{"location":"Containers/Blynk_server/#blynk-server","text":"This is a custom implementation of Blynk Server blynk_server: build: ./services/blynk_server/. container_name: blynk_server restart: unless-stopped ports: - 8180:8080 - 8441:8441 - 9443:9443 volumes: - ./volumes/blynk_server/data:/data To connect to the admin interface navigate to <your pis IP>:9443/admin I don't know anything about this service so you will need to read though the setup on the Project Homepage When setting up the application on your mobile be sure to select custom setup here Writeup From @877dev","title":"Blynk server"},{"location":"Containers/Blynk_server/#getting-started","text":"Log into admin panel at https://youripaddress:9443/admin (Use your Pi's IP address, and ignore Chrome warning). Default credentials: user:admin@blynk.cc pass:admin","title":"Getting started"},{"location":"Containers/Blynk_server/#change-username-and-password","text":"Click on Users > \"email address\" and edit email, name and password. Save changes Restarting the container using Portainer may be required to take effect.","title":"Change username and password"},{"location":"Containers/Blynk_server/#setup-gmail","text":"Optional step, useful for getting the auth token emailed to you. (To be added once confirmed working....)","title":"Setup gmail"},{"location":"Containers/Blynk_server/#iosandroid-app-setup","text":"Login the app as per the photos HERE Press \"New Project\" Give it a name, choose device \"Raspberry Pi 3 B\" so you have plenty of virtual pins available, and lastly select WiFi. Create project and the auth token will be emailed to you (if emails configured). You can also find the token in app under the phone app settings, or in the admin web interface by clicking Users>\"email address\" and scroll down to token.","title":"iOS/Android app setup"},{"location":"Containers/Blynk_server/#quick-usage-guide-for-app","text":"Press on the empty page, the widgets will appear from the right. Select your widget, let's say a button. It appears on the page, press on it to configure. Give it a name and colour if you want. Press on PIN, and select virtual. Choose any pin i.e. V0 Press ok. To start the project running, press top right Play button. You will get an offline message, because no devices are connected to your project via the token. Enter node red.....","title":"Quick usage guide for app"},{"location":"Containers/Blynk_server/#node-red","text":"Install node-red-contrib-blynk-ws from pallette manager Drag a \"write event\" node into your flow, and connect to a debug node Configure the Blynk node for the first time: URL: wss://youripaddress:9443/websockets more info HERE Enter your auth token from before and save/exit. When you deploy the flow, notice the app shows connected message, as does the Blynk node. Press the button on the app, you will notice the payload is sent to the debug node.","title":"Node red"},{"location":"Containers/Blynk_server/#what-next","text":"Further information and advanced setup: https://github.com/blynkkk/blynk-server Check the documentation: https://docs.blynk.cc/ Visit the community forum pages: https://community.blynk.cc/ Interesting post by Peter Knight on MQTT/Node Red flows: https://community.blynk.cc/t/my-home-automation-projects-built-with-mqtt-and-node-red/29045 Some Blynk flow examples: https://github.com/877dev/Node-Red-flow-examples","title":"What next?"},{"location":"Containers/DashMachine/","text":"DashMachine References Homepage Docker Web Interface The web UI can be found on \"your_ip\":5000 . The default credentials are: * User: admin * Password: admin About DashMachine DashMachine is a web application bookmark dashboard. It allows you to have all your application bookmarks available in one place, grouped and organized how you want to see them. Within the context of IOTstack, DashMachine can help you organize your deployed services.","title":"DashMachine"},{"location":"Containers/DashMachine/#dashmachine","text":"","title":"DashMachine"},{"location":"Containers/DashMachine/#references","text":"Homepage Docker","title":"References"},{"location":"Containers/DashMachine/#web-interface","text":"The web UI can be found on \"your_ip\":5000 . The default credentials are: * User: admin * Password: admin","title":"Web Interface"},{"location":"Containers/DashMachine/#about-dashmachine","text":"DashMachine is a web application bookmark dashboard. It allows you to have all your application bookmarks available in one place, grouped and organized how you want to see them. Within the context of IOTstack, DashMachine can help you organize your deployed services.","title":"About DashMachine"},{"location":"Containers/EspruinoHub/","text":"Espruinohub This is a testing container I tried it however the container keeps restarting docker logs espruinohub I get \"BLE Broken?\" but could just be i dont have any BLE devices nearby web interface is on \"{your_Pis_IP}:1888\" see EspruinoHub#status--websocket-mqtt--espruino-web-ide for other details. there were no recommendations for persistent data volumes. so docker-compose down may destroy all you configurations so use docker-compose stop in stead Please check existing issues if you encounter a problem, and then open a new issue if your problem has not been reported.","title":"Espruinohub"},{"location":"Containers/EspruinoHub/#espruinohub","text":"This is a testing container I tried it however the container keeps restarting docker logs espruinohub I get \"BLE Broken?\" but could just be i dont have any BLE devices nearby web interface is on \"{your_Pis_IP}:1888\" see EspruinoHub#status--websocket-mqtt--espruino-web-ide for other details. there were no recommendations for persistent data volumes. so docker-compose down may destroy all you configurations so use docker-compose stop in stead Please check existing issues if you encounter a problem, and then open a new issue if your problem has not been reported.","title":"Espruinohub"},{"location":"Containers/Grafana/","text":"Grafana References Docker Website Setting your time-zone The default ~/IOTstack/services/grafana/grafana.env contains this line: #TZ=Africa/Johannesburg Uncomment that line and change the right hand side to your own timezone . Security If Grafana has just been installed but has never been launched then the following will be true: The folder ~/IOTstack/volumes/grafana will not exist; and The file ~/IOTstack/services/grafana/grafana.env will contain these lines: ``` GF_SECURITY_ADMIN_USER=admin GF_SECURITY_ADMIN_PASSWORD=admin ``` You should see those lines as documentation rather than something you are being invited to edit. It is telling you that the default administative user for Grafana is \"admin\" and that the default password for that user is \"admin\". If you do not change anything then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will: Expect you to login as user \"admin\" with password \"admin\"; and then Force you to change the default password to something else. Thereafter, you will login as \"admin\" with whatever password you chose. You can change the administrator's password as often as you like via the web UI ( profile button, change password tab). This method (of not touching these two keys in grafana.env ) is the recommended approach. Please try to resist the temptation to fiddle! I want a different admin username (not recommended) If, before you bring up the stack for the first time, you do this: GF_SECURITY_ADMIN_USER=jack #GF_SECURITY_ADMIN_PASSWORD=admin then, when you bring up the stack and connect on port 3000, Grafana will: Expect you to login as user \"jack\" with password \"admin\"; and then Force you to change the default password to something else. Thereafter, \"jack\" will be the Grafana administrator and you will login with the password you chose, until you decide to change the password to something else via the web UI. Don't think you can come back later and tweak the Grafana administrator name in the environment variables. It doesn't work that way. It's a one-shot. I want a different default admin password (not recommended) Well, first off, the two methods above both make you set a different password on first login so there probably isn't much point to this. But, if you really insist\u2026 If, before you bring up the stack for the first time, you do this: #GF_SECURITY_ADMIN_USER=admin GF_SECURITY_ADMIN_PASSWORD=jack then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will expect you to login as user \"admin\" with password \"jack\". Grafana will not force you to change the password on first login but you will still be able to change it via the web UI. But don't think you can come back later and change the password in the environment variables. It doesn't work that way. It's a one-shot. I want to change everything (not recommended) If, before you bring up the stack for the first time, you do this: GF_SECURITY_ADMIN_USER=bill GF_SECURITY_ADMIN_PASSWORD=jack then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will expect you to login as user \"bill\" with password \"jack\". Grafana will not force you to change the password on first login but you will still be able to change it via the web UI. But don't think you can come back later and tweak either the username or password in the environment variables. It doesn't work that way. It's a one-shot. Distilling it down Before Grafana is launched for the first time: GF_SECURITY_ADMIN_USER has a default value of \"admin\". You can explicitly set it to \"admin\" or some other value. Whatever option you choose then that's the account name of Grafana's administrative user. But choosing any value other than \"admin\" is probably a bad idea. GF_SECURITY_ADMIN_PASSWORD has a default value of \"admin\". You can explicitly set it to \"admin\" or some other value. If its value is \"admin\" then you will be forced to change it the first time you login to Grafana. If its value is something other than \"admin\" then that will be the password until you change it via the web UI. These two environment keys only work for the first launch of Grafana. Once Grafana has been launched, you can never change either the username or the password by editing grafana.env . For this reason, it is better to leave grafana.env in its shrink-wrapped state. Your first login is as \"admin/admin\" and then you set the password you actually want when Grafana prompts you to change it. HELP \u2013 I forgot my Grafana admin password! Assuming your IOTstack is up, the magic incantation is: $ docker exec grafana grafana-cli --homepath \"/usr/share/grafana\" admin reset-admin-password \"admin\" Then, use a browser to connect to your Raspberry Pi on port 3000. Grafana will: Expect you login as user \"admin\" with password \"admin\"; and then Force you to change the default password to something else. This magic incantation assumes that your administrative username is \"admin\". If you ignored the advice above and changed the administator username to something else then all bets are off. It might work anyway but we haven't tested it. Sorry. But that's why we said changing the username was not recommended. Overriding Grafana settings Grafana documentation contains a list of settings . Settings are described in terms of how they appear in \".ini\" files. An example of the sort of thing you might want to do is to enable anonymous access to your Grafana dashboards. The Grafana documentation describes this in \".ini\" format as: [auth.anonymous] enabled = true # Organization name that should be used for unauthenticated users org_name = Main Org. # Role for unauthenticated users, other valid values are `Editor` and `Admin` org_role = Viewer \".ini\" format is not really appropriate in a Docker context. Instead, you use environment variables to override Docker's settings. Environment variables are placed in ~/IOTstack/services/grafana/grafana.env . You need to convert \".ini\" format to environment variable syntax. The rules are: Start with \"GF_\", then Append the [section name], replacing any periods with underscores, then Append the section key \"as is\", then Append an \"=\", then Append the right hand side in quotes. Applying those rules gets you: GF_AUTH_ANONYMOUS_ENABLED=\"true\" GF_AUTH_ANONYMOUS_ORG_NAME=\"Main Org.\" GF_AUTH_ANONYMOUS_ORG_ROLE=\"Viewer\" It is not strictly necessary to encapsulate every right hand side value in quotes. In the above, both \"true\" and \"Viewer\" would work without quotes, whereas \"Main Org.\" needs quotes because of the embedded space. After you have changed ~/IOTstack/services/grafana/grafana.env , you need to propagate the changes into the Grafana container: $ cd ~/IOTstack $ docker-compose stop grafana $ docker-compose up -d In theory, the second command could be omitted, or both the second and third commands could be replaced with \"docker-compose restart grafana\" but experience suggests stopping the container is more reliable. A slightly more real-world example would involve choosing a different default organisation name for anonymous access. This example uses \"ChezMoi\". First, the environment key needs to be set to that value: GF_AUTH_ANONYMOUS_ORG_NAME=ChezMoi Then that change needs to be propagated into the Grafana container as explained above. Next, Grafana needs to be told that \"ChezMoi\" is the default organisation: Use your browser to login to Grafana as an administrator. From the \"Server Admin\" slide-out menu on the left hand side, choose \"Orgs\". In the list that appears, click on \"Main Org\". This opens an editing panel. Change the \"Name\" field to \"ChezMoi\" and click \"Update\". Sign-out of Grafana. You will be taken back to the login screen. Your URL bar will look something like this: http://myhost.mydomain.com:3000/login 6. Edit the URL to remove the \"login\" suffix and press return. If all your changes were applied successfully, you will have anonymous access and the URL will look something like this: http://myhost.mydomain.com:3000/?orgId=1 HELP \u2013 I made a mess! \"I made a bit of a mess with Grafana. First time user. Steep learning curve. False starts, many. Mistakes, unavoidable. Been there, done that. But now I really need to start from a clean slate. And, yes, I understand there is no undo for this.\" Begin by stopping Grafana: $ cd ~/IOTstack $ docker-compose stop grafana You have two options: Destroy your settings and dashboards but retain any plugins you may have installed: $ sudo rm ~/IOTstack/volumes/grafana/data/grafana.db Nuke everything (triple-check this command before you hit return): $ sudo rm -rf ~/IOTstack/volumes/grafana/data This is where you should edit ~/IOTstack/services/grafana/grafana.env to correct any problems (such as choosing an administrative username other than \"admin\"). When you are ready, bring Grafana back up again: $ cd ~/IOTstack $ docker-compose up -d Grafana will automatically recreate everything it needs. You will be able to login as \"admin/admin\".","title":"Grafana"},{"location":"Containers/Grafana/#grafana","text":"","title":"Grafana"},{"location":"Containers/Grafana/#references","text":"Docker Website","title":"References"},{"location":"Containers/Grafana/#setting-your-time-zone","text":"The default ~/IOTstack/services/grafana/grafana.env contains this line: #TZ=Africa/Johannesburg Uncomment that line and change the right hand side to your own timezone .","title":"Setting your time-zone"},{"location":"Containers/Grafana/#security","text":"If Grafana has just been installed but has never been launched then the following will be true: The folder ~/IOTstack/volumes/grafana will not exist; and The file ~/IOTstack/services/grafana/grafana.env will contain these lines: ```","title":"Security"},{"location":"Containers/Grafana/#gf_security_admin_useradmin","text":"","title":"GF_SECURITY_ADMIN_USER=admin"},{"location":"Containers/Grafana/#gf_security_admin_passwordadmin","text":"``` You should see those lines as documentation rather than something you are being invited to edit. It is telling you that the default administative user for Grafana is \"admin\" and that the default password for that user is \"admin\". If you do not change anything then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will: Expect you to login as user \"admin\" with password \"admin\"; and then Force you to change the default password to something else. Thereafter, you will login as \"admin\" with whatever password you chose. You can change the administrator's password as often as you like via the web UI ( profile button, change password tab). This method (of not touching these two keys in grafana.env ) is the recommended approach. Please try to resist the temptation to fiddle!","title":"GF_SECURITY_ADMIN_PASSWORD=admin"},{"location":"Containers/Grafana/#i-want-a-different-admin-username-not-recommended","text":"If, before you bring up the stack for the first time, you do this: GF_SECURITY_ADMIN_USER=jack #GF_SECURITY_ADMIN_PASSWORD=admin then, when you bring up the stack and connect on port 3000, Grafana will: Expect you to login as user \"jack\" with password \"admin\"; and then Force you to change the default password to something else. Thereafter, \"jack\" will be the Grafana administrator and you will login with the password you chose, until you decide to change the password to something else via the web UI. Don't think you can come back later and tweak the Grafana administrator name in the environment variables. It doesn't work that way. It's a one-shot.","title":"I want a different admin username (not recommended)"},{"location":"Containers/Grafana/#i-want-a-different-default-admin-password-not-recommended","text":"Well, first off, the two methods above both make you set a different password on first login so there probably isn't much point to this. But, if you really insist\u2026 If, before you bring up the stack for the first time, you do this: #GF_SECURITY_ADMIN_USER=admin GF_SECURITY_ADMIN_PASSWORD=jack then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will expect you to login as user \"admin\" with password \"jack\". Grafana will not force you to change the password on first login but you will still be able to change it via the web UI. But don't think you can come back later and change the password in the environment variables. It doesn't work that way. It's a one-shot.","title":"I want a different default admin password (not recommended)"},{"location":"Containers/Grafana/#i-want-to-change-everything-not-recommended","text":"If, before you bring up the stack for the first time, you do this: GF_SECURITY_ADMIN_USER=bill GF_SECURITY_ADMIN_PASSWORD=jack then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will expect you to login as user \"bill\" with password \"jack\". Grafana will not force you to change the password on first login but you will still be able to change it via the web UI. But don't think you can come back later and tweak either the username or password in the environment variables. It doesn't work that way. It's a one-shot.","title":"I want to change everything (not recommended)"},{"location":"Containers/Grafana/#distilling-it-down","text":"Before Grafana is launched for the first time: GF_SECURITY_ADMIN_USER has a default value of \"admin\". You can explicitly set it to \"admin\" or some other value. Whatever option you choose then that's the account name of Grafana's administrative user. But choosing any value other than \"admin\" is probably a bad idea. GF_SECURITY_ADMIN_PASSWORD has a default value of \"admin\". You can explicitly set it to \"admin\" or some other value. If its value is \"admin\" then you will be forced to change it the first time you login to Grafana. If its value is something other than \"admin\" then that will be the password until you change it via the web UI. These two environment keys only work for the first launch of Grafana. Once Grafana has been launched, you can never change either the username or the password by editing grafana.env . For this reason, it is better to leave grafana.env in its shrink-wrapped state. Your first login is as \"admin/admin\" and then you set the password you actually want when Grafana prompts you to change it.","title":"Distilling it down"},{"location":"Containers/Grafana/#help-i-forgot-my-grafana-admin-password","text":"Assuming your IOTstack is up, the magic incantation is: $ docker exec grafana grafana-cli --homepath \"/usr/share/grafana\" admin reset-admin-password \"admin\" Then, use a browser to connect to your Raspberry Pi on port 3000. Grafana will: Expect you login as user \"admin\" with password \"admin\"; and then Force you to change the default password to something else. This magic incantation assumes that your administrative username is \"admin\". If you ignored the advice above and changed the administator username to something else then all bets are off. It might work anyway but we haven't tested it. Sorry. But that's why we said changing the username was not recommended.","title":"HELP \u2013 I forgot my Grafana admin password!"},{"location":"Containers/Grafana/#overriding-grafana-settings","text":"Grafana documentation contains a list of settings . Settings are described in terms of how they appear in \".ini\" files. An example of the sort of thing you might want to do is to enable anonymous access to your Grafana dashboards. The Grafana documentation describes this in \".ini\" format as: [auth.anonymous] enabled = true # Organization name that should be used for unauthenticated users org_name = Main Org. # Role for unauthenticated users, other valid values are `Editor` and `Admin` org_role = Viewer \".ini\" format is not really appropriate in a Docker context. Instead, you use environment variables to override Docker's settings. Environment variables are placed in ~/IOTstack/services/grafana/grafana.env . You need to convert \".ini\" format to environment variable syntax. The rules are: Start with \"GF_\", then Append the [section name], replacing any periods with underscores, then Append the section key \"as is\", then Append an \"=\", then Append the right hand side in quotes. Applying those rules gets you: GF_AUTH_ANONYMOUS_ENABLED=\"true\" GF_AUTH_ANONYMOUS_ORG_NAME=\"Main Org.\" GF_AUTH_ANONYMOUS_ORG_ROLE=\"Viewer\" It is not strictly necessary to encapsulate every right hand side value in quotes. In the above, both \"true\" and \"Viewer\" would work without quotes, whereas \"Main Org.\" needs quotes because of the embedded space. After you have changed ~/IOTstack/services/grafana/grafana.env , you need to propagate the changes into the Grafana container: $ cd ~/IOTstack $ docker-compose stop grafana $ docker-compose up -d In theory, the second command could be omitted, or both the second and third commands could be replaced with \"docker-compose restart grafana\" but experience suggests stopping the container is more reliable. A slightly more real-world example would involve choosing a different default organisation name for anonymous access. This example uses \"ChezMoi\". First, the environment key needs to be set to that value: GF_AUTH_ANONYMOUS_ORG_NAME=ChezMoi Then that change needs to be propagated into the Grafana container as explained above. Next, Grafana needs to be told that \"ChezMoi\" is the default organisation: Use your browser to login to Grafana as an administrator. From the \"Server Admin\" slide-out menu on the left hand side, choose \"Orgs\". In the list that appears, click on \"Main Org\". This opens an editing panel. Change the \"Name\" field to \"ChezMoi\" and click \"Update\". Sign-out of Grafana. You will be taken back to the login screen. Your URL bar will look something like this: http://myhost.mydomain.com:3000/login 6. Edit the URL to remove the \"login\" suffix and press return. If all your changes were applied successfully, you will have anonymous access and the URL will look something like this: http://myhost.mydomain.com:3000/?orgId=1","title":"Overriding Grafana settings"},{"location":"Containers/Grafana/#help-i-made-a-mess","text":"\"I made a bit of a mess with Grafana. First time user. Steep learning curve. False starts, many. Mistakes, unavoidable. Been there, done that. But now I really need to start from a clean slate. And, yes, I understand there is no undo for this.\" Begin by stopping Grafana: $ cd ~/IOTstack $ docker-compose stop grafana You have two options: Destroy your settings and dashboards but retain any plugins you may have installed: $ sudo rm ~/IOTstack/volumes/grafana/data/grafana.db Nuke everything (triple-check this command before you hit return): $ sudo rm -rf ~/IOTstack/volumes/grafana/data This is where you should edit ~/IOTstack/services/grafana/grafana.env to correct any problems (such as choosing an administrative username other than \"admin\"). When you are ready, bring Grafana back up again: $ cd ~/IOTstack $ docker-compose up -d Grafana will automatically recreate everything it needs. You will be able to login as \"admin/admin\".","title":"HELP \u2013 I made a mess!"},{"location":"Containers/Heimdall/","text":"Heimdall References Homepage Docker Web Interface The web UI can be found on \"your_ip\":8880 About Heimdall From the Heimdall website : Heimdall Application Dashboard is a dashboard for all your web applications. It doesn't need to be limited to applications though, you can add links to anything you like. There are no iframes here, no apps within apps, no abstraction of APIs. if you think something should work a certain way, it probably does. Within the context of IOTstack, the Heimdall Application Dashboard can help you organize your deployed services.","title":"Heimdall"},{"location":"Containers/Heimdall/#heimdall","text":"","title":"Heimdall"},{"location":"Containers/Heimdall/#references","text":"Homepage Docker","title":"References"},{"location":"Containers/Heimdall/#web-interface","text":"The web UI can be found on \"your_ip\":8880","title":"Web Interface"},{"location":"Containers/Heimdall/#about-heimdall","text":"From the Heimdall website : Heimdall Application Dashboard is a dashboard for all your web applications. It doesn't need to be limited to applications though, you can add links to anything you like. There are no iframes here, no apps within apps, no abstraction of APIs. if you think something should work a certain way, it probably does. Within the context of IOTstack, the Heimdall Application Dashboard can help you organize your deployed services.","title":"About Heimdall"},{"location":"Containers/Home-Assistant/","text":"Home assistant References Docker Webpage Hass.io is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control. Port binding is 8123 . Hass.io is exposed to your hosts' network in order to discover devices on your LAN. That means that it does not sit inside docker's network. To avoid confusion There are 2 versions of Home Assistant: Hass.io and Home Assistant Docker. Hass.io uses its own orchastration with 3 docker images: hassio_supervisor , hassio_dns and homeassistant . Home Assistant Docker runs inside a single docker image, and doesn't support all the features that Hass.io does (such as add-ons). IOTstack allows installing either, but we can only offer limited configuration of Hass.io since it is its own platform. More info on versions Menu installation Hass.io installation can be found inside the Native Installs menu on the main menu. Home Assistant can be found in the Build Stack menu. You will be asked to select you device type during the installation. Hass.io is no longer dependant on the IOTstack, it has its own service for maintaining its uptime. Installation Due to the behaviour of Network Manager, it is strongly recomended to connect the Pi over a wired internet connection, rather than WiFi. If you ignore the advice about connecting via Ethernet and install Network Manager while your session is connected via WiFi, your connection will freeze part way through the installation (when Network Manager starts running and unconditionally changes your Raspberry Pi's WiFi MAC address). Ensure your system is up to date with: sudo apt update If not already installed, install the network manager with: sudo apt-get install network-manager apparmor-utils before running the hass.io installation to avoid any potential errors. The installation of Hass.io takes up to 20 minutes (depending on your internet connection). Refrain from restarting your machine until it has come online and you are able to create a user account. Removal To remove Hass.io you first need to stop the service that controls it. Run the following in the terminal: sudo systemctl stop hassio-supervisor.service sudo systemctl disable hassio-supervisor.service This should stop the main service however there are two additional container that still need to be address This will stop the service and disable it from starting on the next boot Next you need to stop the hassio_dns and hassio_supervisor docker stop hassio_supervisor docker stop hassio_dns docker stop homeassistant If you want to remove the containers docker rm hassio_supervisor docker rm hassio_dns docker stop homeassistant After rebooting you should be able to reinstall The stored file are located in /usr/share/hassio which can be removed if you need to Double check with docker ps to see if there are other hassio containers running. They can stopped and removed in the same fashion for the dns and supervisor You can use Portainer to view what is running and clean up the unused images.","title":"Home assistant"},{"location":"Containers/Home-Assistant/#home-assistant","text":"","title":"Home assistant"},{"location":"Containers/Home-Assistant/#references","text":"Docker Webpage Hass.io is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control. Port binding is 8123 . Hass.io is exposed to your hosts' network in order to discover devices on your LAN. That means that it does not sit inside docker's network.","title":"References"},{"location":"Containers/Home-Assistant/#to-avoid-confusion","text":"There are 2 versions of Home Assistant: Hass.io and Home Assistant Docker. Hass.io uses its own orchastration with 3 docker images: hassio_supervisor , hassio_dns and homeassistant . Home Assistant Docker runs inside a single docker image, and doesn't support all the features that Hass.io does (such as add-ons). IOTstack allows installing either, but we can only offer limited configuration of Hass.io since it is its own platform. More info on versions","title":"To avoid confusion"},{"location":"Containers/Home-Assistant/#menu-installation","text":"Hass.io installation can be found inside the Native Installs menu on the main menu. Home Assistant can be found in the Build Stack menu. You will be asked to select you device type during the installation. Hass.io is no longer dependant on the IOTstack, it has its own service for maintaining its uptime.","title":"Menu installation"},{"location":"Containers/Home-Assistant/#installation","text":"Due to the behaviour of Network Manager, it is strongly recomended to connect the Pi over a wired internet connection, rather than WiFi. If you ignore the advice about connecting via Ethernet and install Network Manager while your session is connected via WiFi, your connection will freeze part way through the installation (when Network Manager starts running and unconditionally changes your Raspberry Pi's WiFi MAC address). Ensure your system is up to date with: sudo apt update If not already installed, install the network manager with: sudo apt-get install network-manager apparmor-utils before running the hass.io installation to avoid any potential errors. The installation of Hass.io takes up to 20 minutes (depending on your internet connection). Refrain from restarting your machine until it has come online and you are able to create a user account.","title":"Installation"},{"location":"Containers/Home-Assistant/#removal","text":"To remove Hass.io you first need to stop the service that controls it. Run the following in the terminal: sudo systemctl stop hassio-supervisor.service sudo systemctl disable hassio-supervisor.service This should stop the main service however there are two additional container that still need to be address This will stop the service and disable it from starting on the next boot Next you need to stop the hassio_dns and hassio_supervisor docker stop hassio_supervisor docker stop hassio_dns docker stop homeassistant If you want to remove the containers docker rm hassio_supervisor docker rm hassio_dns docker stop homeassistant After rebooting you should be able to reinstall The stored file are located in /usr/share/hassio which can be removed if you need to Double check with docker ps to see if there are other hassio containers running. They can stopped and removed in the same fashion for the dns and supervisor You can use Portainer to view what is running and clean up the unused images.","title":"Removal"},{"location":"Containers/Homer/","text":"Homer References Homepage Docker Web Interface The web UI can be found on \"your_ip\":8881 About Homer From the Homer README : A dead simple static HOMepage for your servER to keep your services on hand, from a simple yaml configuration file. You can find an example of the config.yml file here . Within the context of IOTstack, Homer can help you organize your deployed services.","title":"Homer"},{"location":"Containers/Homer/#homer","text":"","title":"Homer"},{"location":"Containers/Homer/#references","text":"Homepage Docker","title":"References"},{"location":"Containers/Homer/#web-interface","text":"The web UI can be found on \"your_ip\":8881","title":"Web Interface"},{"location":"Containers/Homer/#about-homer","text":"From the Homer README : A dead simple static HOMepage for your servER to keep your services on hand, from a simple yaml configuration file. You can find an example of the config.yml file here . Within the context of IOTstack, Homer can help you organize your deployed services.","title":"About Homer"},{"location":"Containers/InfluxDB/","text":"InfluxDB References Docker Website Security The credentials and default database name for influxdb are stored in the file called influxdb/influx.env . The default username and password is set to \"nodered\" for both. It is HIGHLY recommended that you change them. The environment file contains several commented out options allowing you to set several access options such as default admin user credentials as well as the default database name. Any change to the environment file will require a restart of the service. To access the terminal for influxdb execute ./services/influxdb/terminal.sh . Here you can set additional parameters or create other databases.","title":"InfluxDB"},{"location":"Containers/InfluxDB/#influxdb","text":"","title":"InfluxDB"},{"location":"Containers/InfluxDB/#references","text":"Docker Website","title":"References"},{"location":"Containers/InfluxDB/#security","text":"The credentials and default database name for influxdb are stored in the file called influxdb/influx.env . The default username and password is set to \"nodered\" for both. It is HIGHLY recommended that you change them. The environment file contains several commented out options allowing you to set several access options such as default admin user credentials as well as the default database name. Any change to the environment file will require a restart of the service. To access the terminal for influxdb execute ./services/influxdb/terminal.sh . Here you can set additional parameters or create other databases.","title":"Security"},{"location":"Containers/MariaDB/","text":"Source Docker hub Webpage About MariaDB is a fork of MySQL. This is an unofficial image provided by linuxserver.io because there is no official image for arm Conneting to the DB The port is 3306. It exists inside the docker network so you can connect via mariadb:3306 for internal connections. For external connections use <your Pis IP>:3306 Setup Before starting the stack edit the ./services/mariadb/mariadb.env file and set your access details. This is optional however you will only have one shot at the preconfig. If you start the container without setting the passwords then you will have to either delete its volume directory or enter the terminal and change manually The env file has three commented fields for credentials, either all three must be commented or un-commented. You can't have only one or two, its all or nothing. Terminal A terminal is provided to access mariadb by the cli. execute ./services/mariadb/terminal.sh . You will need to run mysql -uroot -p to enter mariadbs interface","title":"MariaDB"},{"location":"Containers/MariaDB/#source","text":"Docker hub Webpage","title":"Source"},{"location":"Containers/MariaDB/#about","text":"MariaDB is a fork of MySQL. This is an unofficial image provided by linuxserver.io because there is no official image for arm","title":"About"},{"location":"Containers/MariaDB/#conneting-to-the-db","text":"The port is 3306. It exists inside the docker network so you can connect via mariadb:3306 for internal connections. For external connections use <your Pis IP>:3306","title":"Conneting to the DB"},{"location":"Containers/MariaDB/#setup","text":"Before starting the stack edit the ./services/mariadb/mariadb.env file and set your access details. This is optional however you will only have one shot at the preconfig. If you start the container without setting the passwords then you will have to either delete its volume directory or enter the terminal and change manually The env file has three commented fields for credentials, either all three must be commented or un-commented. You can't have only one or two, its all or nothing.","title":"Setup"},{"location":"Containers/MariaDB/#terminal","text":"A terminal is provided to access mariadb by the cli. execute ./services/mariadb/terminal.sh . You will need to run mysql -uroot -p to enter mariadbs interface","title":"Terminal"},{"location":"Containers/Mosquitto/","text":"Mosquitto References Docker Website mosquitto.conf documentation Setting up passwords video Definitions docker-compose.yml \u21d2 ~/IOTstack/docker-compose.yml mosquitto.conf \u21d2 ~/IOTstack/services/mosquitto/mosquitto.conf mosquitto.log \u21d2 ~/IOTstack/volumes/mosquitto/log/mosquitto.log service.yml \u21d2 ~/IOTstack/.templates/mosquitto/service.yml volumes/mosquitto \u21d2 ~/IOTstack/volumes/mosquitto/ Logging Mosquitto logging is controlled by mosquitto.conf . This is the default configuration: #log_dest file /mosquitto/log/mosquitto.log # To avoid flash wearing log_dest stdout When log_dest is set to stdout , you inspect Mosquitto's logs like this: $ docker logs mosquitto Logs written to stdout are ephemeral and will disappear when your IOTstack is restarted but this configuration reduces wear and tear on your SD card. The alternative, which may be more appropriate if you are running on an SSD or HD, is to change mosquitto.conf to be like this: log_dest file /mosquitto/log/mosquitto.log # To avoid flash wearing #log_dest stdout and then restart Mosquitto: $ cd ~/IOTstack $ docker-compose restart mosquitto With this configuration, you inspect Mosquitto's logs like this: $ tail ~/IOTstack/volumes/mosquitto/log/mosquitto.log Logs written to mosquitto.log do not disappear when your IOTstack is restarted. They persist until you take action to prune the file. Security By default, the Mosquitto container has no password. You can leave it that way if you like but it's always a good idea to secure your services. Assuming your IOTstack is running: Open a shell in the mosquitto container: $ docker exec -it mosquitto sh In the following, replace \u00abMYUSER\u00bb with the username you want to use for controlling access to Mosquitto and then run these commands: $ mosquitto_passwd -c /mosquitto/pwfile/pwfile \u00abMYUSER\u00bb $ exit mosquitto_passwd will ask you to type a password and confirm it. The path on the right hand side of: -c /mosquitto/pwfile/pwfile is inside the container. Outside the container, it maps to: ~/IOTstack/volumes/mosquitto/pwfile/pwfile You should be able to see the result of setting a username and password like this: $ cat ~/IOTstack/volumes/mosquitto/pwfile/pwfile MYUSER:$6$lBYlxjWtLON0fm96$3qgcEyr/nKvxk3C2Jk36kkILJK7nLdIeLhuywVOVkVbJUjBeqUmCLOA/T6qAq2+hyyJdZ52ALTi+onMEEaM0qQ== $ Open mosquitto.conf in a text editor. Find this line: ``` password_file /mosquitto/pwfile/pwfile ``` Remove the # in front of password_file. Save. Restart Mosquitto: $ cd ~/IOTstack $ docker-compose restart mosquitto Use the new credentials where necessary (eg Node-Red). Notes: You can revert to password-disabled state by going back to step 3, re-inserting the \"#\", then restarting Mosquitto as per step 4. If mosquitto keeps restarting after you implement password checking, the most likely explanation will be something wrong with the password file. Implement the advice in the previous note. Running as root By default, the Mosquitto container is launched as root but then downgrades its privileges to run as user ID 1883. Mosquitto is unusual because most containers just accept the privileges they were launched with. In most cases, that means containers run as root. Don't make the mistake of thinking this means that processes running inside containers can do whatever they like to your host system. A process inside a container is contained . What a process can affect outside its container is governed by the port, device and volume mappings you see in the docker-compose.yml . You can check how mosquitto has been launched like this: $ ps -eo euser,ruser,suser,fuser,comm | grep mosquitto EUSER RUSER SUSER FUSER COMMAND 1883 1883 1883 1883 mosquitto If you have a use-case that needs Mosquitto to run with root privileges: Open docker-compose.yml in a text editor and find this: mosquitto: \u2026 [snip] \u2026 user: \"1883\" change it to: mosquitto: \u2026 [snip] \u2026 user: \"0\" Edit mosquitto.conf to add this line: user root Apply the change: $ cd ~/IOTstack $ docker-compose stop mosquitto $ docker-compose up -d A clean install of Mosquitto via the IOTstack menu sets everything in volumes/mosquitto to user and group 1883. That permission structure will still work if you change Mosquitto to run with root privileges. However, running as root may have the side effect of changing privilege levels within volumes/mosquitto . Keep this in mind if you decide to switch back to running Mosquitto as user 1883 because it is less likely to work. Port 9001 In earlier versions of IOTstack, service.yml included two port mappings which were included in docker-compose.yml when Mosquitto was chosen in the menu: ports: - \"1883:1883\" - \"9001:9001\" Issue 67 explored the topic of port 9001 and showed that: The base image for mosquitto did not expose port 9001; and The running container was not listening to port 9001. On that basis, the mapping for port 9001 was removed from service.yml . If you have a use-case that needs port 9001, you can re-enable support by: Inserting the port mapping under the mosquitto definition in docker-compose.yml : - \"9001:9001\" Inserting the following lines in mosquitto.conf : listener 1883 listener 9001 You need both lines. If you omit 1883 then mosquitto will stop listening to port 1883 and will only listen to port 9001. Restarting the container: $ cd ~/IOTstack $ docker-compose up -d Please consider raising an issue to document your use-case. If you think your use-case has general application then please also consider creating a pull request to make the changes permanent.","title":"Mosquitto"},{"location":"Containers/Mosquitto/#mosquitto","text":"","title":"Mosquitto"},{"location":"Containers/Mosquitto/#references","text":"Docker Website mosquitto.conf documentation Setting up passwords video","title":"References"},{"location":"Containers/Mosquitto/#definitions","text":"docker-compose.yml \u21d2 ~/IOTstack/docker-compose.yml mosquitto.conf \u21d2 ~/IOTstack/services/mosquitto/mosquitto.conf mosquitto.log \u21d2 ~/IOTstack/volumes/mosquitto/log/mosquitto.log service.yml \u21d2 ~/IOTstack/.templates/mosquitto/service.yml volumes/mosquitto \u21d2 ~/IOTstack/volumes/mosquitto/","title":"Definitions"},{"location":"Containers/Mosquitto/#logging","text":"Mosquitto logging is controlled by mosquitto.conf . This is the default configuration: #log_dest file /mosquitto/log/mosquitto.log # To avoid flash wearing log_dest stdout When log_dest is set to stdout , you inspect Mosquitto's logs like this: $ docker logs mosquitto Logs written to stdout are ephemeral and will disappear when your IOTstack is restarted but this configuration reduces wear and tear on your SD card. The alternative, which may be more appropriate if you are running on an SSD or HD, is to change mosquitto.conf to be like this: log_dest file /mosquitto/log/mosquitto.log # To avoid flash wearing #log_dest stdout and then restart Mosquitto: $ cd ~/IOTstack $ docker-compose restart mosquitto With this configuration, you inspect Mosquitto's logs like this: $ tail ~/IOTstack/volumes/mosquitto/log/mosquitto.log Logs written to mosquitto.log do not disappear when your IOTstack is restarted. They persist until you take action to prune the file.","title":"Logging"},{"location":"Containers/Mosquitto/#security","text":"By default, the Mosquitto container has no password. You can leave it that way if you like but it's always a good idea to secure your services. Assuming your IOTstack is running: Open a shell in the mosquitto container: $ docker exec -it mosquitto sh In the following, replace \u00abMYUSER\u00bb with the username you want to use for controlling access to Mosquitto and then run these commands: $ mosquitto_passwd -c /mosquitto/pwfile/pwfile \u00abMYUSER\u00bb $ exit mosquitto_passwd will ask you to type a password and confirm it. The path on the right hand side of: -c /mosquitto/pwfile/pwfile is inside the container. Outside the container, it maps to: ~/IOTstack/volumes/mosquitto/pwfile/pwfile You should be able to see the result of setting a username and password like this: $ cat ~/IOTstack/volumes/mosquitto/pwfile/pwfile MYUSER:$6$lBYlxjWtLON0fm96$3qgcEyr/nKvxk3C2Jk36kkILJK7nLdIeLhuywVOVkVbJUjBeqUmCLOA/T6qAq2+hyyJdZ52ALTi+onMEEaM0qQ== $ Open mosquitto.conf in a text editor. Find this line: ```","title":"Security"},{"location":"Containers/Mosquitto/#password_file-mosquittopwfilepwfile","text":"``` Remove the # in front of password_file. Save. Restart Mosquitto: $ cd ~/IOTstack $ docker-compose restart mosquitto Use the new credentials where necessary (eg Node-Red). Notes: You can revert to password-disabled state by going back to step 3, re-inserting the \"#\", then restarting Mosquitto as per step 4. If mosquitto keeps restarting after you implement password checking, the most likely explanation will be something wrong with the password file. Implement the advice in the previous note.","title":"password_file /mosquitto/pwfile/pwfile"},{"location":"Containers/Mosquitto/#running-as-root","text":"By default, the Mosquitto container is launched as root but then downgrades its privileges to run as user ID 1883. Mosquitto is unusual because most containers just accept the privileges they were launched with. In most cases, that means containers run as root. Don't make the mistake of thinking this means that processes running inside containers can do whatever they like to your host system. A process inside a container is contained . What a process can affect outside its container is governed by the port, device and volume mappings you see in the docker-compose.yml . You can check how mosquitto has been launched like this: $ ps -eo euser,ruser,suser,fuser,comm | grep mosquitto EUSER RUSER SUSER FUSER COMMAND 1883 1883 1883 1883 mosquitto If you have a use-case that needs Mosquitto to run with root privileges: Open docker-compose.yml in a text editor and find this: mosquitto: \u2026 [snip] \u2026 user: \"1883\" change it to: mosquitto: \u2026 [snip] \u2026 user: \"0\" Edit mosquitto.conf to add this line: user root Apply the change: $ cd ~/IOTstack $ docker-compose stop mosquitto $ docker-compose up -d A clean install of Mosquitto via the IOTstack menu sets everything in volumes/mosquitto to user and group 1883. That permission structure will still work if you change Mosquitto to run with root privileges. However, running as root may have the side effect of changing privilege levels within volumes/mosquitto . Keep this in mind if you decide to switch back to running Mosquitto as user 1883 because it is less likely to work.","title":"Running as root"},{"location":"Containers/Mosquitto/#port-9001","text":"In earlier versions of IOTstack, service.yml included two port mappings which were included in docker-compose.yml when Mosquitto was chosen in the menu: ports: - \"1883:1883\" - \"9001:9001\" Issue 67 explored the topic of port 9001 and showed that: The base image for mosquitto did not expose port 9001; and The running container was not listening to port 9001. On that basis, the mapping for port 9001 was removed from service.yml . If you have a use-case that needs port 9001, you can re-enable support by: Inserting the port mapping under the mosquitto definition in docker-compose.yml : - \"9001:9001\" Inserting the following lines in mosquitto.conf : listener 1883 listener 9001 You need both lines. If you omit 1883 then mosquitto will stop listening to port 1883 and will only listen to port 9001. Restarting the container: $ cd ~/IOTstack $ docker-compose up -d Please consider raising an issue to document your use-case. If you think your use-case has general application then please also consider creating a pull request to make the changes permanent.","title":"Port 9001"},{"location":"Containers/MotionEye/","text":"MotionEye References Website About MotionEye is a camera/webcam package. The port is set to 8765 Config This is the yml entry. Notice that the devices is commented out. This is because if you don't have a camera attached then it will fail to start. Uncomment if you need to. This is for a Pi camera, you will need to add additional lines for usb cameras motioneye: image: \"ccrisan/motioneye:master-armhf\" container_name: \"motioneye\" restart: unless-stopped ports: - 8765:8765 - 8081:8081 volumes: - /etc/localtime:/etc/localtime:ro - ./volumes/motioneye/etc_motioneye:/etc/motioneye - ./volumes/motioneye/var_lib_motioneye:/var/lib/motioneye #devices: # - \"/dev/video0:/dev/video0\" Login Details On first login you will be asked for login details. The default user is admin (all lowercase) with no password Storage By default local camera data will be stored in /var/lib/motioneye/camera_name in the container which equates to the following: Remote motioneye If you have connected to a remote motion eye note that the directory is on that device and has nothing to do with the container.","title":"MotionEye"},{"location":"Containers/MotionEye/#motioneye","text":"","title":"MotionEye"},{"location":"Containers/MotionEye/#references","text":"Website","title":"References"},{"location":"Containers/MotionEye/#about","text":"MotionEye is a camera/webcam package. The port is set to 8765","title":"About"},{"location":"Containers/MotionEye/#config","text":"This is the yml entry. Notice that the devices is commented out. This is because if you don't have a camera attached then it will fail to start. Uncomment if you need to. This is for a Pi camera, you will need to add additional lines for usb cameras motioneye: image: \"ccrisan/motioneye:master-armhf\" container_name: \"motioneye\" restart: unless-stopped ports: - 8765:8765 - 8081:8081 volumes: - /etc/localtime:/etc/localtime:ro - ./volumes/motioneye/etc_motioneye:/etc/motioneye - ./volumes/motioneye/var_lib_motioneye:/var/lib/motioneye #devices: # - \"/dev/video0:/dev/video0\"","title":"Config"},{"location":"Containers/MotionEye/#login-details","text":"On first login you will be asked for login details. The default user is admin (all lowercase) with no password","title":"Login Details"},{"location":"Containers/MotionEye/#storage","text":"By default local camera data will be stored in /var/lib/motioneye/camera_name in the container which equates to the following:","title":"Storage"},{"location":"Containers/MotionEye/#remote-motioneye","text":"If you have connected to a remote motion eye note that the directory is on that device and has nothing to do with the container.","title":"Remote motioneye"},{"location":"Containers/NextCloud/","text":"Next Cloud DO NOT EXPOSE PORT 80 TO THE WEB It is a very bad idea to expose unencrypted traffic to the web. You will need to use a reverse-proxy to ensure your password is not stolen and your account hacked. I'm still working on getting a good encrypted reverse proxy working. However in the interim you can use a VPN tunnel like OpenVPN or Zerotier to securely connect to your private cloud Backups Nextcloud has been excluded from the docker_backup script due to its potential size. Once I've found a better way of backing it up I will add a dedicated script for it. Setup Next-Cloud recommends using MySQL/MariaDB for the accounts and file list. The alternative is to use SQLite however they strongly discourage using it This is the service yml. Notice that there are in fact two containers, one for the db and the other for the cloud itself. You will need to change the passwords before starting the stack (remember to change the docker-compose.yml and ./services/nextcloud/service.yml), if you dont you will need to delete the volume directory and start again. nextcloud: image: nextcloud container_name: nextcloud ports: - 9321:80 volumes: - ./volumes/nextcloud/html:/var/www/html restart: unless-stopped depends_on: - nextcloud_db nextcloud_db: image: linuxserver/mariadb container_name: nextcloud_db volumes: - ./volumes/nextcloud/db:/config environment: - MYSQL_ROOT_PASSWORD=stronger_password - MYSQL_PASSWORD=strong_password - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud The port is 9321 click on the storage options, select maraiadb/mysql and fill in the details as follows Note that you data will be stored in ./volumes/nextcloud/html/data/{account} Also note that file permissions are \"www-data\" so you cant simply copy data into this folder directly, you should use the web interface or the app. It would be a good idea to mount an external drive to store the data in rather than on your sd card. details to follow shortly. Something like: The external drive will have to be an ext4 formatted drive because smb, fat32 and NTFS can't handle linux file permissions. If the permissions aren't set to \"www-data\" then the container wont be able to write to the disk. Just a warning: If your database gets corrupted then your nextcloud is pretty much stuffed","title":"Next Cloud"},{"location":"Containers/NextCloud/#next-cloud","text":"","title":"Next Cloud"},{"location":"Containers/NextCloud/#do-not-expose-port-80-to-the-web","text":"It is a very bad idea to expose unencrypted traffic to the web. You will need to use a reverse-proxy to ensure your password is not stolen and your account hacked. I'm still working on getting a good encrypted reverse proxy working. However in the interim you can use a VPN tunnel like OpenVPN or Zerotier to securely connect to your private cloud","title":"DO NOT EXPOSE PORT 80 TO THE WEB"},{"location":"Containers/NextCloud/#backups","text":"Nextcloud has been excluded from the docker_backup script due to its potential size. Once I've found a better way of backing it up I will add a dedicated script for it.","title":"Backups"},{"location":"Containers/NextCloud/#setup","text":"Next-Cloud recommends using MySQL/MariaDB for the accounts and file list. The alternative is to use SQLite however they strongly discourage using it This is the service yml. Notice that there are in fact two containers, one for the db and the other for the cloud itself. You will need to change the passwords before starting the stack (remember to change the docker-compose.yml and ./services/nextcloud/service.yml), if you dont you will need to delete the volume directory and start again. nextcloud: image: nextcloud container_name: nextcloud ports: - 9321:80 volumes: - ./volumes/nextcloud/html:/var/www/html restart: unless-stopped depends_on: - nextcloud_db nextcloud_db: image: linuxserver/mariadb container_name: nextcloud_db volumes: - ./volumes/nextcloud/db:/config environment: - MYSQL_ROOT_PASSWORD=stronger_password - MYSQL_PASSWORD=strong_password - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud The port is 9321 click on the storage options, select maraiadb/mysql and fill in the details as follows Note that you data will be stored in ./volumes/nextcloud/html/data/{account} Also note that file permissions are \"www-data\" so you cant simply copy data into this folder directly, you should use the web interface or the app. It would be a good idea to mount an external drive to store the data in rather than on your sd card. details to follow shortly. Something like: The external drive will have to be an ext4 formatted drive because smb, fat32 and NTFS can't handle linux file permissions. If the permissions aren't set to \"www-data\" then the container wont be able to write to the disk. Just a warning: If your database gets corrupted then your nextcloud is pretty much stuffed","title":"Setup"},{"location":"Containers/Node-RED/","text":"Node-RED References Docker website Build warning The Node-RED build will complain about several issues. This is completely normal behaviour. SQLite Thanks to @fragolinux the SQLite node will install now. WARNING it will output many error and will look as if it has gotten stuck. Just give it time and it will continue. GPIO To communicate to your Pi's GPIO you need to use the node-red-node-pi-gpiod node. It allowes you to connect to multiple Pis from the same nodered service. You need to make sure that pigpdiod is running. The recommended method is listed here You run the following command sudo nano /etc/rc.local and add the line /usr/bin/pigpiod above exit 0 and reboot the Pi. There is an option to secure the service see the writeup for further instuctions. Fot the Rpi Image you will also need to update to the most recent version sudo apt-get update sudo apt-get install pigpio python-pigpio python3-pigpio Drop the gpio node and use your Pi's IP. Example: 192.168.1.123 (127.0.0.1 won't work because this is the local address of every computer'.) Securing Node-RED To secure Node-RED you need a password hash. There is a terminal script ./services/nodered/terminal.sh execute it to get into the terminal. Copy the helper text node -e ..... PASSWORD , paste it and change your password to get a hash. Open the file ./volumes/nodered/data/settings.js and follow the writeup on https://nodered.org/docs/user-guide/runtime/securing-node-red for further instructions Sharing files between Node-RED and the host Containers run in a sandboxed environment they can't see the host (the Pi itself) file system. This presents a problem if you want to read a file directly to the host from inside the container. Fortunately there is a method, the containers have been set up with volume mapping. The volume maps a specific directory or file from the host file system into the container. Therefore if you write to that directory both the host and the container can see the files. Consider the following: The docker-compose.yml file shows the following for Node-RED volumes: - ./volumes/nodered/data:/data If inside Node-RED you were to write to the /data folder then the host would see it in ~/IOTstack/volumes/nodered/data (the ./volumes above implies relative to the docker-compose.yml file) The flow writes the file /data/test.txt and it is visible in the host as ~/IOTstack/volumes/nodered/data/test.txt Remember, files and directories in the volume are persistent between restarts. If you save your data elsewhere it will be destroyed should you restart. Creating a subdirectory in volume i.e. /data/storage/ would be advised Using Bluetooth In order to allow Node-RED to access the Pi's Bluetooth module the docker-comose.yml file needs to be modified to allow it access. network_mode: \"host\" needs to be added (make sure the indentation is correct, us spaces not tabs): nodered: container_name: nodered build: ./services/nodered/. restart: unless-stopped user: \"0\" network_mode: \"host\" By activating host mode the Node-RED container can no longer access containers by name http://influxdb:8086 will no longer work. Node-RED thinks it now is the host and therefore access to the following services will look as follows: * influxdb http://127.0.0.1:8086 * GPIO 127.0.0.1 port 8888 * MQTT 127.0.0.1 Unused node in Protainer Portainer will report that the nodered image is unsed, this is normal due to the method used build the image. This is normal behavior. It is not advised to remove it as it is used as the base for the iotstack_nodered image, you will need to redownload it should you rebuild the nodered image. Running the exec node against the host Pi Due to the isolation between containers and the host the exec node will run against the container. There is a solution to work around this. You can use ssh to execute a script on the pi. It requires a little setup but is possible. For this example I'll be running a simple script called test.sh I create a file called test.sh in my IOTstack directory with nano The contents are as follows: #!/bin/bash echo \"hello\" exit 0 The exit 0 will stop the exec node from reporting an issue. Its a good idea to add the shebang at the top. make it executable with chmod +x test.sh This nodered running open the nodered terminal with ./services/nodered/terminal.sh or docker exec -it nodered /bin/bash or use portainer create the ssh folder in the data directory (the /data directory is persistently mapped volume) mkdir -p /data/ssh create key, this will require naming the output file ssh-keygen -f /data/ssh/nodered put in any additional config you want key type strength copy the key to the Pi. When asked for a password leave it blank copy the ssh-key to your pi ssh-copy-id -i /data/ssh/nodered pi@192.168.x.x replace with your static IP address. You will have to reply yes to the prompt. You may also see an error referring to regular expressions however you can ignore it. now to execute a script on the pi run ssh -i /data/ssh/nodered pi@192.168.x.x /home/pi/IOTstack/test.sh type exit to leave the terminal (you could also restart your pi with ssh -i /data/ssh/nodered pi@192.168.x.x sudo reboot ) in node red in your exec node you can run the command ssh -i /data/ssh/nodered pi@192.168.x.x /home/pi/IOTstack/test.sh other the script or command of your choice","title":"Node-RED"},{"location":"Containers/Node-RED/#node-red","text":"","title":"Node-RED"},{"location":"Containers/Node-RED/#references","text":"Docker website","title":"References"},{"location":"Containers/Node-RED/#build-warning","text":"The Node-RED build will complain about several issues. This is completely normal behaviour.","title":"Build warning"},{"location":"Containers/Node-RED/#sqlite","text":"Thanks to @fragolinux the SQLite node will install now. WARNING it will output many error and will look as if it has gotten stuck. Just give it time and it will continue.","title":"SQLite"},{"location":"Containers/Node-RED/#gpio","text":"To communicate to your Pi's GPIO you need to use the node-red-node-pi-gpiod node. It allowes you to connect to multiple Pis from the same nodered service. You need to make sure that pigpdiod is running. The recommended method is listed here You run the following command sudo nano /etc/rc.local and add the line /usr/bin/pigpiod above exit 0 and reboot the Pi. There is an option to secure the service see the writeup for further instuctions. Fot the Rpi Image you will also need to update to the most recent version sudo apt-get update sudo apt-get install pigpio python-pigpio python3-pigpio Drop the gpio node and use your Pi's IP. Example: 192.168.1.123 (127.0.0.1 won't work because this is the local address of every computer'.)","title":"GPIO"},{"location":"Containers/Node-RED/#securing-node-red","text":"To secure Node-RED you need a password hash. There is a terminal script ./services/nodered/terminal.sh execute it to get into the terminal. Copy the helper text node -e ..... PASSWORD , paste it and change your password to get a hash. Open the file ./volumes/nodered/data/settings.js and follow the writeup on https://nodered.org/docs/user-guide/runtime/securing-node-red for further instructions","title":"Securing Node-RED"},{"location":"Containers/Node-RED/#sharing-files-between-node-red-and-the-host","text":"Containers run in a sandboxed environment they can't see the host (the Pi itself) file system. This presents a problem if you want to read a file directly to the host from inside the container. Fortunately there is a method, the containers have been set up with volume mapping. The volume maps a specific directory or file from the host file system into the container. Therefore if you write to that directory both the host and the container can see the files. Consider the following: The docker-compose.yml file shows the following for Node-RED volumes: - ./volumes/nodered/data:/data If inside Node-RED you were to write to the /data folder then the host would see it in ~/IOTstack/volumes/nodered/data (the ./volumes above implies relative to the docker-compose.yml file) The flow writes the file /data/test.txt and it is visible in the host as ~/IOTstack/volumes/nodered/data/test.txt Remember, files and directories in the volume are persistent between restarts. If you save your data elsewhere it will be destroyed should you restart. Creating a subdirectory in volume i.e. /data/storage/ would be advised","title":"Sharing files between Node-RED and the host"},{"location":"Containers/Node-RED/#using-bluetooth","text":"In order to allow Node-RED to access the Pi's Bluetooth module the docker-comose.yml file needs to be modified to allow it access. network_mode: \"host\" needs to be added (make sure the indentation is correct, us spaces not tabs): nodered: container_name: nodered build: ./services/nodered/. restart: unless-stopped user: \"0\" network_mode: \"host\" By activating host mode the Node-RED container can no longer access containers by name http://influxdb:8086 will no longer work. Node-RED thinks it now is the host and therefore access to the following services will look as follows: * influxdb http://127.0.0.1:8086 * GPIO 127.0.0.1 port 8888 * MQTT 127.0.0.1","title":"Using Bluetooth"},{"location":"Containers/Node-RED/#unused-node-in-protainer","text":"Portainer will report that the nodered image is unsed, this is normal due to the method used build the image. This is normal behavior. It is not advised to remove it as it is used as the base for the iotstack_nodered image, you will need to redownload it should you rebuild the nodered image.","title":"Unused node in Protainer"},{"location":"Containers/Node-RED/#running-the-exec-node-against-the-host-pi","text":"Due to the isolation between containers and the host the exec node will run against the container. There is a solution to work around this. You can use ssh to execute a script on the pi. It requires a little setup but is possible. For this example I'll be running a simple script called test.sh I create a file called test.sh in my IOTstack directory with nano The contents are as follows: #!/bin/bash echo \"hello\" exit 0 The exit 0 will stop the exec node from reporting an issue. Its a good idea to add the shebang at the top. make it executable with chmod +x test.sh This nodered running open the nodered terminal with ./services/nodered/terminal.sh or docker exec -it nodered /bin/bash or use portainer create the ssh folder in the data directory (the /data directory is persistently mapped volume) mkdir -p /data/ssh create key, this will require naming the output file ssh-keygen -f /data/ssh/nodered put in any additional config you want key type strength copy the key to the Pi. When asked for a password leave it blank copy the ssh-key to your pi ssh-copy-id -i /data/ssh/nodered pi@192.168.x.x replace with your static IP address. You will have to reply yes to the prompt. You may also see an error referring to regular expressions however you can ignore it. now to execute a script on the pi run ssh -i /data/ssh/nodered pi@192.168.x.x /home/pi/IOTstack/test.sh type exit to leave the terminal (you could also restart your pi with ssh -i /data/ssh/nodered pi@192.168.x.x sudo reboot ) in node red in your exec node you can run the command ssh -i /data/ssh/nodered pi@192.168.x.x /home/pi/IOTstack/test.sh other the script or command of your choice","title":"Running the exec node against the host Pi"},{"location":"Containers/Pi-hole/","text":"Pi-hole Pi-hole is a fantastic utility to reduce ads The interface can be found on \"your_ip\":8089/admin Default password is pihole . This can be changed in the ~/IOTstack/services/pihole/pihole.env file To enable your router to use the pihole container edit your DNS settings on your router to point to your Pi's IP address","title":"Pi-hole"},{"location":"Containers/Pi-hole/#pi-hole","text":"Pi-hole is a fantastic utility to reduce ads The interface can be found on \"your_ip\":8089/admin Default password is pihole . This can be changed in the ~/IOTstack/services/pihole/pihole.env file To enable your router to use the pihole container edit your DNS settings on your router to point to your Pi's IP address","title":"Pi-hole"},{"location":"Containers/Plex/","text":"Plex References Homepage Docker Web interface The web UI can be found on \"your_ip\":32400/web Mounting an external drive by UUID to the home directory official mounting guide Create a directory in you home directory called mnt with a subdirectory HDD . Follow the instruction above to mount your external drive to /home/pi/mnt/HDD in you fstab edit your docker-compose.yml file under plex and uncomment the volumes for tv series and movies (modify the path to point to your media locations). Run docker-compose up -d to rebuild plex with the new volumes","title":"Plex"},{"location":"Containers/Plex/#plex","text":"","title":"Plex"},{"location":"Containers/Plex/#references","text":"Homepage Docker","title":"References"},{"location":"Containers/Plex/#web-interface","text":"The web UI can be found on \"your_ip\":32400/web","title":"Web interface"},{"location":"Containers/Plex/#mounting-an-external-drive-by-uuid-to-the-home-directory","text":"official mounting guide Create a directory in you home directory called mnt with a subdirectory HDD . Follow the instruction above to mount your external drive to /home/pi/mnt/HDD in you fstab edit your docker-compose.yml file under plex and uncomment the volumes for tv series and movies (modify the path to point to your media locations). Run docker-compose up -d to rebuild plex with the new volumes","title":"Mounting an external drive by UUID to the home directory"},{"location":"Containers/Portainer-agent/","text":"Portainer agent References Docker Docs About The protainer agent is a great way to add a second docker instance to a existing portainer instance. this allows you to mananage multiple docker enviroments form one prortainer instance Adding to an existing instance When you want to add the the agent to an existing portianer instance. You go to the endpoints tab. Click on Add endpoint Select Agent Enter the name of the agent Enter the url of the endpoint ip-of-agent-instance:9001 Click on add endpoint","title":"Portainer agent"},{"location":"Containers/Portainer-agent/#portainer-agent","text":"","title":"Portainer agent"},{"location":"Containers/Portainer-agent/#references","text":"Docker Docs","title":"References"},{"location":"Containers/Portainer-agent/#about","text":"The protainer agent is a great way to add a second docker instance to a existing portainer instance. this allows you to mananage multiple docker enviroments form one prortainer instance","title":"About"},{"location":"Containers/Portainer-agent/#adding-to-an-existing-instance","text":"When you want to add the the agent to an existing portianer instance. You go to the endpoints tab. Click on Add endpoint Select Agent Enter the name of the agent Enter the url of the endpoint ip-of-agent-instance:9001 Click on add endpoint","title":"Adding to an existing instance"},{"location":"Containers/Portainer-ce/","text":"Portainer CE References Docker Website Definition \"#yourip\" means any of the following: the IP address of your Raspberry Pi (eg 192.168.1.10 ) the multicast domain name of your Raspberry Pi (eg iot-hub.local ) the domain name of your Raspberry Pi (eg iot-hub.mydomain.com ) About Portainer CE Portainer CE (Community Edition) is an application for managing Docker. It is a successor to Portainer . According to the Portainer CE documentation Portainer 1.24.x will continue as a separate code branch, released as portainer/portainer:latest, and will receive ongoing security updates until at least 1st Sept 2021. No new features will be added beyond what was available in 1.24.1. From that it should be clear that Portainer is deprecated and that Portainer CE is the way forward. Portainer CE coexistence with Portainer IOTstack has been set up so that Portainer CE and Portainer can coexist. This is intended as a short-term migration aid rather than a long-term proposition. If you are a first-time user of IOTstack, you should choose Portainer CE and forget about Portainer . Installing Portainer CE Run the menu: $ cd ~/IOTstack $ ./menu.sh Choose \"Build Stack\", select \"Portainer-ce\", press [TAB] then \"\\<Ok>\" and follow through to the end of the menu process, typically choosing \"Do not overwrite\" for any existing services. When the menu finishes: $ docker-compose up -d Ignore any message like this: WARNING: Found orphan containers (portainer) for this project \u2026 Migration note Portainer CE and Portainer use different locations for their persistent data: Edition Persistent Data Directory Reference Portainer ~/IOTstack/volumes/portainer [A] Portainer CE ~/IOTstack/volumes/portainer-ce [B] If you have been running Portainer but have never run Portainer CE then: [A] will exist, but [B] will not exist. Whenever \"Portainer-ce\" is enabled in menu.sh , a check is made for the presence of [A] combined with the absence [B]. If and only if that situation exists, [B] is initialised as a copy of [A]. This one-time copy is intended to preserve your Portainer settings and admin user password for use in Portainer CE . Thereafter, any settings you change in Portainer CE will not be reflected in Portainer , nor vice versa. Port Number = 9002 Both Portainer CE and Portainer are usually configured to listen to port 9000 but, in the IOTstack implementation: Portainer CE uses port 9002; and Portainer uses port 9000. You can always change the port numbers in your docker-compose.yml . First run of Portainer CE In your web browser navigate to #yourip:9002/ . If you are migrating from Portainer : review the Migration note which explains why your Portainer credentials will likely apply to Portainer CE , and then supply your Portainer credentials. If you are not migrating from Portainer : the first screen will suggest a username of \"admin\" and ask for a password. Supply those credentials and click \"Create User\". the second screen will ask you to select a connection method. For IOTstack, \"Docker (Manage the local Docker environment)\" is usually appropriate so click that and then click \"Connect\". From there, you can click on the \"Local\" group and take a look around. One of the things Portainer CE can help you do is find unused containers but beware of reading too much into this because, sometimes, an \"unused\" container is actually the base for another container (eg Node-Red). There are 'Quick actions' to view logs and other stats. This can all be done from terminal commands but Portainer CE makes it easier. Ceasing use of Portainer As soon as you are happy that Portainer CE meets your needs, you can dispense with Portainer . IOTstack only has limited support for getting rid of unwanted services so you should do the following. Stop Portainer from running and remove its image: $ cd ~/IOTstack $ docker-compose stop portainer $ docker-compose rm -f portainer $ docker rmi portainer/portainer Either: run menu.sh choose \"Build Stack\" de-select \"portainer\", and follow through to the end choosing \"Do not overwrite\" for existing services, or: edit docker-compose.yml and remove these lines: portainer: container_name: portainer image: portainer/portainer restart: unless-stopped ports: - \"9000:9000\" volumes: - /var/run/docker.sock:/var/run/docker.sock - ./volumes/portainer/data:/data edit services/selection.txt and remove this line: portainer Tidy-up: $ cd ~/IOTstack $ rm -rf ./services/portainer $ sudo rm -rf ./volumes/portainer Setting the Public IP address for your end-point If you click on a \"Published Port\" in the \"Containers\" list, your browser may return an error saying something like \"can't connect to server\" associated with an IP address of \"0.0.0.0\". To fix that problem, proceed as shown below: Click \"Endpoints\" in the left hand panel. Click the name \"local\" in the list of Endpoints. Click in the \"Public IP\" field. Enter one of the following: The multicast DNS (MDNS) name of your Raspberry Pi (eg iot-hub.local ) The fully-qualified domain name (FQDN) of your Raspberry Pi (eg iot-hub.mydomain.com ) The IP address of your Raspberry Pi (eg 192.168.1.10 ) Click \"Update endpoint\". To remove the Public IP address, repeat the above steps but clear the \"Public IP\" field in step 3. The reason why you have to tell Portainer CE which Public IP address to use is because an instance of Portainer CE does not necessarily have to be running on the same Raspberry Pi as the Docker containers it is managing. Keep in mind that clicking on a \"Published Port\" does not guarantee that your browser can open a connection. For example: Port 1883 for Mosquitto expects MQTT packets. It will not respond to HTTP, so any attempt will fail. Port 8089 for PiHole will respond to HTTP but PiHole may reject or mis-handle your attempt. Port 1880 for NodeRed will respond normally. All things considered, you will get more consistent behaviour if you simply bookmark the URLs you want to use for your IOTstack services. If you forget your password If you forget the password you created for Portainer CE , you can recover by doing the following: $ cd ~/IOTstack $ docker-compose stop portainer-ce $ sudo rm -r ./volumes/portainer-ce $ docker-compose start portainer-ce Then use your browser to navigate to #yourip:9002/ and follow the steps in if you are not migrating from Portainer .","title":"Portainer CE"},{"location":"Containers/Portainer-ce/#portainer-ce","text":"","title":"Portainer CE"},{"location":"Containers/Portainer-ce/#references","text":"Docker Website","title":"References"},{"location":"Containers/Portainer-ce/#definition","text":"\"#yourip\" means any of the following: the IP address of your Raspberry Pi (eg 192.168.1.10 ) the multicast domain name of your Raspberry Pi (eg iot-hub.local ) the domain name of your Raspberry Pi (eg iot-hub.mydomain.com )","title":"Definition"},{"location":"Containers/Portainer-ce/#about-portainer-ce","text":"Portainer CE (Community Edition) is an application for managing Docker. It is a successor to Portainer . According to the Portainer CE documentation Portainer 1.24.x will continue as a separate code branch, released as portainer/portainer:latest, and will receive ongoing security updates until at least 1st Sept 2021. No new features will be added beyond what was available in 1.24.1. From that it should be clear that Portainer is deprecated and that Portainer CE is the way forward.","title":"About Portainer CE"},{"location":"Containers/Portainer-ce/#portainer-ce-coexistence-with-portainer","text":"IOTstack has been set up so that Portainer CE and Portainer can coexist. This is intended as a short-term migration aid rather than a long-term proposition. If you are a first-time user of IOTstack, you should choose Portainer CE and forget about Portainer .","title":"Portainer CE coexistence with Portainer"},{"location":"Containers/Portainer-ce/#installing-portainer-ce","text":"Run the menu: $ cd ~/IOTstack $ ./menu.sh Choose \"Build Stack\", select \"Portainer-ce\", press [TAB] then \"\\<Ok>\" and follow through to the end of the menu process, typically choosing \"Do not overwrite\" for any existing services. When the menu finishes: $ docker-compose up -d Ignore any message like this: WARNING: Found orphan containers (portainer) for this project \u2026","title":"Installing Portainer CE"},{"location":"Containers/Portainer-ce/#migration-note","text":"Portainer CE and Portainer use different locations for their persistent data: Edition Persistent Data Directory Reference Portainer ~/IOTstack/volumes/portainer [A] Portainer CE ~/IOTstack/volumes/portainer-ce [B] If you have been running Portainer but have never run Portainer CE then: [A] will exist, but [B] will not exist. Whenever \"Portainer-ce\" is enabled in menu.sh , a check is made for the presence of [A] combined with the absence [B]. If and only if that situation exists, [B] is initialised as a copy of [A]. This one-time copy is intended to preserve your Portainer settings and admin user password for use in Portainer CE . Thereafter, any settings you change in Portainer CE will not be reflected in Portainer , nor vice versa.","title":" Migration note "},{"location":"Containers/Portainer-ce/#port-number-9002","text":"Both Portainer CE and Portainer are usually configured to listen to port 9000 but, in the IOTstack implementation: Portainer CE uses port 9002; and Portainer uses port 9000. You can always change the port numbers in your docker-compose.yml .","title":"Port Number = 9002"},{"location":"Containers/Portainer-ce/#first-run-of-portainer-ce","text":"In your web browser navigate to #yourip:9002/ . If you are migrating from Portainer : review the Migration note which explains why your Portainer credentials will likely apply to Portainer CE , and then supply your Portainer credentials. If you are not migrating from Portainer : the first screen will suggest a username of \"admin\" and ask for a password. Supply those credentials and click \"Create User\". the second screen will ask you to select a connection method. For IOTstack, \"Docker (Manage the local Docker environment)\" is usually appropriate so click that and then click \"Connect\". From there, you can click on the \"Local\" group and take a look around. One of the things Portainer CE can help you do is find unused containers but beware of reading too much into this because, sometimes, an \"unused\" container is actually the base for another container (eg Node-Red). There are 'Quick actions' to view logs and other stats. This can all be done from terminal commands but Portainer CE makes it easier.","title":"First run of Portainer CE"},{"location":"Containers/Portainer-ce/#ceasing-use-of-portainer","text":"As soon as you are happy that Portainer CE meets your needs, you can dispense with Portainer . IOTstack only has limited support for getting rid of unwanted services so you should do the following. Stop Portainer from running and remove its image: $ cd ~/IOTstack $ docker-compose stop portainer $ docker-compose rm -f portainer $ docker rmi portainer/portainer Either: run menu.sh choose \"Build Stack\" de-select \"portainer\", and follow through to the end choosing \"Do not overwrite\" for existing services, or: edit docker-compose.yml and remove these lines: portainer: container_name: portainer image: portainer/portainer restart: unless-stopped ports: - \"9000:9000\" volumes: - /var/run/docker.sock:/var/run/docker.sock - ./volumes/portainer/data:/data edit services/selection.txt and remove this line: portainer Tidy-up: $ cd ~/IOTstack $ rm -rf ./services/portainer $ sudo rm -rf ./volumes/portainer","title":"Ceasing use of Portainer"},{"location":"Containers/Portainer-ce/#setting-the-public-ip-address-for-your-end-point","text":"If you click on a \"Published Port\" in the \"Containers\" list, your browser may return an error saying something like \"can't connect to server\" associated with an IP address of \"0.0.0.0\". To fix that problem, proceed as shown below: Click \"Endpoints\" in the left hand panel. Click the name \"local\" in the list of Endpoints. Click in the \"Public IP\" field. Enter one of the following: The multicast DNS (MDNS) name of your Raspberry Pi (eg iot-hub.local ) The fully-qualified domain name (FQDN) of your Raspberry Pi (eg iot-hub.mydomain.com ) The IP address of your Raspberry Pi (eg 192.168.1.10 ) Click \"Update endpoint\". To remove the Public IP address, repeat the above steps but clear the \"Public IP\" field in step 3. The reason why you have to tell Portainer CE which Public IP address to use is because an instance of Portainer CE does not necessarily have to be running on the same Raspberry Pi as the Docker containers it is managing. Keep in mind that clicking on a \"Published Port\" does not guarantee that your browser can open a connection. For example: Port 1883 for Mosquitto expects MQTT packets. It will not respond to HTTP, so any attempt will fail. Port 8089 for PiHole will respond to HTTP but PiHole may reject or mis-handle your attempt. Port 1880 for NodeRed will respond normally. All things considered, you will get more consistent behaviour if you simply bookmark the URLs you want to use for your IOTstack services.","title":"Setting the Public IP address for your end-point"},{"location":"Containers/Portainer-ce/#if-you-forget-your-password","text":"If you forget the password you created for Portainer CE , you can recover by doing the following: $ cd ~/IOTstack $ docker-compose stop portainer-ce $ sudo rm -r ./volumes/portainer-ce $ docker-compose start portainer-ce Then use your browser to navigate to #yourip:9002/ and follow the steps in if you are not migrating from Portainer .","title":"If you forget your password"},{"location":"Containers/Portainer/","text":"Portainer References Docker Website Portainer restart by itself There is an issue with the armhf Portainer image where it randomly restarts. This does not affect its operation. The bug has been reported. About Portainer is a great application for managing Docker. In your web browser navigate to #yourip:9000 . You will be asked to choose a password. In the next window select 'Local' and connect, it shouldn't ask you this again. From here you can play around, click local, and take a look around. This can help you find unused images/containers. On the Containers section, there are 'Quick actions' to view logs and other stats. Note: This can all be done from the CLI but portainer just makes it much much easier. Setup Public IP When you first run Portainer and navigate to the Containers list you will see that there is a clickable link to the ports however this will direct you to 0.0.0.0:port . This is because Portainer doesn't know your IP address. This can be set in the endpoint and set the public IP Forgotten password If you have forgotten the password you created for the container, stop the stack remove portainers volume with sudo rm -r ./volumes/portainer and start the stack. Your browser may get a little confused when it restarts. Just navigate to \"yourip:9000\" (may require more than one attempt) and create your new login details. If it doesn't ask you to connect to the 'Local' docker or shows an empty endpoint just logout and log back in and it will give you the option. From now on it should just work fine.","title":"Portainer"},{"location":"Containers/Portainer/#portainer","text":"","title":"Portainer"},{"location":"Containers/Portainer/#references","text":"Docker Website","title":"References"},{"location":"Containers/Portainer/#portainer-restart-by-itself","text":"There is an issue with the armhf Portainer image where it randomly restarts. This does not affect its operation. The bug has been reported.","title":"Portainer restart by itself"},{"location":"Containers/Portainer/#about","text":"Portainer is a great application for managing Docker. In your web browser navigate to #yourip:9000 . You will be asked to choose a password. In the next window select 'Local' and connect, it shouldn't ask you this again. From here you can play around, click local, and take a look around. This can help you find unused images/containers. On the Containers section, there are 'Quick actions' to view logs and other stats. Note: This can all be done from the CLI but portainer just makes it much much easier.","title":"About"},{"location":"Containers/Portainer/#setup-public-ip","text":"When you first run Portainer and navigate to the Containers list you will see that there is a clickable link to the ports however this will direct you to 0.0.0.0:port . This is because Portainer doesn't know your IP address. This can be set in the endpoint and set the public IP","title":"Setup Public IP"},{"location":"Containers/Portainer/#forgotten-password","text":"If you have forgotten the password you created for the container, stop the stack remove portainers volume with sudo rm -r ./volumes/portainer and start the stack. Your browser may get a little confused when it restarts. Just navigate to \"yourip:9000\" (may require more than one attempt) and create your new login details. If it doesn't ask you to connect to the 'Local' docker or shows an empty endpoint just logout and log back in and it will give you the option. From now on it should just work fine.","title":"Forgotten password"},{"location":"Containers/PostgreSQL/","text":"PostgreSQL References Docker Website About PostgreSQL is an SQL server, for those that need an SQL database. The database credentials can be found in the file ./volumes/postgres/postgres.env . It is highly recommended to change the user, password and default database","title":"PostgreSQL"},{"location":"Containers/PostgreSQL/#postgresql","text":"","title":"PostgreSQL"},{"location":"Containers/PostgreSQL/#references","text":"Docker Website","title":"References"},{"location":"Containers/PostgreSQL/#about","text":"PostgreSQL is an SQL server, for those that need an SQL database. The database credentials can be found in the file ./volumes/postgres/postgres.env . It is highly recommended to change the user, password and default database","title":"About"},{"location":"Containers/Python/","text":"Python Docker hub Running python code in docker In order to run code in docker the container needs to be build from a Dockerfile. There are 2 key files in the service directory services/python/requirements.txt Normally on your system you would install modules with pip and they would be available system wide. The container that comes off Docker hub is blank and we will have to install them and bake them into the container. Before your first run add the modules that you require to the requirements.txt, each on a new line flask bs4 IMPORTANT : Every time you alter the requirements file you will need to rebuild the container and bake in the new modules To build the container run docker-compose build python . services/python/service.yml This is the template that gets concatenated into docker-compose.yml and there are a few things to note here python: container_name: python build: ./services/python/. restart: unless-stopped network_mode: host volumes: - ./volumes/python/app:/usr/src/app The container runs in host network mode. This is because i have no idea which ports you want to use. The implication of this is you will not be able to connect by name to the other container and therefore if you want to connect to the mqtt service or influx you will need to use localhost or 127.0.0.1 because the python container \"thinks\" from network perspective that it is the Pi The container is set to restart unless stopped. Therefore if you write an application it will effectively execute in an endless loop. If you only want a run once method then you will need to comment out the \"restart\" section in the docker-compose.yml file and the service.yml Where to put your code You will need to copy your code to IOTstack/volumes/python/app . The container is set to execute app.py as the main file. writing to the console If you execute a print statement the text will appear in the console of the container. The output can be accessed by running docker logs python writing to disk Inside the container the working directory is /usr/src/app as mapped in the volume command. It would be advised to read or write any data from this directory. Image clutter Doing multiple builds of the python image will create many unused images. These can be cleaned up inside portainer or by running ./scripts/prune-images.sh","title":"Python"},{"location":"Containers/Python/#python","text":"Docker hub","title":"Python"},{"location":"Containers/Python/#running-python-code-in-docker","text":"In order to run code in docker the container needs to be build from a Dockerfile. There are 2 key files in the service directory","title":"Running python code in docker"},{"location":"Containers/Python/#servicespythonrequirementstxt","text":"Normally on your system you would install modules with pip and they would be available system wide. The container that comes off Docker hub is blank and we will have to install them and bake them into the container. Before your first run add the modules that you require to the requirements.txt, each on a new line flask bs4 IMPORTANT : Every time you alter the requirements file you will need to rebuild the container and bake in the new modules To build the container run docker-compose build python .","title":"services/python/requirements.txt"},{"location":"Containers/Python/#servicespythonserviceyml","text":"This is the template that gets concatenated into docker-compose.yml and there are a few things to note here python: container_name: python build: ./services/python/. restart: unless-stopped network_mode: host volumes: - ./volumes/python/app:/usr/src/app The container runs in host network mode. This is because i have no idea which ports you want to use. The implication of this is you will not be able to connect by name to the other container and therefore if you want to connect to the mqtt service or influx you will need to use localhost or 127.0.0.1 because the python container \"thinks\" from network perspective that it is the Pi The container is set to restart unless stopped. Therefore if you write an application it will effectively execute in an endless loop. If you only want a run once method then you will need to comment out the \"restart\" section in the docker-compose.yml file and the service.yml","title":"services/python/service.yml"},{"location":"Containers/Python/#where-to-put-your-code","text":"You will need to copy your code to IOTstack/volumes/python/app . The container is set to execute app.py as the main file.","title":"Where to put your code"},{"location":"Containers/Python/#writing-to-the-console","text":"If you execute a print statement the text will appear in the console of the container. The output can be accessed by running docker logs python","title":"writing to the console"},{"location":"Containers/Python/#writing-to-disk","text":"Inside the container the working directory is /usr/src/app as mapped in the volume command. It would be advised to read or write any data from this directory.","title":"writing to disk"},{"location":"Containers/Python/#image-clutter","text":"Doing multiple builds of the python image will create many unused images. These can be cleaned up inside portainer or by running ./scripts/prune-images.sh","title":"Image clutter"},{"location":"Containers/RTL_433-docker/","text":"RTL_433 Docker Requirements, you will need to have a SDR dongle for you to be able to use RTL. I've tested this with a RTL2838 Make sure you can see your receiver by running lsusb $ lsusb Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 004: ID 0bda:2838 Realtek Semiconductor Corp. RTL2838 DVB-T Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Before starting the container please install RTL_433 from the native installs menu. This will setup your environment with the correct variables and programs. It is also advised to run RTL_433 to verify that it is working correctly on your system. The container is designed to send all detected messages over mqtt Edit the IOTstack/services/rtl_433/rtl_433.env file with your relevant settings for your mqtt server: MQTT_ADDRESS=mosquitto MQTT_PORT=1833 #MQTT_USER=myuser #MQTT_PASSWORD=mypassword MQTT_TOPIC=RTL_433 the container starts with the command rtl_433 -F mqtt:.... currently it does not filter any packets, you will need to do this in Node-RED","title":"RTL_433 Docker"},{"location":"Containers/RTL_433-docker/#rtl_433-docker","text":"Requirements, you will need to have a SDR dongle for you to be able to use RTL. I've tested this with a RTL2838 Make sure you can see your receiver by running lsusb $ lsusb Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 004: ID 0bda:2838 Realtek Semiconductor Corp. RTL2838 DVB-T Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Before starting the container please install RTL_433 from the native installs menu. This will setup your environment with the correct variables and programs. It is also advised to run RTL_433 to verify that it is working correctly on your system. The container is designed to send all detected messages over mqtt Edit the IOTstack/services/rtl_433/rtl_433.env file with your relevant settings for your mqtt server: MQTT_ADDRESS=mosquitto MQTT_PORT=1833 #MQTT_USER=myuser #MQTT_PASSWORD=mypassword MQTT_TOPIC=RTL_433 the container starts with the command rtl_433 -F mqtt:.... currently it does not filter any packets, you will need to do this in Node-RED","title":"RTL_433 Docker"},{"location":"Containers/TasmoAdmin/","text":"TasmoAdmin References Homepage Docker Web interface The web UI can be found on \"your_ip\":8088 Usage (instructions to follow)","title":"TasmoAdmin"},{"location":"Containers/TasmoAdmin/#tasmoadmin","text":"","title":"TasmoAdmin"},{"location":"Containers/TasmoAdmin/#references","text":"Homepage Docker","title":"References"},{"location":"Containers/TasmoAdmin/#web-interface","text":"The web UI can be found on \"your_ip\":8088","title":"Web interface"},{"location":"Containers/TasmoAdmin/#usage","text":"(instructions to follow)","title":"Usage"},{"location":"Containers/WireGuard/","text":"WireGuard WireGuard is a fast, modern, secure VPN tunnel. It can securely connect you to your home network, allowing you to access your home network's local services from anywhere. It can also secure your traffic when using public internet connections. WARNING: These instructions require that you have privileges to configure your network's gateway. If you are not able to make changes to your network's firewall settings, then you will not be able to finish this setup. If you are able to make these changes, then proceed to the next steps. Setup There are a few things to configure before starting up the WireGuard container. First, it may be necessary to set up a way to locate your home network from the internet. One way to achieve this, if you haven't done so yet, is to set up a DuckDNS account as described in the Wiki under the section DuckDNS. This address will be used in the following configuration. After configuring the service, it needs to be made accessible from outside the home network. Lastly, each device will need WireGuard installed and set up with the details of your WireGuard service. WireGuard Configuration The Custom services and overriding default settings for IOTstack page describes how to use a compose-override.yml file to allow ./menu.sh to automatically incorporate your custom configurations into the final docker-compose.yml file that is responsible for defining all service containers. You will need to create the compose-override.yml before building your stack via ./menu.sh . If you have already built your stack, you'll have to re-build it after creating compose-override.yml . Here is an example compose-override.yml file: services: wireguard: environment: - PUID=1000 - PGID=1000 - TZ=America/Los_Angeles - SERVERURL=<Your-DuckDNS-account>.duckdns.org #optional - SERVERPORT=51820 #optional - PEERS=3 #optional - PEERDNS=auto #optional - INTERNAL_SUBNET=100.64.0.0/24 #optional The values you will probably want to change are TZ to your own timezone, SERVERURL to your own DuckDNS address and PEERS to set the number of devices you plan to connect to your VPN. If you also decide to edit the SERVERPORT value, you will also need to include a matching value in the ports: section as follows: services: wireguard: environment: - PUID=1000 - PGID=1000 - TZ=America/Los_Angeles - SERVERURL=<Your-DuckDNS-account>.duckdns.org #optional - SERVERPORT=55555 #optional - PEERS=3 #optional - PEERDNS=auto #optional - INTERNAL_SUBNET=100.64.0.0/24 #optional ports: - 55555:55555/udp If you customize other containers, just make sure the file only says services: once at the beginning of the file. Once you are done, you can run ./menu.sh to build your stack. Finally, check that your changes were successfully integrated by running: $ cat docker-compose.yml If everything looks good, you can run the following to start your container: $ docker-compose up -d Network Configuration A typical home network will have a firewall configured that effectively blocks all incoming attempts to open a new connection with devices on the network. However, in order to use our VPN from outside of our home network (which is precisely the point of running the service!), we need to configure port fowarding to allow incoming connections to reach our device running IOTstack. This step of the configuration varies based on the specific gateway device for your network. Note that these instructions assume you have privileges to configure your gateway's firewall settings (see warning above). This section will include some tips, but if you are unsure how to do this, the best idea would be to search the web for \"[YOUR DEVICE NAME] port forwarding configuration\". NOTE: WireGuard uses UDP, not TCP. So make sure your port forwarding rules are for UDP only. First, it's a good idea to check that WireGuard is at least accessible on the local network by using nmap : $ sudo nmap -sU -p 51820 ip.of.IOTstack.device OR from the IOTstack device itself: $ sudo nmap -sU -p 51820 127.0.0.1 PORT STATE SERVICE 51820/udp open|filtered unknown MAC Address: XX:XX:XX:XX:XX:XX (Unknown) If your result looks similar, then WireGuard is up and running and you simply need to set up port forwarding. Notice again, that WireGuard uses UDP. Many routers/gateways are configurable via a web interface, in which case you will only need the ip address of the device, as well as the account and password to access it. You should be able to find your gateway's address with the following command: $ ip route | grep default default via 192.168.1.1 dev eth0 proto dhcp metric 100 Then copy the ip to a browser window to configure. The login credentials may be physically printed on the device if you have never logged in or changed the default credentials. Follow the instructions to configure UDP port forwarding for your network. Make sure that you configure only UDP port forwarding, only pointing specifically at your IOTstack device (by ip or hostname, whichever is more appropriate for your network configuration) and only for port 51820 (or whichever port you have configured for WireGuard). Remember that you are opening this port to the public internet, so be careful not to leave anything open that you're not using or point to the wrong device. Once you are finished, save your changes and test that the port is open from the internet, again using nmap : $ sudo nmap -sU -p 51820 <your-duckdns-account>.duckdns.org PORT STATE SERVICE 51820/udp open|filtered unknown MAC Address: XX:XX:XX:XX:XX:XX (Unknown) If everything looks good, then the last step is to set up your devices to connect to your WireGuard service. Device Setup Lastly, it's time to set up each device to connect to your VPN. You will need to install the WireGuard client on each device. This can be typically be done via each device's app store or package manager. For complete install instructions, see the WireGuard Installation page . QR Code Mobile Device Setup After the client is installed on your devices, each one needs its own WireGuard peer configuration. The easiest devices to set up are mobile devices, which can be done by using the QR codes that are automatically generated for each WireGuard PEER, as defined in the docker-compose.yml file. The QR codes are located in the following locations: ~/IOTstack/services/wireguard/config/peer1/peer1.png ~/IOTstack/services/wireguard/config/peer2/peer2.png ~/IOTstack/services/wireguard/config/peer3/peer3.png ... To copy the files from a Raspberry Pi onto another Linux machine for example, you can use the following command: $ sudo scp pi@<Rpi-ip-address>:/home/pi/IOTstack/services/wireguard/config/peer1/peer1.png ~/peer1.png (Hint: you can use the scp -i flag to specify an IdentityFile or better yet, scp -F flag if you have your device configured in .ssh/config ) You can repeat this step for each peer's QR code .png file and then scan the QR codes in the mobile app on your devices. The devices should now be configured and able to connect to your VPN. Setting Up Other Devices Setting up other devices is a bit more complicated. Refer to the WireGuard Quick Start page or search for instructions specific to your OS.","title":"WireGuard"},{"location":"Containers/WireGuard/#wireguard","text":"WireGuard is a fast, modern, secure VPN tunnel. It can securely connect you to your home network, allowing you to access your home network's local services from anywhere. It can also secure your traffic when using public internet connections. WARNING: These instructions require that you have privileges to configure your network's gateway. If you are not able to make changes to your network's firewall settings, then you will not be able to finish this setup. If you are able to make these changes, then proceed to the next steps.","title":"WireGuard"},{"location":"Containers/WireGuard/#setup","text":"There are a few things to configure before starting up the WireGuard container. First, it may be necessary to set up a way to locate your home network from the internet. One way to achieve this, if you haven't done so yet, is to set up a DuckDNS account as described in the Wiki under the section DuckDNS. This address will be used in the following configuration. After configuring the service, it needs to be made accessible from outside the home network. Lastly, each device will need WireGuard installed and set up with the details of your WireGuard service.","title":"Setup"},{"location":"Containers/WireGuard/#wireguard-configuration","text":"The Custom services and overriding default settings for IOTstack page describes how to use a compose-override.yml file to allow ./menu.sh to automatically incorporate your custom configurations into the final docker-compose.yml file that is responsible for defining all service containers. You will need to create the compose-override.yml before building your stack via ./menu.sh . If you have already built your stack, you'll have to re-build it after creating compose-override.yml . Here is an example compose-override.yml file: services: wireguard: environment: - PUID=1000 - PGID=1000 - TZ=America/Los_Angeles - SERVERURL=<Your-DuckDNS-account>.duckdns.org #optional - SERVERPORT=51820 #optional - PEERS=3 #optional - PEERDNS=auto #optional - INTERNAL_SUBNET=100.64.0.0/24 #optional The values you will probably want to change are TZ to your own timezone, SERVERURL to your own DuckDNS address and PEERS to set the number of devices you plan to connect to your VPN. If you also decide to edit the SERVERPORT value, you will also need to include a matching value in the ports: section as follows: services: wireguard: environment: - PUID=1000 - PGID=1000 - TZ=America/Los_Angeles - SERVERURL=<Your-DuckDNS-account>.duckdns.org #optional - SERVERPORT=55555 #optional - PEERS=3 #optional - PEERDNS=auto #optional - INTERNAL_SUBNET=100.64.0.0/24 #optional ports: - 55555:55555/udp If you customize other containers, just make sure the file only says services: once at the beginning of the file. Once you are done, you can run ./menu.sh to build your stack. Finally, check that your changes were successfully integrated by running: $ cat docker-compose.yml If everything looks good, you can run the following to start your container: $ docker-compose up -d","title":"WireGuard Configuration"},{"location":"Containers/WireGuard/#network-configuration","text":"A typical home network will have a firewall configured that effectively blocks all incoming attempts to open a new connection with devices on the network. However, in order to use our VPN from outside of our home network (which is precisely the point of running the service!), we need to configure port fowarding to allow incoming connections to reach our device running IOTstack. This step of the configuration varies based on the specific gateway device for your network. Note that these instructions assume you have privileges to configure your gateway's firewall settings (see warning above). This section will include some tips, but if you are unsure how to do this, the best idea would be to search the web for \"[YOUR DEVICE NAME] port forwarding configuration\". NOTE: WireGuard uses UDP, not TCP. So make sure your port forwarding rules are for UDP only. First, it's a good idea to check that WireGuard is at least accessible on the local network by using nmap : $ sudo nmap -sU -p 51820 ip.of.IOTstack.device OR from the IOTstack device itself: $ sudo nmap -sU -p 51820 127.0.0.1 PORT STATE SERVICE 51820/udp open|filtered unknown MAC Address: XX:XX:XX:XX:XX:XX (Unknown) If your result looks similar, then WireGuard is up and running and you simply need to set up port forwarding. Notice again, that WireGuard uses UDP. Many routers/gateways are configurable via a web interface, in which case you will only need the ip address of the device, as well as the account and password to access it. You should be able to find your gateway's address with the following command: $ ip route | grep default default via 192.168.1.1 dev eth0 proto dhcp metric 100 Then copy the ip to a browser window to configure. The login credentials may be physically printed on the device if you have never logged in or changed the default credentials. Follow the instructions to configure UDP port forwarding for your network. Make sure that you configure only UDP port forwarding, only pointing specifically at your IOTstack device (by ip or hostname, whichever is more appropriate for your network configuration) and only for port 51820 (or whichever port you have configured for WireGuard). Remember that you are opening this port to the public internet, so be careful not to leave anything open that you're not using or point to the wrong device. Once you are finished, save your changes and test that the port is open from the internet, again using nmap : $ sudo nmap -sU -p 51820 <your-duckdns-account>.duckdns.org PORT STATE SERVICE 51820/udp open|filtered unknown MAC Address: XX:XX:XX:XX:XX:XX (Unknown) If everything looks good, then the last step is to set up your devices to connect to your WireGuard service.","title":"Network Configuration"},{"location":"Containers/WireGuard/#device-setup","text":"Lastly, it's time to set up each device to connect to your VPN. You will need to install the WireGuard client on each device. This can be typically be done via each device's app store or package manager. For complete install instructions, see the WireGuard Installation page .","title":"Device Setup"},{"location":"Containers/WireGuard/#qr-code-mobile-device-setup","text":"After the client is installed on your devices, each one needs its own WireGuard peer configuration. The easiest devices to set up are mobile devices, which can be done by using the QR codes that are automatically generated for each WireGuard PEER, as defined in the docker-compose.yml file. The QR codes are located in the following locations: ~/IOTstack/services/wireguard/config/peer1/peer1.png ~/IOTstack/services/wireguard/config/peer2/peer2.png ~/IOTstack/services/wireguard/config/peer3/peer3.png ... To copy the files from a Raspberry Pi onto another Linux machine for example, you can use the following command: $ sudo scp pi@<Rpi-ip-address>:/home/pi/IOTstack/services/wireguard/config/peer1/peer1.png ~/peer1.png (Hint: you can use the scp -i flag to specify an IdentityFile or better yet, scp -F flag if you have your device configured in .ssh/config ) You can repeat this step for each peer's QR code .png file and then scan the QR codes in the mobile app on your devices. The devices should now be configured and able to connect to your VPN.","title":"QR Code Mobile Device Setup"},{"location":"Containers/WireGuard/#setting-up-other-devices","text":"Setting up other devices is a bit more complicated. Refer to the WireGuard Quick Start page or search for instructions specific to your OS.","title":"Setting Up Other Devices"},{"location":"Containers/Zigbee2MQTT/","text":"Zigbee2MQTT Web Guide Flashing the CC2531 Figuring-out your device identifier Service definition change - April 2021 The IOTstack service definition for Zigbee2MQTT is at the following path: ~/IOTstack/.templates/zigbee2mqtt/service.yml As of April 2021, the service definition changed: The Zigbee2MQTT container no longer runs in host mode. Adds timezone support. Builds the container from a Dockerfile providing appropriate defaults for IOTstack. Re-adds a port mapping for port 8080 (the Zigbee2MQTT web UI). If you were running the Zigbee2MQTT service before this change, you may wish to compare and contrast your active service definition (in docker-compose.yml ) with the revised template. Note: You may need to git pull to update your local copy of the IOTstack repository against GitHub. First startup with CC2531 adapter The service definition includes: devices: - /dev/ttyAMA0:/dev/ttyACM0 # should work even if no adapter #- /dev/ttyACM0:/dev/ttyACM0 # should work if CC2531 connected #- /dev/ttyUSB0:/dev/ttyACM0 # Electrolama zig-a-zig-ah! (zzh!) maybe other as well The default device ( /dev/ttyAMA0 ) probably will not work for any Zigbee adapter. It is only there because /dev/ttyAMA0 exists on Raspberry Pis. Its presence permits the container to come up even though it will not actually be able to connect to an adapter. If you have a CC2531, properly flashed and connected to a USB port, you should be able to see it: $ ls -l /dev/ttyACM0 crw-rw---- 1 root dialout 166, 0 Apr 7 09:38 /dev/ttyACM0 If you see the error \"No such file or directory\", you will need to first figure out why your device is not visible. Assuming your CC2531 is visible: Change the device mapping in docker-compose.yml to deactivate ttyAMA0 in favour of activating ttyACM0 : devices: #- /dev/ttyAMA0:/dev/ttyACM0 # should work even if no adapter - /dev/ttyACM0:/dev/ttyACM0 # should work if CC2531 connected #- /dev/ttyUSB0:/dev/ttyACM0 # Electrolama zig-a-zig-ah! (zzh!) maybe other as well Bring up the container: $ cd ~/IOTstack $ docker-compose up -d zigbee2mqtt You can also follow the instructions in the Zigbee2MQTT documentation to work out the identifier of your device and use that instead of /dev/ttyACM0 . Then, your docker-compose.yml might look something like this: devices: #- /dev/ttyAMA0:/dev/ttyACM0 # should work even if no adapter #- /dev/ttyACM0:/dev/ttyACM0 # should work if CC2531 connected #- /dev/ttyUSB0:/dev/ttyACM0 # Electrolama zig-a-zig-ah! (zzh!) maybe other as well - \"/dev/serial/by-id/usb-Texas_Instruments_TI_CC2531_USB_CDC___xxx:/dev/ttyACM0\" First startup with other adapters Similar principles apply if you use other adapters. You must work out how the adapter presents itself on your Raspberry Pi and then map it to /dev/ttyACM0 inside the container (ie the common right hand side of every device definition). Configuration file Active configuration file Under IOTstack, the active configuration file for Zigbee2MQTT appears at the following path: ~/IOTstack/volumes/zigbee2mqtt/data/configuration.yaml After you make any changes to the configuration file (using sudo ), you need to inform the running container by: $ cd ~/IOTstack $ docker-compose restart zigbee2mqtt Default configuration file The IOTstack version of Zigbee2MQTT is built using a Dockerfile located at: ~/IOTstack/.templates/zigbee2mqtt/Dockerfile The Dockerfile downloads the base koenkk/zigbee2mqtt image from DockerHub and then alters the default configuration file as it builds a local image to: change the default MQTT server URL from \"mqtt://localhost\" to \"mqtt://mosquitto\"; and activate the Zigbee2MQTT web interface on port 8080. Those changes are intended to help new IOTstack installations get started with a minimum of fuss. However, the default configuration file will only become the active configuration file in two situations: On a first install of Zigbee2MQTT; or If you erase the container's persistent storage area. For example: $ cd ~/IOTstack $ docker-compose stop zigbee2mqtt $ docker-compose rm -f zigbee2mqtt $ sudo rm -rf ./volumes/zigbee2mqtt $ docker-compose up -d zigbee2mqtt In either of those situations, the active configuration file will be initialised by copying the default configuration file into place as the container comes up. If you have an existing configuration file If you have an existing active Zigbee2MQTT configuration file, you may need to make two changes: Alter the Mosquitto URL: before: server: 'mqtt://localhost' - after: server: 'mqtt://mosquitto' Enable the web interface (if necessary): append: frontend: port: 8080 Checking that the container is working Checking status $ docker ps --format \"table {{.Names}}\\t{{.RunningFor}}\\t{{.Status}}\" --filter name=\"zigbee2mqtt\" NAMES CREATED STATUS zigbee2mqtt 2 hours ago Up 2 hours You are looking for signs that the container is restarting (ie the \"Status\" column only ever shows a low number of seconds). Checking the log $ docker logs zigbee2mqtt You are looking for evidence of malfunction. Checking that Zigbee2MQTT is able to communicate with Mosquitto If you have the Mosquitto clients installed ( sudo apt install -y mosquitto-clients ), you can run the following command: $ mosquitto_sub -v -h \"localhost\" -t \"zigbee2mqtt/#\" -F \"%I %t %p\" One of two things will happen: silence, indicating that Zigbee2MQTT is not able to communicate with Mosquitto. chatter, proving that Zigbee2MQTT can communicate with Mosquitto. Terminate the mosquitto_sub command with a Control-C. Checking that the Zigbee2MQTT web GUI is working Open a browser, and point it to port 8080 on your Raspberry Pi. You should see the Zigbee2MQTT interface. terminal access inside the container To access the terminal run: $ docker exec -it zigbee2mqtt ash ash is not a typo! When you want to leave the container, either type exit and press return, or press Control-D. Container maintenance Because the Zigbee2MQTT container is built from a Dockerfile, a normal pull command will not automatically download any updates released on DockerHub. When you become aware of a new version of Zigbee2MQTT being released on DockerHub, do the following: $ cd ~IOTstack $ docker-compose build --no-cache --pull zigbee2mqtt $ docker-compose up -d zigbee2mqtt $ docker system prune Note: Sometimes it is necessary to repeat the docker system prune command.","title":"Zigbee2MQTT"},{"location":"Containers/Zigbee2MQTT/#zigbee2mqtt","text":"Web Guide Flashing the CC2531 Figuring-out your device identifier","title":"Zigbee2MQTT"},{"location":"Containers/Zigbee2MQTT/#service-definition-change-april-2021","text":"The IOTstack service definition for Zigbee2MQTT is at the following path: ~/IOTstack/.templates/zigbee2mqtt/service.yml As of April 2021, the service definition changed: The Zigbee2MQTT container no longer runs in host mode. Adds timezone support. Builds the container from a Dockerfile providing appropriate defaults for IOTstack. Re-adds a port mapping for port 8080 (the Zigbee2MQTT web UI). If you were running the Zigbee2MQTT service before this change, you may wish to compare and contrast your active service definition (in docker-compose.yml ) with the revised template. Note: You may need to git pull to update your local copy of the IOTstack repository against GitHub.","title":"Service definition change - April 2021"},{"location":"Containers/Zigbee2MQTT/#first-startup-with-cc2531-adapter","text":"The service definition includes: devices: - /dev/ttyAMA0:/dev/ttyACM0 # should work even if no adapter #- /dev/ttyACM0:/dev/ttyACM0 # should work if CC2531 connected #- /dev/ttyUSB0:/dev/ttyACM0 # Electrolama zig-a-zig-ah! (zzh!) maybe other as well The default device ( /dev/ttyAMA0 ) probably will not work for any Zigbee adapter. It is only there because /dev/ttyAMA0 exists on Raspberry Pis. Its presence permits the container to come up even though it will not actually be able to connect to an adapter. If you have a CC2531, properly flashed and connected to a USB port, you should be able to see it: $ ls -l /dev/ttyACM0 crw-rw---- 1 root dialout 166, 0 Apr 7 09:38 /dev/ttyACM0 If you see the error \"No such file or directory\", you will need to first figure out why your device is not visible. Assuming your CC2531 is visible: Change the device mapping in docker-compose.yml to deactivate ttyAMA0 in favour of activating ttyACM0 : devices: #- /dev/ttyAMA0:/dev/ttyACM0 # should work even if no adapter - /dev/ttyACM0:/dev/ttyACM0 # should work if CC2531 connected #- /dev/ttyUSB0:/dev/ttyACM0 # Electrolama zig-a-zig-ah! (zzh!) maybe other as well Bring up the container: $ cd ~/IOTstack $ docker-compose up -d zigbee2mqtt You can also follow the instructions in the Zigbee2MQTT documentation to work out the identifier of your device and use that instead of /dev/ttyACM0 . Then, your docker-compose.yml might look something like this: devices: #- /dev/ttyAMA0:/dev/ttyACM0 # should work even if no adapter #- /dev/ttyACM0:/dev/ttyACM0 # should work if CC2531 connected #- /dev/ttyUSB0:/dev/ttyACM0 # Electrolama zig-a-zig-ah! (zzh!) maybe other as well - \"/dev/serial/by-id/usb-Texas_Instruments_TI_CC2531_USB_CDC___xxx:/dev/ttyACM0\"","title":"First startup with CC2531 adapter"},{"location":"Containers/Zigbee2MQTT/#first-startup-with-other-adapters","text":"Similar principles apply if you use other adapters. You must work out how the adapter presents itself on your Raspberry Pi and then map it to /dev/ttyACM0 inside the container (ie the common right hand side of every device definition).","title":"First startup with other adapters"},{"location":"Containers/Zigbee2MQTT/#configuration-file","text":"","title":"Configuration file"},{"location":"Containers/Zigbee2MQTT/#active-configuration-file","text":"Under IOTstack, the active configuration file for Zigbee2MQTT appears at the following path: ~/IOTstack/volumes/zigbee2mqtt/data/configuration.yaml After you make any changes to the configuration file (using sudo ), you need to inform the running container by: $ cd ~/IOTstack $ docker-compose restart zigbee2mqtt","title":"Active configuration file"},{"location":"Containers/Zigbee2MQTT/#default-configuration-file","text":"The IOTstack version of Zigbee2MQTT is built using a Dockerfile located at: ~/IOTstack/.templates/zigbee2mqtt/Dockerfile The Dockerfile downloads the base koenkk/zigbee2mqtt image from DockerHub and then alters the default configuration file as it builds a local image to: change the default MQTT server URL from \"mqtt://localhost\" to \"mqtt://mosquitto\"; and activate the Zigbee2MQTT web interface on port 8080. Those changes are intended to help new IOTstack installations get started with a minimum of fuss. However, the default configuration file will only become the active configuration file in two situations: On a first install of Zigbee2MQTT; or If you erase the container's persistent storage area. For example: $ cd ~/IOTstack $ docker-compose stop zigbee2mqtt $ docker-compose rm -f zigbee2mqtt $ sudo rm -rf ./volumes/zigbee2mqtt $ docker-compose up -d zigbee2mqtt In either of those situations, the active configuration file will be initialised by copying the default configuration file into place as the container comes up.","title":"Default configuration file"},{"location":"Containers/Zigbee2MQTT/#if-you-have-an-existing-configuration-file","text":"If you have an existing active Zigbee2MQTT configuration file, you may need to make two changes: Alter the Mosquitto URL: before: server: 'mqtt://localhost' - after: server: 'mqtt://mosquitto' Enable the web interface (if necessary): append: frontend: port: 8080","title":"If you have an existing configuration file"},{"location":"Containers/Zigbee2MQTT/#checking-that-the-container-is-working","text":"","title":"Checking that the container is working"},{"location":"Containers/Zigbee2MQTT/#checking-status","text":"$ docker ps --format \"table {{.Names}}\\t{{.RunningFor}}\\t{{.Status}}\" --filter name=\"zigbee2mqtt\" NAMES CREATED STATUS zigbee2mqtt 2 hours ago Up 2 hours You are looking for signs that the container is restarting (ie the \"Status\" column only ever shows a low number of seconds).","title":"Checking status"},{"location":"Containers/Zigbee2MQTT/#checking-the-log","text":"$ docker logs zigbee2mqtt You are looking for evidence of malfunction.","title":"Checking the log"},{"location":"Containers/Zigbee2MQTT/#checking-that-zigbee2mqtt-is-able-to-communicate-with-mosquitto","text":"If you have the Mosquitto clients installed ( sudo apt install -y mosquitto-clients ), you can run the following command: $ mosquitto_sub -v -h \"localhost\" -t \"zigbee2mqtt/#\" -F \"%I %t %p\" One of two things will happen: silence, indicating that Zigbee2MQTT is not able to communicate with Mosquitto. chatter, proving that Zigbee2MQTT can communicate with Mosquitto. Terminate the mosquitto_sub command with a Control-C.","title":"Checking that Zigbee2MQTT is able to communicate with Mosquitto"},{"location":"Containers/Zigbee2MQTT/#checking-that-the-zigbee2mqtt-web-gui-is-working","text":"Open a browser, and point it to port 8080 on your Raspberry Pi. You should see the Zigbee2MQTT interface.","title":"Checking that the Zigbee2MQTT web GUI is working"},{"location":"Containers/Zigbee2MQTT/#terminal-access-inside-the-container","text":"To access the terminal run: $ docker exec -it zigbee2mqtt ash ash is not a typo! When you want to leave the container, either type exit and press return, or press Control-D.","title":"terminal access inside the container"},{"location":"Containers/Zigbee2MQTT/#container-maintenance","text":"Because the Zigbee2MQTT container is built from a Dockerfile, a normal pull command will not automatically download any updates released on DockerHub. When you become aware of a new version of Zigbee2MQTT being released on DockerHub, do the following: $ cd ~IOTstack $ docker-compose build --no-cache --pull zigbee2mqtt $ docker-compose up -d zigbee2mqtt $ docker system prune Note: Sometimes it is necessary to repeat the docker system prune command.","title":"Container maintenance"},{"location":"Containers/Zigbee2mqttassistant/","text":"Zigbee2Mqtt Assistant References Docker Website About This service a web frontend which displays Zigbee2Mqtt service messages and able to control it over MQTT. For the servie a working MQTT server is required and that have to be configured. Environment Parameters Z2MA_SETTINGS__MQTTSERVER=mosquitto - The MQTT service instance which is used by Zigbee2Mqtt instance. Here, \"mosquitto\" is the name of the container. Z2MA_SETTINGS__MQTTUSERNAME=name - Used if your MQTT service has authentication enabled. Optional. Z2MA_SETTINGS__MQTTPASSWORD=password - Used if your MQTT service has authentication enabled. Optional. TZ=Etc/UTC - Set to your timezone. Optional but recommended. Accessing the UI The Zigbee2Mqtt Assistant UI is available using port 8880. For example: http://your.local.ip.address:8880/","title":"Zigbee2Mqtt Assistant"},{"location":"Containers/Zigbee2mqttassistant/#zigbee2mqtt-assistant","text":"","title":"Zigbee2Mqtt Assistant"},{"location":"Containers/Zigbee2mqttassistant/#references","text":"Docker Website","title":"References"},{"location":"Containers/Zigbee2mqttassistant/#about","text":"This service a web frontend which displays Zigbee2Mqtt service messages and able to control it over MQTT. For the servie a working MQTT server is required and that have to be configured.","title":"About"},{"location":"Containers/Zigbee2mqttassistant/#environment-parameters","text":"Z2MA_SETTINGS__MQTTSERVER=mosquitto - The MQTT service instance which is used by Zigbee2Mqtt instance. Here, \"mosquitto\" is the name of the container. Z2MA_SETTINGS__MQTTUSERNAME=name - Used if your MQTT service has authentication enabled. Optional. Z2MA_SETTINGS__MQTTPASSWORD=password - Used if your MQTT service has authentication enabled. Optional. TZ=Etc/UTC - Set to your timezone. Optional but recommended.","title":"Environment Parameters"},{"location":"Containers/Zigbee2mqttassistant/#accessing-the-ui","text":"The Zigbee2Mqtt Assistant UI is available using port 8880. For example: http://your.local.ip.address:8880/","title":"Accessing the UI"},{"location":"Containers/deconz/","text":"deCONZ References Docker Website Troubleshooting Make sure your Conbee/Conbee II/RaspBee gateway is connected. If your gateway is not detected, or no lights can be paired, try moving the device to another usb port, reboot your computer and build the stack from the menu again cd ~/IOTstack && bash ./menu.sh (select \"Pull full service from template\" if prompted). The gateway must be plugged in when the deCONZ Docker container is being built. Before running docker-compose up -d , make sure your Linux user is part of the dialout group, which allows the user access to serial devices (i.e. Conbee/Conbee II/RaspBee). If you are not certain, simply add your user to the dialout group by running the following command (username \"pi\" being used as an example): sudo usermod -a -G dialout pi Now run docker-compose up -d to build the stack. If you are still experiencing issues, run docker-compose down to remove all containers from the stack and then docker-compose up -d to build them again. Use a 0.5-1m usb extension cable with ConBee (II) to avoid wifi and bluetooth noise/interference from your Raspberry Pi (recommended by the manufacturer and often the solution to poor performance). Accessing the Phoscon UI The Phoscon UI is available using port 8090 (http://your.local.ip.address:8090/) Viewing the deCONZ Zigbee mesh The Zigbee mesh can be viewed using VNC on port 5901. The default VNC password is \"changeme\". Connecting deCONZ and Node-RED Install node-red-contrib-deconz via the \"Manage palette\" menu in Node-RED (if not already installed) and follow these 2 simple steps (also shown in the video below): Step 1: In the Phoscon UI, Go to Settings > Gateway > Advanced and click \"Authenticate app\". Step 2: In Node-RED, open a deCONZ node, select \"Add new deonz-server\", insert your ip adress and port 8090 and click \"Get settings\". Click \"Add\", \"Done\" and \"Deploy\". Your device list will not be updated before deploying.","title":"deCONZ"},{"location":"Containers/deconz/#deconz","text":"","title":"deCONZ"},{"location":"Containers/deconz/#references","text":"Docker Website","title":"References"},{"location":"Containers/deconz/#troubleshooting","text":"Make sure your Conbee/Conbee II/RaspBee gateway is connected. If your gateway is not detected, or no lights can be paired, try moving the device to another usb port, reboot your computer and build the stack from the menu again cd ~/IOTstack && bash ./menu.sh (select \"Pull full service from template\" if prompted). The gateway must be plugged in when the deCONZ Docker container is being built. Before running docker-compose up -d , make sure your Linux user is part of the dialout group, which allows the user access to serial devices (i.e. Conbee/Conbee II/RaspBee). If you are not certain, simply add your user to the dialout group by running the following command (username \"pi\" being used as an example): sudo usermod -a -G dialout pi Now run docker-compose up -d to build the stack. If you are still experiencing issues, run docker-compose down to remove all containers from the stack and then docker-compose up -d to build them again. Use a 0.5-1m usb extension cable with ConBee (II) to avoid wifi and bluetooth noise/interference from your Raspberry Pi (recommended by the manufacturer and often the solution to poor performance).","title":"Troubleshooting"},{"location":"Containers/deconz/#accessing-the-phoscon-ui","text":"The Phoscon UI is available using port 8090 (http://your.local.ip.address:8090/)","title":"Accessing the Phoscon UI"},{"location":"Containers/deconz/#viewing-the-deconz-zigbee-mesh","text":"The Zigbee mesh can be viewed using VNC on port 5901. The default VNC password is \"changeme\".","title":"Viewing the deCONZ Zigbee mesh"},{"location":"Containers/deconz/#connecting-deconz-and-node-red","text":"Install node-red-contrib-deconz via the \"Manage palette\" menu in Node-RED (if not already installed) and follow these 2 simple steps (also shown in the video below): Step 1: In the Phoscon UI, Go to Settings > Gateway > Advanced and click \"Authenticate app\". Step 2: In Node-RED, open a deCONZ node, select \"Add new deonz-server\", insert your ip adress and port 8090 and click \"Get settings\". Click \"Add\", \"Done\" and \"Deploy\". Your device list will not be updated before deploying.","title":"Connecting deCONZ and Node-RED"},{"location":"Containers/diyHue/","text":"DIY hue website About diyHue is a utility to contol the lights in your home Setup Before you start diyHue you will need to get your IP and MAC addresses. Run ip addr in the terminal Enter these values into the ./services/diyhue/diyhue.env file The default username and password it Hue and Hue respectively Usage The web interface is available on port 8070","title":"DIY hue"},{"location":"Containers/diyHue/#diy-hue","text":"website","title":"DIY hue"},{"location":"Containers/diyHue/#about","text":"diyHue is a utility to contol the lights in your home","title":"About"},{"location":"Containers/diyHue/#setup","text":"Before you start diyHue you will need to get your IP and MAC addresses. Run ip addr in the terminal Enter these values into the ./services/diyhue/diyhue.env file The default username and password it Hue and Hue respectively","title":"Setup"},{"location":"Containers/diyHue/#usage","text":"The web interface is available on port 8070","title":"Usage"},{"location":"Containers/openHAB/","text":"Openhab References Docker website openHAB has been added without Amazon Dashbutton binding. Port binding is 8080 for http and 8443 for https.","title":"Openhab"},{"location":"Containers/openHAB/#openhab","text":"","title":"Openhab"},{"location":"Containers/openHAB/#references","text":"Docker website openHAB has been added without Amazon Dashbutton binding. Port binding is 8080 for http and 8443 for https.","title":"References"},{"location":"Containers/x2go/","text":"x2go x2go is an \"alternative\" to using VNC for a remote connection. It uses X11 forwarding over ssh to provide a desktop environment Reason for using: I have a Pi 4 and I didn't buy a micro hdmi cable. You can use VNC however you are limited to a 800x600 window. Installation Install with sudo apt install x2goserver x2go cant connect to the native Raspbian Desktop so you will need to install another with sudo tasksel I chose Xfce because it is light weight. Install the x2go client from their website Now I have a full-screen client YouTube tutorial Laurence systems","title":"x2go"},{"location":"Containers/x2go/#x2go","text":"x2go is an \"alternative\" to using VNC for a remote connection. It uses X11 forwarding over ssh to provide a desktop environment Reason for using: I have a Pi 4 and I didn't buy a micro hdmi cable. You can use VNC however you are limited to a 800x600 window.","title":"x2go"},{"location":"Containers/x2go/#installation","text":"Install with sudo apt install x2goserver x2go cant connect to the native Raspbian Desktop so you will need to install another with sudo tasksel I chose Xfce because it is light weight. Install the x2go client from their website Now I have a full-screen client","title":"Installation"},{"location":"Containers/x2go/#youtube-tutorial","text":"Laurence systems","title":"YouTube tutorial"}]}